{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In the notebook, you can find a collection of Text Preprocessing steps that are common in a variety of NLP tasks. However, the list is probably not complete.\n",
        "\n",
        "It contains:\n",
        " - Text Cleaning steps\n",
        " - Text Normalization steps\n",
        " - Sources of information\n",
        "\n",
        "It does NOT contain:\n",
        " - Word Embeddings\n",
        " - Bag of Words\n",
        " - TF-IDF"
      ],
      "metadata": {
        "id": "tzIw4_XRqlyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q unidecode pyspellchecker autocorrect"
      ],
      "metadata": {
        "id": "2u2IfhF8f6kE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What does the code do?**\n",
        "   - This code installs three Python packages: `unidecode`, `pyspellchecker`, and `autocorrect` using pip. These packages are commonly used for text preprocessing and spell-checking tasks in natural language processing (NLP) applications.\n",
        "\n",
        "2. **Why is the `-q` flag used in the `pip install` command?**\n",
        "   - The `-q` flag stands for \"quiet\" mode, which suppresses most of the output generated by pip during the installation process. This is useful for keeping the output clean and concise, especially when running scripts or commands in automated environments.\n",
        "\n",
        "3. **What is the purpose of the `unidecode` package?**\n",
        "   - The `unidecode` package is used for transliterating Unicode characters into ASCII characters. It's commonly used to remove accents and diacritics from text, making it easier to process and analyze text data that contains non-ASCII characters.\n",
        "\n",
        "4. **What functionality does the `pyspellchecker` package provide?**\n",
        "   - The `pyspellchecker` package provides tools for spell checking in Python. It allows users to identify and correct misspelled words in text data. This package is useful for tasks such as text correction, typo detection, and improving the accuracy of natural language processing models.\n",
        "\n",
        "5. **How does the `autocorrect` package differ from `pyspellchecker`?**\n",
        "   - The `autocorrect` package is another Python library for correcting spelling errors in text. While both `autocorrect` and `pyspellchecker` offer similar functionality, `autocorrect` focuses more on automatically correcting misspelled words as you type, akin to the autocorrect feature in word processors and text editors. It employs a different algorithm for spell correction compared to `pyspellchecker`."
      ],
      "metadata": {
        "id": "3-nlThydRG45"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up8fGY5LfqEd"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. [Lowercasing](#lowercase)\n",
        "2. [Punctuation Removal](#removal)\n",
        "    - [str.translate()](#rmvl-translate)\n",
        "    - [Regular Expressions](#rmvl-re)\n",
        "      - [re.compile()](#rmvl-re-compile)\n",
        "      - [re.escape()](#rmvl-re-escape)\n",
        "      - [Remove not words and not whitespaces](#rmvl-re-not-words)\n",
        "    - [Excluding Punctuation](#rmvl-exclude)\n",
        "    - [String Replace](#rmvl-str-replace)\n",
        "3. [Numbers Removal](#removal-nb)\n",
        "    - [str.translate()](#rmvl-nb-tr)\n",
        "    - [Regular Expressions](#rmvl-nb-re)\n",
        "    - [join with not digit](#rmvl-nb-not-digit)\n",
        "    - [join with not alpha](#rmvl-nb-not-alpha)\n",
        "4. [HTML Tags Removal](#html-rmvl)\n",
        "    - [Regular Expressions](#html-rmvl-re)\n",
        "    - [Beautiful Soup](#html-rmvl-bs)\n",
        "5. [URL Removal](#url-rmvl)\n",
        "    - [Regular Expressions](#url-rmvl-re)\n",
        "6. [Newlines / spaces / tabs Removal](#spaces-rmvl)\n",
        "    - [String split](#spaces-rmvl-split)\n",
        "    - [Regular Expressions](#spaces-rmvl-re)\n",
        "        - [re.sub()](#spaces-rmvl-re)\n",
        "        - [re.findall()](#spaces-rmvl-re-findall)\n",
        "7. [Emojis Removal](#emoji-rmvl)\n",
        "    - [Regular Expressions](#emoji-rmvl-re)\n",
        "8. [Replacing accented characters](#accented)\n",
        "    - [Unidecode](#accented-unidecode)\n",
        "9. [Spelling corrections](#spell)\n",
        "    - [SpellChecker](#spell-checker)\n",
        "    - [Autocorrect](#spell-autocorrect)\n",
        "    - [TextBlob](#spell-tb)\n",
        "9. [Tokenization](#tknz)\n",
        "    - [Regular Expressions](#tknz-re)\n",
        "    - [NLTK](#tknz-nltk)\n",
        "    - [SpaCy](#tknz-spacy)\n",
        "    - [Gensim](#tknz-gensim)\n",
        "    - [Comparision of the methods](#tknz-compare)\n",
        "10. [Sentence Tokenization](#tknz-sents)\n",
        "    - [Regular Expressions](#tknz-sents-re)\n",
        "    - [NLTK](#tknz-sents-nltk)\n",
        "    - [SpaCy](#tknz-sents-spacy)\n",
        "11. [Stopwords Removal](#stopwords)\n",
        "    - [NLTK](#stopwords-nltk)\n",
        "    - [SpaCy](#stopwords-spacy)\n",
        "    - [Gensim](#stopwords-gensim)\n",
        "    - [Comparision of the methods](#stopwords-compare)\n",
        "    - [Comparision of stopwords lists](#stopwords-compare2)\n",
        "12. [Lemmatization](#lemma)\n",
        "    - [NLTK](#lemma-nltk)\n",
        "    - [SpaCy](#lemma-spacy)\n",
        "    - [TextBlob](#lemma-tb)\n",
        "    - [Adding Tags](#lemma-tags)\n",
        "        - [NLTK](#lemma-tags-nltk)\n",
        "        - [TextBlob](#lemma-tags-tb)\n",
        "    - [Comparision of the methods](#lemma-compare)\n",
        "13. [Stemming](#stem)\n",
        "    - [PorterStemmer](#stem-ps)\n",
        "    - [SnowballStemmer](#stem-sno)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_zwL6LEWfqEg"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is the purpose of importing the `string` module in this code?**\n",
        "   - The `string` module provides a collection of string constants and functions for working with strings in Python. It includes constants like `string.ascii_letters`, `string.digits`, and functions like `string.punctuation`. Importing `string` module in this code suggests that string manipulation or processing involving these constants or functions may be carried out.\n",
        "\n",
        "2. **Why is the `re` module imported?**\n",
        "   - The `re` module in Python provides support for regular expressions, which are a powerful tool for pattern matching and text manipulation. Its usage in this code indicates that the script may involve tasks such as pattern matching, substitution, or extraction based on specific patterns within text data.\n",
        "\n",
        "3. **What is the purpose of importing `nltk`?**\n",
        "   - `nltk` stands for Natural Language Toolkit, which is a leading platform for building Python programs to work with human language data. Importing `nltk` suggests that the script may utilize various NLP functionalities provided by the NLTK library, such as tokenization, stemming, lemmatization, part-of-speech tagging, and more.\n",
        "\n",
        "4. **Why is `spacy` imported?**\n",
        "   - `spacy` is another popular library for natural language processing in Python, known for its efficiency and ease of use. Importing `spacy` indicates that the script may employ Spacy's capabilities for tasks such as named entity recognition, dependency parsing, and text classification.\n",
        "\n",
        "5. **How does `nltk` differ from `spacy` in terms of functionality?**\n",
        "   - While both `nltk` and `spacy` are widely used in NLP tasks, they have different design philosophies and functionalities. `nltk` provides a wide range of basic NLP tools and algorithms, making it suitable for educational purposes and research. On the other hand, `spacy` focuses more on efficiency and ease of use, providing pre-trained models and streamlined APIs for common NLP tasks, making it more suitable for production-level applications."
      ],
      "metadata": {
        "id": "dMIoV-6jRRuP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRjBMWsCfqEi"
      },
      "source": [
        "## Lowercasing <a name=\"lowercase\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8-bIN5xefqEj",
        "outputId": "32f9e691-b599-445c-c365-ac0ae31b13a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello, poland is a very beautiful country!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "text_with_upper = \"Hello, POLAND is a Very BeautiFul CountrY!\"\n",
        "text_with_upper.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1grp1epfqEk"
      },
      "source": [
        "## Punctuation Removal <a name=\"removal\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Pgrii0fqEl"
      },
      "source": [
        "### **Most of them use `string.punctuation`**\n",
        "\n",
        "\n",
        "\n",
        "Based on the [StackOverflow question](https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
        "\n",
        "I ordered the solutions from the fastest to the slowest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Q7WILeX8fqEl"
      },
      "outputs": [],
      "source": [
        "puncts_text = \"HI!!! I overuse ... !(@*punctuations +-*/ and*(& other signs __*(&!!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqYWD41fqEm"
      },
      "source": [
        "### using `str.translate()` <a name=\"rmvl-translate\"></a>\n",
        "Probably the fastest way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9jjc7koxfqEn",
        "outputId": "d8c83df0-218d-422b-ee68-fdf39fdcec49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI I overuse  punctuations  and other signs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "puncts_text.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WXWE3WfqEo"
      },
      "source": [
        "### Using `re` <a name=\"rmvl-re\"></a>\n",
        "\n",
        "`re` stands for Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVkEJ-AGfqEo"
      },
      "source": [
        "#### Method 1 with string.punctuation <a name=\"rmvl-re-compile\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e_sCPXcEfqEp",
        "outputId": "e88f7736-14ec-4e23-c279-acaf74b20ba3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI I overuse  punctuations  and other signs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "regex.sub('', puncts_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvozz4kFfqEp"
      },
      "source": [
        "#### Method 1 in a single line <a name=\"rmvl-re-compile\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8Lqr_F7efqEp",
        "outputId": "000d98d0-eb5d-4ce6-c370-4d6b34b7f50c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI I overuse  punctuations  and other signs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "re.sub(f'[{re.escape(string.punctuation)}]','', puncts_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI50ACVifqEp"
      },
      "source": [
        "#### Method 2 <a name=\"rmvl-re-not-words\"></a>\n",
        "\n",
        "Removes **not words** and **not spaces**\n",
        "\n",
        "Note: `re` treats underscore as a word so its results are different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pRRTHtsffqEp",
        "outputId": "a73a7b70-bd42-487f-eb44-1117d684fed5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI I overuse  punctuations  and other signs __'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "re.sub(r'[^\\w\\s]','', puncts_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9698-jbfqEq"
      },
      "source": [
        "### Excluding string.punctuation <a name=\"rmvl-exclude\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "01L8-n-vfqEq",
        "outputId": "f7f703d2-6638-4b8c-bd47-82d93917e344"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI I overuse  punctuations  and other signs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# a slower solution\n",
        "exclude = set(string.punctuation)\n",
        "\"\".join(ch for ch in puncts_text if ch not in exclude)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vboDXqP6fqEq"
      },
      "source": [
        "### Using `str.replace()` <a name=\"rmvl-str-replace\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9yCPcHQXfqEr",
        "outputId": "335e1fb5-a183-4038-a11d-541a56202373"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI I overuse  punctuations  and other signs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "clean_text = puncts_text\n",
        "for c in string.punctuation:\n",
        "    clean_text = clean_text.replace(c, \"\")\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular expressions, often abbreviated as regex or regexp, provide a powerful and flexible way to search, match, and manipulate text strings based on specific patterns. They are widely used in various programming languages and text processing tools for tasks such as data validation, search and replace operations, and text extraction.\n",
        "\n",
        "Here's a more detailed explanation of regex:\n",
        "\n",
        "1. **Pattern Matching**: Regex allows you to define a pattern, which is a sequence of characters that describes a set of strings. For example, the pattern `[0-9]+` matches one or more digits.\n",
        "\n",
        "2. **Metacharacters**: Regex uses special characters called metacharacters to represent different types of characters or character classes. For example:\n",
        "   - `.` matches any single character except newline.\n",
        "   - `\\d` matches any digit (equivalent to `[0-9]`).\n",
        "   - `\\w` matches any alphanumeric character (equivalent to `[a-zA-Z0-9_]`).\n",
        "   - `\\s` matches any whitespace character.\n",
        "   - `[ ]` defines a character class, matching any single character within the brackets.\n",
        "\n",
        "3. **Quantifiers**: Quantifiers specify the number of occurrences of a pattern. For example:\n",
        "   - `*` matches zero or more occurrences.\n",
        "   - `+` matches one or more occurrences.\n",
        "   - `?` matches zero or one occurrence.\n",
        "   - `{n}` matches exactly n occurrences.\n",
        "   - `{n,}` matches n or more occurrences.\n",
        "   - `{n,m}` matches between n and m occurrences.\n",
        "\n",
        "4. **Anchors**: Anchors are used to specify the position in the string where a match should occur. For example:\n",
        "   - `^` matches the start of the string.\n",
        "   - `$` matches the end of the string.\n",
        "   - `\\b` matches a word boundary.\n",
        "\n",
        "5. **Grouping and Capturing**: Parentheses `()` are used to group patterns together. They also create capturing groups, which can be referenced later. For example:\n",
        "   - `(abc)+` matches one or more occurrences of the sequence \"abc\".\n",
        "   - `(a|b)` matches either \"a\" or \"b\".\n",
        "\n",
        "6. **Modifiers**: Modifiers are used to specify options or flags for the regex pattern. For example:\n",
        "   - `i` makes the pattern case-insensitive.\n",
        "   - `m` enables multi-line mode, where `^` and `$` match the start and end of each line.\n",
        "   - `g` performs a global search, finding all matches rather than stopping after the first match.\n",
        "\n",
        "Regex provides a concise and expressive way to describe complex text patterns, but it can also be challenging to master due to its compact syntax and wide range of features. However, once you become familiar with regex, it becomes an invaluable tool for text processing and manipulation tasks."
      ],
      "metadata": {
        "id": "fTlw2xVNSJOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is the purpose of the `text_with_upper` variable and the subsequent `.lower()` method call?**\n",
        "   - `text_with_upper` contains a string with mixed uppercase and lowercase characters. The `.lower()` method is called on this string to convert all characters to lowercase. This operation is commonly used for case normalization in text processing tasks to ensure consistent comparison and analysis.\n",
        "\n",
        "2. **What does the `puncts_text` variable represent, and why are various methods applied to remove punctuation?**\n",
        "   - `puncts_text` contains a string with various punctuation marks. Several methods are applied to remove punctuation from this string. This task is often performed in text preprocessing to eliminate noise and simplify text data for further analysis or processing.\n",
        "\n",
        "3. **How do the different methods (`translate`, `regex`, `re.sub`, manual replacement) achieve punctuation removal?**\n",
        "   - The methods shown (`translate`, `regex`, `re.sub`, manual replacement) all achieve the same goal of removing punctuation from the `puncts_text` string but using different approaches. They utilize string translation, regular expressions, and manual character replacement techniques. Each method has its advantages in terms of performance, readability, and flexibility.\n",
        "\n",
        "4. **Why is there a comment mentioning a \"slower solution\"?**\n",
        "   - The comment refers to the last method used for removing punctuation, which involves iterating over each character in the string and manually replacing punctuation characters with an empty string. This method is generally less efficient compared to other methods, especially for longer strings or when processing large volumes of text.\n",
        "\n",
        "5. **What is the purpose of the `clean_text` variable, and how does it differ from the original `puncts_text`?**\n",
        "   - The `clean_text` variable stores the result of removing punctuation from the `puncts_text` string using a manual replacement method. It differs from the original `puncts_text` by containing the same text but with all punctuation characters removed. This cleaned version of the text may be easier to process or analyze in certain text-based applications."
      ],
      "metadata": {
        "id": "goTV202OR3WV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odTIKFtYfqEr"
      },
      "source": [
        "## Numbers Removal <a name=\"removal-nb\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "351imb8yfqEr"
      },
      "outputs": [],
      "source": [
        "text_numbers = '12abcd405'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evf2G7iyfqEr"
      },
      "source": [
        "### Using `str.translate` with `string.digits` <a name=\"rmvl-nb-tr\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rFuuSvF-fqEr",
        "outputId": "a450d364-0607-4d42-921e-c67c8dacbcea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcd'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "text_numbers.translate(str.maketrans('', '', string.digits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwfhtqcjfqEr"
      },
      "source": [
        "### Using `re` <a name=\"rmvl-nb-re\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "52FCSuk0fqEs",
        "outputId": "3427c86c-1ed0-444c-9126-9f3ea8fa62a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcd'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "re.sub(r'\\d+', '', text_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zRfVtpbNfqEs",
        "outputId": "e1f6f0ea-bf27-4b3c-e27f-b086e11d6d4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcd'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "re.sub(r'[0-9]+', '', text_numbers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBCjLblnfqEs"
      },
      "source": [
        "### Using `join()` and NOT `isdigit()` <a name=\"rmvl-nb-not-digit\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r9b6DcThfqEs",
        "outputId": "e81f7dc8-b28a-4305-9c79-fb2ada38d6d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcd'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "\"\".join(i for i in text_numbers if not i.isdigit())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZAwIivefqEs"
      },
      "source": [
        "### Using `join()` and `isalpha()` <a name=\"rmvl-nb-not-alpha\"></a>\n",
        "\n",
        "This actually isn't the correct solution, because `isalpha()` is True only for letters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Bq0UqHIVfqEs",
        "outputId": "49912010-d230-47f8-fb30-e5c1885c198f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcd'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "\"\".join(i for i in text_numbers if i.isalpha())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What does the `text_numbers` variable represent?**\n",
        "   - `text_numbers` contains a string that includes both alphabetic characters and numerical digits. This string serves as the input text from which we aim to remove numerical digits.\n",
        "\n",
        "2. **Why are multiple methods used to remove numbers from the text?**\n",
        "   - The code demonstrates various methods to achieve the same task, which is removing numbers from the text. This variety showcases different approaches using string manipulation and regular expressions, allowing users to choose the method that best fits their requirements or preferences.\n",
        "\n",
        "3. **What is the purpose of the `str.maketrans('', '', string.digits)` method used in the `translate` function?**\n",
        "   - This method constructs a translation table to be used with the `translate` function. It maps each digit character to `None`, effectively removing all digits from the input string. This method is a straightforward and efficient way to perform character-based replacements in Python.\n",
        "\n",
        "4. **How do regular expressions (`re.sub`) facilitate number removal in the text?**\n",
        "   - Regular expressions provide a flexible and powerful way to match patterns within strings. The `re.sub` function with the pattern `r'\\d+'` searches for one or more consecutive digits (`\\d+`) in the text and replaces them with an empty string, effectively removing all numerical digits from the text.\n",
        "\n",
        "5. **What is the purpose of the last two methods using list comprehensions?**\n",
        "   - The last two methods utilize list comprehensions to iterate over each character in the `text_numbers` string. The first list comprehension removes digits (`if not i.isdigit()`) from the text, leaving only alphabetic characters. The second list comprehension keeps only alphabetic characters (`if i.isalpha()`), effectively removing numbers from the text. These methods demonstrate alternative approaches using Python's built-in string methods for character filtering."
      ],
      "metadata": {
        "id": "P6LiD74zSo_i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_kpX-1afqEs"
      },
      "source": [
        "## HTML Tags Removal <a name=\"html-rmvl\"></a>\n",
        "\n",
        "Solutions from [Stack Overflow](https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "1kHLzxI3fqEt"
      },
      "outputs": [],
      "source": [
        "html_text = \"\"\"<tr class=\"color-5 negri a-bottom\">\n",
        "<td class=\"a-center\" width=\"11%\"><div style=\"min-width: 80px\">3-Pointers</div></td>\n",
        "<td><div class=\"left\" style=\"min-width: 120px; max-width:175px; width: 57%\">\n",
        "<div class=\"left margen-l2\">Player</div>\n",
        "<div class=\"right\"> Team</div>\n",
        "</div>\n",
        "</td>\n",
        "<td><div style=\"min-width: 60px; \">Season</div></td>\n",
        "<td><div class=\"\">W/L Game</div>\n",
        "</td>\n",
        "</tr>\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PCdYYjVfqEt"
      },
      "source": [
        "### Using `re` <a name=\"html-rmvl-re\"></a>\n",
        "\n",
        " - begin with tag opening '<'\n",
        " - then not '<'\n",
        " - not '<' at least once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ptkQK8ZDfqEt",
        "outputId": "2017eedc-b464-4139-8c8a-aa7adcfaa2af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n3-Pointers\\n\\nPlayer\\n Team\\n\\n\\nSeason\\nW/L Game\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "re.sub('<[^<]+?>', '', html_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fABLaKrUfqEt"
      },
      "source": [
        "### Using `BeautifulSoup` <a name=\"html-rmvl-bs\"></a>\n",
        "\n",
        " - `get_text()` removes HTML tags\n",
        " - `strip = True` removes whitespaces and newlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "nVr2AyYNfqEt"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CLebXc_jfqEt",
        "outputId": "bddde67e-85c4-46d2-8a5d-527a619cf60c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3-Pointers,Player,Team,Season,W/L Game'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "soup = BeautifulSoup(html_text, 'html.parser')\n",
        "soup.get_text(\",\", strip=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_5b7vEfqEt"
      },
      "source": [
        "### Extreme example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "YqGUX3DRfqEu"
      },
      "outputs": [],
      "source": [
        "html_comment = \"<img<!-- --> src=x onerror=alert(1);//><!-- -->\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewk1Y58_fqEu"
      },
      "source": [
        "Both solutions fail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wX_Dm5UmfqEu",
        "outputId": "ba8866ef-1324-4afb-ad27-b793340f6d69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<img src=x onerror=alert(1);//>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "re.sub('<[^<]+?>', '', html_comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BqVkDW8qfqEu",
        "outputId": "4de378be-589e-46c4-ca1c-9f8a5271694f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'src=x onerror=alert(1);//>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "soup = BeautifulSoup(html_comment, 'html.parser')\n",
        "soup.get_text(\",\", strip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kK70YpdEfqEu",
        "outputId": "1777e3f9-342a-4d9e-bb0a-3d36157011cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' src=x onerror=alert(1);//>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "tag_re = re.compile(r'(<!--.*?-->|<[^>]*>)')\n",
        "no_tags = tag_re.sub('', html_comment)\n",
        "no_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xbiMfDqzfqEv",
        "outputId": "d102b444-f6e0-41d9-83d2-eb60a2a5da24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' src=x onerror=alert(1);//&gt;'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "import html\n",
        "\n",
        "html.escape(no_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What does the provided `html_text` variable contain?**\n",
        "   - `html_text` is a string containing HTML markup representing a table row (`<tr>`) with various attributes and nested elements such as table cells (`<td>`) and div containers (`<div>`). The text inside these HTML tags may include content like player names, team names, and other information related to basketball statistics.\n",
        "\n",
        "2. **How does the `re.sub('<[^<]+?>', '', html_text)` line remove HTML tags from the `html_text` variable?**\n",
        "   - This line uses a regular expression pattern (`<[^<]+?>`) to match any HTML tag (`<...>`) and replace it with an empty string. This effectively removes all HTML tags and leaves only the text content within the HTML markup.\n",
        "\n",
        "3. **What is the purpose of using BeautifulSoup in the code?**\n",
        "   - BeautifulSoup is a Python library used for parsing HTML and XML documents, extracting data, and navigating the document tree. In this code, BeautifulSoup is used to parse the `html_text` string and retrieve the text content while discarding the HTML tags. This is achieved using the `get_text()` method with appropriate parameters.\n",
        "\n",
        "4. **How does the `tag_re.sub('', html_comment)` line remove HTML comments from the `html_comment` variable?**\n",
        "   - This line utilizes a regular expression (`tag_re`) to match HTML comments (`<!-- ... -->`) and HTML tags (`<...>`), then replaces them with an empty string. This effectively removes both HTML comments and tags from the `html_comment` string.\n",
        "\n",
        "5. **Why is `html.escape(no_tags)` used in the code?**\n",
        "   - The `html.escape()` function is used to escape special characters in the `no_tags` string, ensuring that the resulting text is safe to display as plain text in HTML. This prevents potential security vulnerabilities such as cross-site scripting (XSS) attacks by converting characters like `<`, `>`, `&`, and `\"`."
      ],
      "metadata": {
        "id": "9OAo8B_GTV5W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQqRkIUefqEv"
      },
      "source": [
        "## URL Removal <a name=\"url-rmvl\"></a>\n",
        "\n",
        "From this [StackOverflow Question](https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/11332580)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "sjqgjDRIfqEv"
      },
      "outputs": [],
      "source": [
        "text_url = \"\"\"text1\n",
        "text2\n",
        "http://url.com/bla1/blah1/\n",
        "text3\n",
        "text4\n",
        "http://url.com/bla2/blah2/\n",
        "text5\n",
        "text6\n",
        "http://url.com/bla3/blah3/\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbOexv9MfqEv"
      },
      "source": [
        "### Using `re` <a name=\"url-rmvl-re\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UJAPRJ7ZfqEv",
        "outputId": "4336fd95-6acf-44ed-fd67-e5af50f05666"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'text1\\ntext2\\n\\ntext3\\ntext4\\n\\ntext5\\ntext6\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "re.sub(r'http\\S+', '', text_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3y9yGyzfqEw"
      },
      "source": [
        "## Newlines, spaces and tabs removal <a name=\"spaces-rmvl\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9sKd91sfqEw"
      },
      "source": [
        "### Using `str.split()` and `join()` <a name=\"spaces-rmvl-split\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "piBiy52YfqEw",
        "outputId": "b1b04a35-062b-4fc8-edb3-d0472b26b7ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to Remove all white spaces, new lines and tabs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "my_str=\"I want to Remove all white \\t\\n\\n\\r spaces, new lines \\n and tabs \\t\"\n",
        "\" \".join(my_str.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oXvZGj1fqEw"
      },
      "source": [
        "### Using `re` <a name=\"spaces-rmvl-re\"></a>\n",
        "\n",
        " - **`\\s` stands for whitespace character, equivalent to `[ \\n\\r\\t\\f]`**\n",
        " - **`\\S` stands for not whitespace character, equivalent to `[^\\s]`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "cWp-R1ktfqEx",
        "outputId": "ed4312e5-cf5d-472c-ed77-6118e04c5519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-4191088617.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re.sub('\\s+', ' ', my_str)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to Remove all white spaces, new lines and tabs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "re.sub('\\s+', ' ', my_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "iGfrsMykfqEx",
        "outputId": "fc5c772d-6dd9-4599-b810-560ba5be3c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
            "/tmp/ipython-input-2158050741.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  re.sub('[^\\S]+', ' ', my_str)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to Remove all white spaces, new lines and tabs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "re.sub('[^\\S]+', ' ', my_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_yuf3fG5fqEx",
        "outputId": "611ea48f-a8fd-4246-8493-be2ffea99f8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to Remove all white spaces, new lines and tabs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "re.sub('[\\t\\n\\r\\f ]+', ' ', my_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev2Tm_vVfqEx"
      },
      "source": [
        "### Using `re.findall()` <a name=\"spaces-rmvl-re-findall\"></a>\n",
        "\n",
        "Taken from [StackOverflow](https://stackoverflow.com/questions/4697882/how-can-i-find-all-matches-to-a-regular-expression-in-python)\n",
        "\n",
        "NOTE: I believe this approach is slower than the one with `re.sub()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "wdHwQ-mSfqEx",
        "outputId": "2da88f77-2df6-430c-ed4f-42517fb91df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "/tmp/ipython-input-4238487864.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  match = re.findall('[\\w]+ ', my_str)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to Remove all white new lines and tabs '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "match = re.findall('[\\w]+ ', my_str)\n",
        "\"\".join(match)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is the purpose of the `text_url` variable?**\n",
        "   - `text_url` is a multiline string containing text along with URLs interspersed within it. These URLs may represent links to various web pages or resources.\n",
        "\n",
        "2. **How does the `re.sub(r'http\\S+', '', text_url)` line remove URLs from the `text_url` variable?**\n",
        "   - This line uses a regular expression pattern (`http\\S+`) to match any URL starting with `http` followed by non-whitespace characters (`\\S+`). It then replaces all matched URLs with an empty string, effectively removing them from the text.\n",
        "\n",
        "3. **What is the purpose of using `str.split()` and `join()` to remove white spaces, new lines, and tabs?**\n",
        "   - These methods are used to remove white spaces, new lines, and tabs from the given string `my_str`. By splitting the string into a list of words using `split()`, and then rejoining these words with a single space separator using `join()`, all consecutive whitespace characters are effectively replaced by a single space.\n",
        "\n",
        "4. **How do the different regular expressions (`\\s+`, `[^\\S]+`, `[\\t\\n\\r\\f ]+`) achieve whitespace removal in the text?**\n",
        "   - Each of these regular expressions matches one or more whitespace characters, including spaces, tabs, newlines, and carriage returns. They then replace these matches with a single space, effectively collapsing multiple consecutive whitespace characters into a single space.\n",
        "\n",
        "5. **What does the `re.findall('[\\w]+ ', my_str)` line accomplish in the context of whitespace removal?**\n",
        "   - This line uses a regular expression pattern (`[\\w]+`) to match one or more word characters (alphanumeric characters and underscores) followed by a space. It then finds all such matches in the string `my_str`. By joining these matches together with an empty string (`\"\".join(match)`), it effectively removes all non-whitespace characters, leaving only whitespace between words."
      ],
      "metadata": {
        "id": "ozzJNGFaxtlR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFPReQgEfqEx"
      },
      "source": [
        "## Emojis Removal <a name=\"emoji-rmvl\"></a>\n",
        "\n",
        "Found [here](https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b#gistcomment-3315605)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CDKq1-vpfqEy",
        "outputId": "38ad9884-4e62-4abb-b935-f8be10d7e0c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi  How is your  and . Have a nice weekend '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "text_emojis = u\"Hi  How is your  and . Have a nice weekend \\U0001F600\\U0001F300\"\n",
        "text_emojis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `re` <a name=\"emoji-rmvl-re\"></a>"
      ],
      "metadata": {
        "id": "g-d-Y2zSi9N4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cmJMnP2-fqEy",
        "outputId": "9be59b4b-4f0d-4e59-83c9-1ee81c59a18f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi  How is your  and . Have a nice weekend '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "emojis_pattern = re.compile(pattern=\"[\"\n",
        "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                    u\"\\U00002702-\\U000027B0\"\n",
        "                    u\"\\U00002702-\\U000027B0\"\n",
        "                    u\"\\U000024C2-\\U0001F251\"\n",
        "                    u\"\\U0001f926-\\U0001f937\"\n",
        "                    u\"\\U00010000-\\U0010ffff\"\n",
        "                    u\"\\u2640-\\u2642\"\n",
        "                    u\"\\u2600-\\u2B55\"\n",
        "                    u\"\\u200d\"\n",
        "                    u\"\\u23cf\"\n",
        "                    u\"\\u23e9\"\n",
        "                    u\"\\u231a\"\n",
        "                    u\"\\ufe0f\"  # dingbats\n",
        "                    u\"\\u3030\"\n",
        "                \"]+\", flags = re.UNICODE)\n",
        "\n",
        "emojis_pattern.sub(r'', text_emojis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uJd0MOofqEz"
      },
      "source": [
        "## Replacing Accented Characters <a name=\"accented\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `unidecode` <a name=\"accented-unidecode\"></a>"
      ],
      "metadata": {
        "id": "Bs2j4WXBjdu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "31epNvbDfqEz",
        "outputId": "1530672d-10f8-4ccc-d99c-7b294f9ad83e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Malaga, aeeohello. Polish: nNcCsSeaozZzZ letters. German uoaoss letters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "import unidecode\n",
        "text_accented = \"Mlaga, hello. Polish:  letters. German  letters\"\n",
        "\n",
        "unidecode.unidecode(text_accented)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What does the `text_emojis` variable contain, and how are emojis represented in Python strings?**\n",
        "   - `text_emojis` contains a string with various emojis encoded as Unicode characters, such as , , and . Emojis in Python strings are represented using Unicode characters, which allow for the inclusion of diverse symbols and characters from different languages and character sets.\n",
        "\n",
        "2. **What is the purpose of the `emojis_pattern` variable and the associated regular expression pattern?**\n",
        "   - `emojis_pattern` defines a regular expression pattern that matches a wide range of Unicode characters representing emojis, symbols, and pictographs. This pattern is designed to identify and extract emojis from text data, enabling their removal or processing separately from other text content.\n",
        "\n",
        "3. **How does the `emojis_pattern.sub(r'', text_emojis)` line remove emojis from the `text_emojis` variable?**\n",
        "   - This line uses the `sub()` method of the `emojis_pattern` regex object to substitute all matches of the emoji pattern with an empty string (`r''`). As a result, all emojis identified by the regex pattern are effectively removed from the original text.\n",
        "\n",
        "4. **What does the `unidecode.unidecode(text_accented)` function accomplish?**\n",
        "   - The `unidecode.unidecode()` function is used to remove diacritics and accents from accented characters in the `text_accented` string. It transliterates accented characters into their ASCII equivalents, making the text more suitable for processing and comparison in contexts where accents are not significant.\n",
        "\n",
        "5. **Why is handling accented characters important in text processing tasks?**\n",
        "   - Accented characters can pose challenges in text processing and analysis tasks due to variations in character encodings and representations across different systems and languages. Removing accents using methods like `unidecode` helps standardize text data, ensuring consistent and accurate processing across different environments and applications."
      ],
      "metadata": {
        "id": "m_PNLshFwl1P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mII1zh5fqEz"
      },
      "source": [
        "## Spelling corrections <a name=\"spell\"></a>\n",
        "\n",
        "Solutions from [StackOverflow](https://stackoverflow.com/questions/13928155/spell-checker-for-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0RsDz9OfqE0"
      },
      "source": [
        "### Using `spellchecker` <a name=\"spell-checker\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TnkAGVYEfqE0",
        "outputId": "ec1a5183-d9ec-4531-a7d4-b85969a517ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I really need some corrections This sentence has misspelled words'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spelling(text):\n",
        "    corrected_text = list()\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        next_word = word\n",
        "        if word in misspelled_words:\n",
        "            next_word = spell.correction(word)\n",
        "        corrected_text.append(next_word)\n",
        "\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "text_misspelled = \"I realli needt smoe corection. This sentnce has mispelled wirds\"\n",
        "correct_spelling(text_misspelled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7g6wzGgfqE0"
      },
      "source": [
        "### Using `autocorrect`<a name=\"spell-autocorrect\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w-TXw7UfqE1",
        "outputId": "39f3c3f0-ba0a-41b9-d9d9-e6efc9a2003b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I really need some correction. This sentence has misspelled words\n"
          ]
        }
      ],
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "speller = Speller(lang='en')\n",
        "\n",
        "print(speller(text_misspelled))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKkbtQYKfqE1"
      },
      "source": [
        "### Using `textblob` <a name=\"spell-tb\"></a>\n",
        "\n",
        "Found [here](https://www.geeksforgeeks.org/python-textblob-correct-method/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDAEDVQgfqE1",
        "outputId": "001ea9da-5205-4d1d-dc9e-505b2384c533"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"I really need some correction. His sentence has dispelled words\")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "TextBlob(text_misspelled).correct()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is the purpose of the `correct_spelling` function in the code?**\n",
        "   - The `correct_spelling` function takes a string of text as input and corrects the misspelled words using a spellchecker. It splits the text into words, identifies misspelled words, and replaces them with their corrected versions using the `SpellChecker` from the `spellchecker` library.\n",
        "\n",
        "2. **How does the `SpellChecker` from the `spellchecker` library work in the context of spelling correction?**\n",
        "   - The `SpellChecker` object from the `spellchecker` library provides methods to identify misspelled words in a text and suggest corrections for them. It utilizes a pre-built dictionary of words and algorithms to determine the most likely correct spelling for a given word based on its context.\n",
        "\n",
        "3. **What is the purpose of the `autocorrect.Speller` object from the `autocorrect` library?**\n",
        "   - The `autocorrect.Speller` object provides a similar functionality to the `SpellChecker`, but it may use a different algorithm or dictionary for spell correction. It also identifies misspelled words and suggests corrections based on a predefined set of rules or patterns.\n",
        "\n",
        "4. **How does the `TextBlob` object from the `textblob` library handle spelling correction?**\n",
        "   - The `TextBlob` object provides a range of natural language processing (NLP) functionalities, including spelling correction. When the `correct()` method is called on a `TextBlob` object, it automatically corrects the spelling of words in the text using statistical methods and language models.\n",
        "\n",
        "5. **What are the differences between the `SpellChecker`, `autocorrect.Speller`, and `TextBlob` approaches to spelling correction?**\n",
        "   - While all three approaches aim to correct misspelled words in a text, they may use different algorithms, dictionaries, and language models. The effectiveness of each approach may vary depending on factors such as the quality of the dictionary, the complexity of the language, and the context of the text. Users may choose the most suitable approach based on their specific requirements and preferences."
      ],
      "metadata": {
        "id": "fXit0kQIw0eG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHlWZ8BifqE2"
      },
      "source": [
        "### Create a longer text\n",
        "\n",
        "Beginning of the Metamorphosis by Franz Kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "kGGqajoBfqE3",
        "outputId": "fba4d8fd-b1a3-4bb8-871a-dcac9887fbde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams,\\n he found himself transformed in his bed into a horrible vermin.  He lay\\n on his armour-like back, and if he lifted his head a little he could see his\\n brown belly, slightly domed and divided by arches into stiff sections.\\n The bedding was hardly able to cover it and seemed ready to slide off any\\n moment.  His many legs, pitifully thin compared with the size of the rest of\\n him, waved about helplessly as he looked.  \"What\\'s happened to me?\" he thought.\\n It wasn\\'t a dream.  His room, a proper human room although a little too small,\\n lay peacefully between its four familiar walls.  A collection of textile\\n samples lay spread out on the table - Samsa was a travelling salesman - and\\n above it there hung a picture that he had recently cut out of an illustrated\\n magazine and housed in a nice, gilded frame.  It showed a lady fitted out with\\n a fur hat and fur boa who sat upright, raising a heavy fur muff that covered\\n the whole of her lower arm towards the viewer.  Gregor then turned to look out\\n the window at the dull weather.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "text_lines = \"\"\"One morning, when Gregor Samsa woke from troubled dreams,\n",
        " he found himself transformed in his bed into a horrible vermin.  He lay\n",
        " on his armour-like back, and if he lifted his head a little he could see his\n",
        " brown belly, slightly domed and divided by arches into stiff sections.\n",
        " The bedding was hardly able to cover it and seemed ready to slide off any\n",
        " moment.  His many legs, pitifully thin compared with the size of the rest of\n",
        " him, waved about helplessly as he looked.  \"What's happened to me?\" he thought.\n",
        " It wasn't a dream.  His room, a proper human room although a little too small,\n",
        " lay peacefully between its four familiar walls.  A collection of textile\n",
        " samples lay spread out on the table - Samsa was a travelling salesman - and\n",
        " above it there hung a picture that he had recently cut out of an illustrated\n",
        " magazine and housed in a nice, gilded frame.  It showed a lady fitted out with\n",
        " a fur hat and fur boa who sat upright, raising a heavy fur muff that covered\n",
        " the whole of her lower arm towards the viewer.  Gregor then turned to look out\n",
        " the window at the dull weather.\"\"\"\n",
        "text_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ek-alOjfqE3"
      },
      "source": [
        "## Tokenize Words <a name=\"tknz\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbdyPCTTfqE3"
      },
      "source": [
        "### Using `re` <a name=\"tknz-re\"></a>\n",
        "\n",
        "Return the list of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpdWVgswfqE4",
        "outputId": "dc5bd8c2-499a-4038-cd06-57d4a25005a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'walls', 'A', 'collection', 'of', 'textile', 'samples', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'was', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "/tmp/ipython-input-4247139559.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  re_tokens = re.findall('[\\w]+', text_lines)\n"
          ]
        }
      ],
      "source": [
        "re_tokens = re.findall('[\\w]+', text_lines)\n",
        "print(len(re_tokens))\n",
        "print(re_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlKo4ts4fqE4"
      },
      "source": [
        "### Using `nltk` <a name=\"tknz-nltk\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlhGKEmsfqE4",
        "outputId": "2b8500ee-647b-4edd-f28a-cd03a7f6de84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 ['This', 'is', 'a', 'sample', 'sentence', '.', 'Let', \"'s\", 'test', 'tokenization', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download both tokenizer resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text_lines = \"This is a sample sentence. Let's test tokenization!\"\n",
        "nltk_tokens = word_tokenize(text_lines)\n",
        "print(len(nltk_tokens), nltk_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq_88qh0fqE4"
      },
      "source": [
        "### Using `spaCy` <a name=\"tknz-spacy\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMoUhOGGfqE4",
        "outputId": "2acc49b7-dc3a-4d8b-e805-68e34211cc36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "[This, is, a, sample, sentence, ., Let, 's, test, tokenization, !]\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text_lines)\n",
        "spacy_tokens = list([token for token in doc])\n",
        "print(len(spacy_tokens))\n",
        "print(spacy_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfUX079EfqE4"
      },
      "source": [
        "### Using `gensim` <a name=\"tknz-gensim\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ7duM1nfqE4",
        "outputId": "8c5b4462-96be-408c-aab9-f8ee8c805bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "9\n",
            "['This', 'is', 'a', 'sample', 'sentence', 'Let', 's', 'test', 'tokenization']\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "gensim_tokens = list(tokenize(text_lines))\n",
        "print(len(gensim_tokens))\n",
        "print(gensim_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What does the `text_lines` variable contain, and what is its significance in the code?**\n",
        "   - `text_lines` contains a multi-line string representing a passage of text from Franz Kafka's novella \"The Metamorphosis.\" This text serves as the input for tokenization, where the goal is to split the text into individual words or tokens.\n",
        "\n",
        "2. **How does the `re.findall('[\\w]+', text_lines)` line tokenize the text using regular expressions?**\n",
        "   - This line uses a regular expression pattern (`[\\w]+`) to match one or more word characters (alphanumeric characters and underscores). It then finds all such matches in the `text_lines` string, effectively tokenizing the text into words. However, this method may not handle certain cases like contractions or hyphenated words well.\n",
        "\n",
        "3. **What is the purpose of the `nltk.tokenize.word_tokenize()` function call?**\n",
        "   - The `word_tokenize()` function from the NLTK library is used to tokenize the text into words using a more sophisticated approach compared to simple regular expressions. It employs a pre-trained tokenizer specifically designed for natural language processing tasks, which may handle a wider range of tokenization challenges effectively.\n",
        "\n",
        "4. **How does the `spacy.load(\"en_core_web_sm\")` and subsequent tokenization using SpaCy work?**\n",
        "   - This code loads the English language model (`en_core_web_sm`) provided by SpaCy, which includes pre-trained models and linguistic annotations. The `nlp()` function then processes the `text_lines` string, creating a SpaCy `Doc` object representing the analyzed text. Tokenization in SpaCy is performed automatically as part of the text processing pipeline, and the resulting `Doc` object contains tokenized words as individual `Token` objects.\n",
        "\n",
        "5. **What is the purpose of the `gensim.utils.tokenize()` function for text tokenization?**\n",
        "   - The `tokenize()` function from the Gensim library is used to tokenize the text into words in a memory-efficient manner, suitable for processing large volumes of text. It implements a simple tokenizer that splits the text into words based on whitespace and punctuation characters. The resulting tokens are returned as a generator, allowing for efficient iteration over the tokens without loading the entire text into memory at once."
      ],
      "metadata": {
        "id": "EOwVCMR4xbZe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgaSfmZrfqE4"
      },
      "source": [
        "### Tokenization Comparision <a name=\"tknz-compare\"></a>\n",
        "\n",
        " - it seems that `gensim` uses the same `re` function, that we showed above. Both returned only words\n",
        " - `nltk` and `spacy` return also punctuations\n",
        " - `spacy` treats a whitespace as a token if there is a double whitespace. In our text each sentence-ending dot is followed by double whitespace. We could clean this but at least we see that `spacy` behaves differently\n",
        " - **not or n't** contraction gives different results. `spacy` and `nltk` splits *wasn't* to *was* and *n't*, whereas `re` and `gensim` to *wasn* and *t*.\n",
        " - when there is a hyphen between words, we get 3 different results. Our example is armour-like. `nltk` returns **a single token**: *armour-like*, `re` and `gensim` return **two tokens**: *armour* and *like*. `spacy` returns **three tokens**: *armour*, *-*, and *like*\n",
        " - `nltk` converts quotation marks. Quote opening: **``**, quote closing **' '**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxLBY4TYfqE5"
      },
      "source": [
        "## Tokenize Sentences <a name=\"tknz-sents\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Y82vxnfqE5"
      },
      "source": [
        "### Using `re` <a name=\"tknz-sents-re\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt84fLP3fqE5",
        "outputId": "ce954053-1a5d-45df-ee2c-b207c8b8be69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, ['This is a sample sentence', \" Let's test tokenization\", ''])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "re_sentences = re.compile('[.?!]').split(text_lines)\n",
        "len(re_sentences), re_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjpg5jsKfqE5"
      },
      "source": [
        "### Using `nltk` <a name=\"tknz-sents-nltk\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7t_uDJ3fqE5",
        "outputId": "c1e4f96c-1cd1-437f-fec3-5ad35b20f1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 ['This is a sample sentence.', \"Let's test tokenization!\"]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk_sentences = sent_tokenize(text_lines)\n",
        "print(len(nltk_sentences), nltk_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhlB9vhofqE5"
      },
      "source": [
        "### Using `spaCy` <a name=\"tknz-sents-spacy\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ7fu0IYfqE5",
        "outputId": "d01e2954-df03-4271-c5c7-28a469ee66f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, [This is a sample sentence., Let's test tokenization!])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text_lines)\n",
        "spacy_sentences = list([sent for sent in doc.sents])\n",
        "len(spacy_sentences), spacy_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What does the `re.compile('[.?!]').split(text_lines)` line accomplish for sentence tokenization?**\n",
        "   - This line uses a regular expression pattern (`[.?!]`) to split the `text_lines` string into a list of substrings at each occurrence of a period, question mark, or exclamation mark. Each substring represents a sentence, effectively tokenizing the text into sentences. However, this method may not handle all cases accurately, such as abbreviations or ellipses within sentences.\n",
        "\n",
        "2. **How does the `nltk.tokenize.sent_tokenize()` function tokenize the text into sentences?**\n",
        "   - The `sent_tokenize()` function from the NLTK library is specifically designed for sentence tokenization. It utilizes pre-trained models and rules to identify sentence boundaries accurately, handling various punctuation marks, abbreviations, and other linguistic features to split the text into individual sentences.\n",
        "\n",
        "3. **What is the purpose of loading the SpaCy English language model (`en_core_web_sm`) in the code?**\n",
        "   - The SpaCy library provides built-in support for sentence segmentation as part of its text processing pipeline. By loading the English language model (`en_core_web_sm`), the code prepares to tokenize the text into sentences using SpaCy's advanced linguistic analysis and rules for sentence boundary detection.\n",
        "\n",
        "4. **How does the `doc.sents` attribute in SpaCy tokenize the text into sentences?**\n",
        "   - Once the text is processed by SpaCy's language model (`nlp(text_lines)`), it creates a `Doc` object representing the analyzed text. The `doc.sents` attribute returns an iterator over sentence spans detected in the text. Each span represents a sentence, allowing for easy iteration over sentences in the text.\n",
        "\n",
        "5. **What are the advantages of using SpaCy for sentence tokenization compared to other methods?**\n",
        "   - SpaCy's sentence segmentation is based on advanced linguistic rules and statistical models, resulting in accurate and reliable sentence boundaries. It can handle complex sentence structures, abbreviations, and other linguistic phenomena effectively, making it suitable for a wide range of text processing tasks. Additionally, SpaCy integrates seamlessly with other SpaCy components and offers efficient processing speeds, making it a preferred choice for many NLP applications."
      ],
      "metadata": {
        "id": "yNjMP-oZxvxC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaM2N-WxfqE5"
      },
      "source": [
        "## Stopwords Removal <a name=\"stopwords\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScoqAHxVfqE6"
      },
      "source": [
        "We'll ignore punctuations. Tokenization step using `re` gives us exactly that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpQTgxsVfqE6"
      },
      "source": [
        "### Using `nltk` <a name=\"stopwords-nltk\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPTZvC3XfqE6",
        "outputId": "17e653e9-fbd1-46b8-87f1-5722ee2eea01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106 ['One', 'morning', 'Gregor', 'Samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'Samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer', 'Gregor', 'turned', 'look', 'window', 'dull', 'weather']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words_nltk = stopwords.words('english')\n",
        "# print(len(stop_words_nltk),stop_words_nltk)\n",
        "\n",
        "filtered_nltk = [word for word in re_tokens if word.lower() not in stop_words_nltk]\n",
        "print(len(filtered_nltk), filtered_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdBPnLnefqE7"
      },
      "source": [
        "### Using `spaCy` <a name=\"stopwords-spacy\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuTDmdV3fqE7",
        "outputId": "491e1b28-2829-45e4-f291-1b54c19d9f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 ['morning', 'Gregor', 'Samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'lifted', 'head', 'little', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'ready', 'slide', 'moment', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 's', 'happened', 'thought', 'wasn', 't', 'dream', 'room', 'proper', 'human', 'room', 'little', 'small', 'lay', 'peacefully', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'Samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'lower', 'arm', 'viewer', 'Gregor', 'turned', 'look', 'window', 'dull', 'weather']\n"
          ]
        }
      ],
      "source": [
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "\n",
        "filtered_spacy = [word for word in re_tokens if word.lower() not in stop_words_spacy]\n",
        "print(len(filtered_spacy), filtered_spacy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD5paBeEfqE8"
      },
      "source": [
        "### Using `gensim` <a name=\"stopwords-gensim\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX5J93H2fqE8",
        "outputId": "b918bebb-2d80-42c6-cee5-942346841d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "97 ['morning', 'Gregor', 'Samsa', 'woke', 'troubled', 'dreams', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'lifted', 'head', 'little', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'ready', 'slide', 'moment', 'legs', 'pitifully', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 's', 'happened', 'thought', 'wasn', 't', 'dream', 'room', 'proper', 'human', 'room', 'little', 'small', 'lay', 'peacefully', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'Samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'lower', 'arm', 'viewer', 'Gregor', 'turned', 'look', 'window', 'dull', 'weather']\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "stop_words_gensim = STOPWORDS\n",
        "\n",
        "filtered_gensim = [word for word in re_tokens if word.lower() not in stop_words_gensim]\n",
        "print(len(filtered_gensim), filtered_gensim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDCfmbUKfqE8"
      },
      "source": [
        "Another method from gensim using the `remove_stopwords` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbFceiH2fqE8",
        "outputId": "ad6e0d31-849a-4e9c-d75f-c002b06911fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46 This sample sentence. Let's test tokenization!\n"
          ]
        }
      ],
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "filtered_sentence_gensim = remove_stopwords(text_lines)\n",
        "print(len(filtered_sentence_gensim), filtered_sentence_gensim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is the purpose of loading stopwords from the NLTK corpus using `stopwords.words('english')`?**\n",
        "   - The NLTK library provides a collection of stopwords for various languages, including English. By loading the stopwords for English using `stopwords.words('english')`, the code retrieves a list of common words that are considered non-informative or irrelevant for text analysis tasks.\n",
        "\n",
        "2. **How does the `nlp.Defaults.stop_words` attribute in SpaCy provide stopwords for English?**\n",
        "   - SpaCy's language models include a set of default stopwords for the language they are trained on. By accessing the `nlp.Defaults.stop_words` attribute, the code retrieves a set of stopwords for English, which can be used to filter out non-essential words from text data.\n",
        "\n",
        "3. **What is the significance of `STOPWORDS` from Gensim's preprocessing module?**\n",
        "   - Gensim provides a set of stopwords through the `STOPWORDS` constant in its preprocessing module. These stopwords are commonly used in text processing tasks and can be employed to filter out unimportant words from text data.\n",
        "\n",
        "4. **How does the list comprehension `[word for word in re_tokens if word.lower() not in stop_words]` remove stopwords from the text?**\n",
        "   - This list comprehension iterates over each word token in the `re_tokens` list and checks if its lowercase version is not present in the `stop_words` set or list (depending on the library). If the word is not a stopword, it is included in the `filtered` list, effectively removing stopwords from the text data.\n",
        "\n",
        "5. **What does the `remove_stopwords(text_lines)` function from Gensim accomplish?**\n",
        "   - The `remove_stopwords()` function from Gensim's preprocessing module removes stopwords from the given text string (`text_lines`). It automatically filters out common stopwords based on the predefined list provided by Gensim's `STOPWORDS` constant, resulting in text with stopwords removed."
      ],
      "metadata": {
        "id": "8dmpyt1yyJ5H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l8cAPu4fqE8"
      },
      "source": [
        "### Comparing results <a name=\"stopwords-compare\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDgnp7PRfqE8"
      },
      "source": [
        "**`nltk` vs `spacy`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g_ix9d4fqE8",
        "outputId": "9789e6f4-2f9b-43e6-e442-3a8d8269917d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['seemed',\n",
              " 't',\n",
              " 'back',\n",
              " 'many',\n",
              " 's',\n",
              " 'whole',\n",
              " 'although',\n",
              " 'One',\n",
              " 'four',\n",
              " 'could',\n",
              " 'wasn',\n",
              " 'see',\n",
              " 'towards']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "list(set(filtered_nltk) ^ set(filtered_spacy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeWX8fsCfqE8"
      },
      "source": [
        "**`nltk` vs `gensim`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lhYjsakfqE9",
        "outputId": "d82a1e8a-07cd-4f43-c84e-5ee09bfb8b29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['seemed',\n",
              " 't',\n",
              " 'back',\n",
              " 'found',\n",
              " 'many',\n",
              " 's',\n",
              " 'whole',\n",
              " 'although',\n",
              " 'One',\n",
              " 'four',\n",
              " 'could',\n",
              " 'thin',\n",
              " 'wasn',\n",
              " 'see',\n",
              " 'towards']"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "list(set(filtered_nltk) ^ set(filtered_gensim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqKb0mYufqE9"
      },
      "source": [
        "**`gensim` vs `spacy`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCqngaR3fqE9",
        "outputId": "4d941b5a-0da9-4833-d72f-200505a4d5cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['found', 'thin']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "list(set(filtered_gensim) ^ set(filtered_spacy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Pvua1PfqE9"
      },
      "source": [
        "### Comparing stopwords lists <a name=\"stopwords-compare2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0NCzbq2fqE9"
      },
      "source": [
        "Lists of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "0OQYsYeTfqE9"
      },
      "outputs": [],
      "source": [
        "# print(\"NLTK stopwords\",stop_words_nltk)\n",
        "# print(\"Spacy stopwords\",stop_words_spacy)\n",
        "# print(\"Gensim stopwords\",stop_words_gensim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abp-mLshfqE9"
      },
      "source": [
        "Comparing length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yxuHBr-fqE9",
        "outputId": "440b874f-55e1-4983-aa94-8ccb5f139374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK stopwords len 198\n",
            "Spacy stopwords len 326\n",
            "Gensim stopwords len 337\n"
          ]
        }
      ],
      "source": [
        "print(\"NLTK stopwords len\", len(stop_words_nltk))\n",
        "print(\"Spacy stopwords len\", len(stop_words_spacy))\n",
        "print(\"Gensim stopwords len\", len(stop_words_gensim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xYxJR-2fqE9"
      },
      "source": [
        "## Lemmatization <a name=\"lemma\"></a>\n",
        "\n",
        "For lemmatization we'll ignore punctuations. Tokenization step using `re` gives us exactly that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvvaVZ9kfqE-"
      },
      "source": [
        "### Using `nltk` <a name=\"lemma-nltk\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPnqDwDnfqE-",
        "outputId": "47d003fb-e0ab-4aa1-cdd7-1630d057ed5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dream', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arch', 'into', 'stiff', 'section', 'The', 'bedding', 'wa', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'leg', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'a', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'it', 'four', 'familiar', 'wall', 'A', 'collection', 'of', 'textile', 'sample', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'wa', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk_lemmatizer = WordNetLemmatizer()\n",
        "nltk_lemmas = [nltk_lemmatizer.lemmatize(w) for w in re_tokens]\n",
        "print(nltk_lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text_lines = \"\"\"One morning, when Gregor Samsa woke from troubled dreams,\n",
        " he found himself transformed in his bed into a horrible vermin.  He lay\n",
        " on his armour-like back, and if he lifted his head a little he could see his\n",
        " brown belly, slightly domed and divided by arches into stiff sections.\n",
        " The bedding was hardly able to cover it and seemed ready to slide off any\n",
        " moment.  His many legs, pitifully thin compared with the size of the rest of\n",
        " him, waved about helplessly as he looked.  \"What's happened to me?\" he thought.\n",
        " It wasn't a dream.  His room, a proper human room although a little too small,\n",
        " lay peacefully between its four familiar walls.  A collection of textile\n",
        " samples lay spread out on the table - Samsa was a travelling salesman - and\n",
        " above it there hung a picture that he had recently cut out of an illustrated\n",
        " magazine and housed in a nice, gilded frame.  It showed a lady fitted out with\n",
        " a fur hat and fur boa who sat upright, raising a heavy fur muff that covered\n",
        " the whole of her lower arm towards the viewer.  Gregor then turned to look out\n",
        " the window at the dull weather.\"\"\""
      ],
      "metadata": {
        "id": "Qh7bJQB95Hll"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWNz0sq4fqE-"
      },
      "source": [
        "### Using `spaCy` <a name=\"lemma-spacy\"></a>\n",
        "\n",
        "[spacy documentation](https://spacy.io/api/lemmatizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkESJVp6fqE-",
        "outputId": "6a2c184e-ced5-4181-d18b-26d5b1d33bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 ['one', 'morning', 'when', 'Gregor', 'Samsa', 'wake', 'from', 'troubled', 'dream', 'he', 'find', 'himself', 'transform', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lie', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lift', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'dome', 'and', 'divide', 'by', 'arch', 'into', 'stiff', 'section', 'the', 'bedding', 'be', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seem', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'leg', 'pitifully', 'thin', 'compare', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'he', 'wave', 'about', 'helplessly', 'as', 'he', 'look', 'what', 's', 'happen', 'to', 'I', 'he', 'think', 'it', 'wasn', 't', 'a', 'dream', 'his', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lie', 'peacefully', 'between', 'its', 'four', 'familiar', 'wall', 'a', 'collection', 'of', 'textile', 'sample', 'lie', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'be', 'a', 'travel', 'salesman', 'and', 'above', 'it', 'there', 'hang', 'a', 'picture', 'that', 'he', 'have', 'recently', 'cut', 'out', 'of', 'an', 'illustrate', 'magazine', 'and', 'house', 'in', 'a', 'nice', 'gild', 'frame', 'it', 'show', 'a', 'lady', 'fit', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sit', 'upright', 'raise', 'a', 'heavy', 'fur', 'muff', 'that', 'cover', 'the', 'whole', 'of', 'her', 'low', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turn', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
          ]
        }
      ],
      "source": [
        "# spacy_text = \" \".join([token.text for token in spacy_tokens])\n",
        "text_from_re_tokens = \" \".join([word for word in re_tokens])\n",
        "# spacy_lemmas = [word.lemma_ for word in nlp(text_lines)]\n",
        "spacy_lemmas = [word.lemma_ for word in nlp(text_from_re_tokens)]\n",
        "print(len(spacy_lemmas), spacy_lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMFEyD3GfqE-"
      },
      "source": [
        "### Using `TextBlob` <a name=\"lemma-tb\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzZ3xwPTfqE-",
        "outputId": "6f7f324d-afc3-46e8-9e89-68c5a37bda36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 ['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dream', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arch', 'into', 'stiff', 'section', 'The', 'bedding', 'wa', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'leg', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'a', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'it', 'four', 'familiar', 'wall', 'A', 'collection', 'of', 'textile', 'sample', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'wa', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob, Word\n",
        "\n",
        "# create a TextBlob for our sentence\n",
        "sent_tb = TextBlob(text_from_re_tokens)\n",
        "\n",
        "# lemmatize each word\n",
        "blob_lemmas = [word.lemmatize() for word in sent_tb.words]\n",
        "print(len(blob_lemmas), blob_lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. **What is the purpose of the `WordNetLemmatizer` from NLTK, and how does it work?**\n",
        "   - The `WordNetLemmatizer` from NLTK is used for lemmatizing words, which involves reducing words to their base or dictionary form (known as lemma). It utilizes WordNet, a lexical database of the English language, to identify the base form of words. The lemmatization process considers the part of speech (POS) of each word to determine the appropriate lemma.\n",
        "\n",
        "2. **How does the list comprehension `[nltk_lemmatizer.lemmatize(w) for w in re_tokens]` lemmatize words using NLTK?**\n",
        "   - This list comprehension iterates over each word token (`w`) in the `re_tokens` list and applies the `lemmatize()` method of the `WordNetLemmatizer` to obtain the lemma of each word. The resulting list contains the lemmatized versions of the words in the `re_tokens` list.\n",
        "\n",
        "3. **What is the purpose of using SpaCy for lemmatization in the code?**\n",
        "   - SpaCy provides built-in support for lemmatization as part of its text processing pipeline. By accessing the `lemma_` attribute of each token in the SpaCy `Doc` object, the code retrieves the lemmatized form of each word token. SpaCy's lemmatization process takes into account the linguistic context of each word to produce accurate lemmas.\n",
        "\n",
        "4. **How does TextBlob facilitate lemmatization, and what does the `TextBlob(text_from_re_tokens)` object represent?**\n",
        "   - TextBlob provides a high-level interface for processing text data, including lemmatization. The `TextBlob(text_from_re_tokens)` object represents a text blob created from the string `text_from_re_tokens`. TextBlob automatically tokenizes the text and provides methods for various text processing tasks, including lemmatization. The `lemmatize()` method is applied to each word in the `TextBlob` object to obtain the lemmatized form.\n",
        "\n",
        "5. **What are the differences between NLTK, SpaCy, and TextBlob in terms of lemmatization?**\n",
        "   - NLTK, SpaCy, and TextBlob offer different approaches to lemmatization, each with its strengths and capabilities. NLTK provides a lemmatizer based on WordNet, which is suitable for basic lemmatization tasks. SpaCy's lemmatization is integrated into its text processing pipeline, offering efficient and accurate lemmatization with support for multiple languages and contextual information. TextBlob provides a simplified interface for text processing tasks, including lemmatization, making it easy to use for basic NLP tasks. Users may choose the most appropriate library based on their specific requirements and preferences."
      ],
      "metadata": {
        "id": "tPd_sZ-5ykPg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Bm4TOrfqE_"
      },
      "source": [
        "### Adding POS Tags to `nltk` and `TextBlob` <a name=\"lemma-tags\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4YU-PPnfqE_"
      },
      "source": [
        "By default, `nltk` and `TextBlob` treat every word as a noun. This is why words like \"woke\", \"found\", or \"transformed\" don't change after the lemmatization step. We can provide more information by adding the corresponding Part of Speech for each token."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `nltk` <a name=\"lemma-tags-nltk\"></a>\n",
        "\n",
        " - we use `pos_tag()` to get tokens along with tags\n",
        " - we call the `lemmatize()` function with the second parameter, that is a tag\n",
        "\n",
        "[Source](https://www.guru99.com/stemming-lemmatization-python-nltk.html)"
      ],
      "metadata": {
        "id": "YcycZjvPml8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UJAxh3kfqE_",
        "outputId": "fa9e431a-c8f1-4665-fac0-d0ca8a2072b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'quickly', 'dog']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import pos_tag\n",
        "from collections import defaultdict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download both taggers (old and new)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example setup\n",
        "nltk_lemmatizer = WordNetLemmatizer()\n",
        "re_tokens = [\"running\", \"quickly\", \"dogs\"]\n",
        "\n",
        "# POS tag mapping\n",
        "tag_map_nltk = defaultdict(lambda : wn.NOUN)\n",
        "tag_map_nltk['J'] = wn.ADJ\n",
        "tag_map_nltk['V'] = wn.VERB\n",
        "tag_map_nltk['R'] = wn.ADV\n",
        "\n",
        "# Lemmatization\n",
        "nltk_lemmas2 = [nltk_lemmatizer.lemmatize(token, tag_map_nltk[tag[0]]) for token, tag in pos_tag(re_tokens)]\n",
        "print(nltk_lemmas2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-i15L9afqE_"
      },
      "source": [
        "### Using `TextBlob` <a name=\"lemma-tags-tb\"></a>\n",
        "\n",
        " - we call `TextBlob(text).tags` to get tokens and tags\n",
        " - we call `word.lemmatize()` with the tag parameter\n",
        "\n",
        "[Source](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#textbloblemmatizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy5SbjO8fqE_",
        "outputId": "1a43c06e-a10a-4e54-bf44-fe4f0233ea88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 ['One', 'morning', 'when', 'Gregor', 'Samsa', 'wake', 'from', 'troubled', 'dream', 'he', 'find', 'himself', 'transform', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lift', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divide', 'by', 'arch', 'into', 'stiff', 'section', 'The', 'bedding', 'be', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seem', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'leg', 'pitifully', 'thin', 'compare', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'wave', 'about', 'helplessly', 'a', 'he', 'look', 'What', 's', 'happen', 'to', 'me', 'he', 'think', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'it', 'four', 'familiar', 'wall', 'A', 'collection', 'of', 'textile', 'sample', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'be', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hang', 'a', 'picture', 'that', 'he', 'have', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'house', 'in', 'a', 'nice', 'gild', 'frame', 'It', 'show', 'a', 'lady', 'fit', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sit', 'upright', 'raise', 'a', 'heavy', 'fur', 'muff', 'that', 'cover', 'the', 'whole', 'of', 'her', 'low', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turn', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
          ]
        }
      ],
      "source": [
        "tag_map_tb = {  \"J\": 'a', # adjectives\n",
        "                \"N\": 'n', # nouns\n",
        "                \"V\": 'v', # verbs\n",
        "                \"R\": 'r'} # adverbs\n",
        "\n",
        "words_and_tags = [(w, tag_map_tb.get(pos[0], 'n')) for w, pos in sent_tb.tags]\n",
        "blob_lemmas2 = [word.lemmatize(tag) for word, tag in words_and_tags]\n",
        "print(len(blob_lemmas2), blob_lemmas2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1SS6ubmfqE_"
      },
      "source": [
        "### Comparing Lemmatizers <a name=\"lemma-compare\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su4WKaPRfqE_",
        "outputId": "e680a94b-84be-4a25-dd48-a290455d3f1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "list(set(nltk_lemmas) ^ set(blob_lemmas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8xCNQUHfqFA",
        "outputId": "2c66aa30-e7c1-45fd-e19a-d654242ed4cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['low',\n",
              " 'from',\n",
              " 't',\n",
              " 'size',\n",
              " 'wake',\n",
              " 'at',\n",
              " 'familiar',\n",
              " 'her',\n",
              " 'who',\n",
              " 'domed',\n",
              " 'above',\n",
              " 'muff',\n",
              " 'compare',\n",
              " 'little',\n",
              " 'spread',\n",
              " 'recently',\n",
              " 'off',\n",
              " 'window',\n",
              " 'able',\n",
              " 'bedding',\n",
              " 'collection',\n",
              " 'himself',\n",
              " 'seem',\n",
              " 'and',\n",
              " 'a',\n",
              " 'cut',\n",
              " 'dream',\n",
              " 'section',\n",
              " 'Gregor',\n",
              " 'gild',\n",
              " 'he',\n",
              " 'wall',\n",
              " 'transform',\n",
              " 'What',\n",
              " 'stiff',\n",
              " 'textile',\n",
              " 'on',\n",
              " 'ready',\n",
              " 'dog',\n",
              " 'belly',\n",
              " 'Samsa',\n",
              " 's',\n",
              " 'to',\n",
              " 'slightly',\n",
              " 'that',\n",
              " 'look',\n",
              " 'lay',\n",
              " 'It',\n",
              " 'quickly',\n",
              " 'too',\n",
              " 'weather',\n",
              " 'picture',\n",
              " 'although',\n",
              " 'One',\n",
              " 'leg',\n",
              " 'A',\n",
              " 'there',\n",
              " 'when',\n",
              " 'morning',\n",
              " 'four',\n",
              " 'moment',\n",
              " 'like',\n",
              " 'cover',\n",
              " 'between',\n",
              " 'his',\n",
              " 'house',\n",
              " 'thin',\n",
              " 'arm',\n",
              " 'vermin',\n",
              " 'wasn',\n",
              " 'see',\n",
              " 'upright',\n",
              " 'the',\n",
              " 'dull',\n",
              " 'small',\n",
              " 'peacefully',\n",
              " 'pitifully',\n",
              " 'arch',\n",
              " 'boa',\n",
              " 'human',\n",
              " 'think',\n",
              " 'lady',\n",
              " 'in',\n",
              " 'hardly',\n",
              " 'raise',\n",
              " 'into',\n",
              " 'He',\n",
              " 'His',\n",
              " 'slide',\n",
              " 'back',\n",
              " 'any',\n",
              " 'by',\n",
              " 'travelling',\n",
              " 'head',\n",
              " 'table',\n",
              " 'turn',\n",
              " 'salesman',\n",
              " 'lift',\n",
              " 'brown',\n",
              " 'divide',\n",
              " 'heavy',\n",
              " 'many',\n",
              " 'about',\n",
              " 'fur',\n",
              " 'hang',\n",
              " 'show',\n",
              " 'wave',\n",
              " 'bed',\n",
              " 'then',\n",
              " 'proper',\n",
              " 'have',\n",
              " 'magazine',\n",
              " 'be',\n",
              " 'an',\n",
              " 'The',\n",
              " 'me',\n",
              " 'troubled',\n",
              " 'horrible',\n",
              " 'find',\n",
              " 'it',\n",
              " 'armour',\n",
              " 'viewer',\n",
              " 'sit',\n",
              " 'whole',\n",
              " 'frame',\n",
              " 'of',\n",
              " 'fit',\n",
              " 'out',\n",
              " 'could',\n",
              " 'run',\n",
              " 'illustrated',\n",
              " 'with',\n",
              " 'helplessly',\n",
              " 'rest',\n",
              " 'him',\n",
              " 'nice',\n",
              " 'towards',\n",
              " 'room',\n",
              " 'happen',\n",
              " 'if',\n",
              " 'sample',\n",
              " 'hat']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "list(set(nltk_lemmas2) ^ set(blob_lemmas2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofrJo4U8fqFA"
      },
      "source": [
        "`nltk` and `TextBlob` return identical results.\n",
        "\n",
        "Let's see what's changed after applying POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvvxnfyGfqFA",
        "outputId": "1abd71a3-efdf-4ad2-c612-31a154835d90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'Gregor',\n",
              " 'He',\n",
              " 'His',\n",
              " 'It',\n",
              " 'One',\n",
              " 'Samsa',\n",
              " 'The',\n",
              " 'What',\n",
              " 'a',\n",
              " 'able',\n",
              " 'about',\n",
              " 'above',\n",
              " 'although',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'arch',\n",
              " 'arm',\n",
              " 'armour',\n",
              " 'at',\n",
              " 'back',\n",
              " 'bed',\n",
              " 'bedding',\n",
              " 'belly',\n",
              " 'between',\n",
              " 'boa',\n",
              " 'brown',\n",
              " 'by',\n",
              " 'collection',\n",
              " 'compared',\n",
              " 'could',\n",
              " 'cover',\n",
              " 'covered',\n",
              " 'cut',\n",
              " 'divided',\n",
              " 'dog',\n",
              " 'domed',\n",
              " 'dream',\n",
              " 'dull',\n",
              " 'familiar',\n",
              " 'fitted',\n",
              " 'found',\n",
              " 'four',\n",
              " 'frame',\n",
              " 'from',\n",
              " 'fur',\n",
              " 'gilded',\n",
              " 'had',\n",
              " 'happened',\n",
              " 'hardly',\n",
              " 'hat',\n",
              " 'he',\n",
              " 'head',\n",
              " 'heavy',\n",
              " 'helplessly',\n",
              " 'her',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'horrible',\n",
              " 'housed',\n",
              " 'human',\n",
              " 'hung',\n",
              " 'if',\n",
              " 'illustrated',\n",
              " 'in',\n",
              " 'into',\n",
              " 'it',\n",
              " 'lady',\n",
              " 'lay',\n",
              " 'leg',\n",
              " 'lifted',\n",
              " 'like',\n",
              " 'little',\n",
              " 'look',\n",
              " 'looked',\n",
              " 'lower',\n",
              " 'magazine',\n",
              " 'many',\n",
              " 'me',\n",
              " 'moment',\n",
              " 'morning',\n",
              " 'muff',\n",
              " 'nice',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'out',\n",
              " 'peacefully',\n",
              " 'picture',\n",
              " 'pitifully',\n",
              " 'proper',\n",
              " 'quickly',\n",
              " 'raising',\n",
              " 'ready',\n",
              " 'recently',\n",
              " 'rest',\n",
              " 'room',\n",
              " 'run',\n",
              " 's',\n",
              " 'salesman',\n",
              " 'sample',\n",
              " 'sat',\n",
              " 'section',\n",
              " 'see',\n",
              " 'seemed',\n",
              " 'showed',\n",
              " 'size',\n",
              " 'slide',\n",
              " 'slightly',\n",
              " 'small',\n",
              " 'spread',\n",
              " 'stiff',\n",
              " 't',\n",
              " 'table',\n",
              " 'textile',\n",
              " 'that',\n",
              " 'the',\n",
              " 'then',\n",
              " 'there',\n",
              " 'thin',\n",
              " 'thought',\n",
              " 'to',\n",
              " 'too',\n",
              " 'towards',\n",
              " 'transformed',\n",
              " 'travelling',\n",
              " 'troubled',\n",
              " 'turned',\n",
              " 'upright',\n",
              " 'vermin',\n",
              " 'viewer',\n",
              " 'wa',\n",
              " 'wall',\n",
              " 'wasn',\n",
              " 'waved',\n",
              " 'weather',\n",
              " 'when',\n",
              " 'who',\n",
              " 'whole',\n",
              " 'window',\n",
              " 'with',\n",
              " 'woke']"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "sorted(list(set(nltk_lemmas) ^ set(nltk_lemmas2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnAex0LSfqFA"
      },
      "source": [
        "`nltk` did a great job at turning past tense verbs to the present tense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSfGchvnfqFA"
      },
      "source": [
        "## Stemming with `nltk` <a name=\"stem\"></a>\n",
        "\n",
        "Compare 2 types of Stemmers. [Source](https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7XVuTeSfqFA"
      },
      "source": [
        "### Using `PorterStemmer()` <a name=\"stem-ps\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHilGLLLfqFA",
        "outputId": "45a89f9a-7031-471f-ef06-7f3b37569308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'quickli', 'dog']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "ps_stemms = [ps.stem(w) for w in re_tokens]\n",
        "print(ps_stemms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. What is stemming and how does the `PorterStemmer` from NLTK perform stemming?**\n",
        "\n",
        "Stemming is the process of reducing words to their root or base form, typically by removing suffixes. The `PorterStemmer` algorithm, developed by Martin Porter, is one of the most widely used stemming algorithms. It applies a series of heuristic rules to remove common suffixes from words, aiming to produce the stem or root form.\n",
        "\n",
        "**2. How does the list comprehension `[ps.stem(w) for w in re_tokens]` perform stemming using the `PorterStemmer`?**\n",
        "\n",
        "This list comprehension iterates over each word token (`w`) in the `re_tokens` list and applies the `stem()` method of the `PorterStemmer` (`ps`) to obtain the stemmed form of each word. The resulting list (`ps_stemms`) contains the stemmed versions of the words in the `re_tokens` list.\n",
        "\n",
        "**3. What are some examples of stemming using the Porter Stemmer?**\n",
        "\n",
        "- \"Troubled\" stems to \"trouble\"\n",
        "- \"Divided\" stems to \"divid\"\n",
        "- \"Sections\" stems to \"section\"\n",
        "- \"Stiff\" stems to \"stiff\"\n",
        "- \"Ready\" stems to \"readi\"\n",
        "\n",
        "**4. How does stemming differ from lemmatization, and when might one be preferred over the other?**\n",
        "\n",
        "Stemming and lemmatization both aim to reduce words to their base forms, but they use different techniques and have different levels of accuracy. Stemming is a simpler and faster process as it applies heuristic rules to chop off suffixes, often resulting in stems that are not actual words. On the other hand, lemmatization involves dictionary lookup to find the lemma or base form of words, resulting in more accurate but potentially slower processing.\n",
        "\n",
        "Stemming might be preferred in applications where speed is crucial, such as information retrieval or text indexing, while lemmatization might be preferred in tasks where accuracy is paramount, such as natural language understanding or text analysis."
      ],
      "metadata": {
        "id": "BHBbsvIUzQon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `SnowballStemmer` <a name=\"stem-sno\"></a>"
      ],
      "metadata": {
        "id": "rzWH37HinpRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTyvqXMQfqFA",
        "outputId": "6fe52d8b-73bb-44ab-bce4-403447b10096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'quick', 'dog']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "sno = SnowballStemmer('english')\n",
        "\n",
        "sno_stemms = [sno.stem(w) for w in re_tokens]\n",
        "print(sno_stemms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**1. What is the Snowball Stemmer and how does it differ from the Porter Stemmer?**\n",
        "The Snowball Stemmer is a stemming algorithm that supports multiple languages. It is an improvement over the Porter Stemmer, offering more accurate stemming for various languages. The Snowball Stemmer is also known as the Porter2 Stemmer. It provides stemming algorithms for different languages, each identified by its language code.\n",
        "\n",
        "**2. How does the `SnowballStemmer` from NLTK perform stemming?**\n",
        "The `SnowballStemmer` in NLTK uses the Snowball stemming algorithm to reduce words to their root or base form. It applies language-specific rules and algorithms to perform stemming, aiming to produce accurate stem forms for words in the specified language.\n",
        "\n",
        "**3. What does the list comprehension `[sno.stem(w) for w in re_tokens]` accomplish?**\n",
        "This list comprehension iterates over each word token (`w`) in the `re_tokens` list and applies the `stem()` method of the `SnowballStemmer` (`sno`) to obtain the stemmed form of each word. The resulting list (`sno_stemms`) contains the stemmed versions of the words in the `re_tokens` list.\n",
        "\n",
        "**4. How does stemming with Snowball Stemmer differ from stemming with Porter Stemmer?**\n",
        "Snowball Stemmer is an improvement over the original Porter Stemmer and provides more accurate stemming for various languages. It offers stemmers for different languages, while the Porter Stemmer is primarily focused on English stemming. Snowball Stemmer uses a more refined algorithm, resulting in better stem forms for words in different languages compared to the Porter Stemmer.\n",
        "\n",
        "**5. In which scenarios might Snowball Stemmer be preferred over Porter Stemmer?**\n",
        "Snowball Stemmer might be preferred over Porter Stemmer when dealing with text data in languages other than English. Since Snowball Stemmer provides support for multiple languages, it can produce more accurate stem forms for words in those languages. Additionally, Snowball Stemmer might be preferred when higher accuracy in stemming is desired, as it offers improved algorithms and language-specific rules."
      ],
      "metadata": {
        "id": "Km2Ujqlazd9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Stemmers <a name=\"stem-compare\"></a>"
      ],
      "metadata": {
        "id": "_WMEwkC-nvGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-0L_4n6fqFA",
        "outputId": "b4877691-1d51-425f-db32-26cda990fc56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['quick', 'quickli']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "sorted(list(set(ps_stemms) ^ set(sno_stemms)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9CDoCtqfqFA"
      },
      "source": [
        "`PorterStemmer` and `SnowballStemmer` handle adverbs differently"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iECs0fhpfqFB"
      },
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "9a4f0234084b86db25ca1539382b5a4944f4886887c3cc83d09ff270c40bf732"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}