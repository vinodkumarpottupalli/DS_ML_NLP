{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **üìò Recap**\n",
        "\n",
        "### **What is an HMM (Hidden Markov Model)?**\n",
        "\n",
        "A **Hidden Markov Model** is a statistical model used when:\n",
        "\n",
        "* Something is happening **behind the scenes** (hidden states) ‚Äî we cannot see these states directly.\n",
        "* We only observe **outputs** (visible signals).\n",
        "* The hidden states follow the **Markov property** (next state depends only on the current state).\n",
        "* Each state emits observations with some probability.\n",
        "\n",
        "Think of it like:\n",
        "\n",
        "> You see the symptoms (observations) but not the actual health condition (hidden state).\n",
        "\n",
        "---\n",
        "\n",
        "# **What the Notebook is Doing**\n",
        "\n",
        "The notebook **teaches HMMs through a fun toy example**:\n",
        "\n",
        "### ‚úÖ **1. A motivating scenario**\n",
        "\n",
        "A friend travels between cities and sends you a selfie each day.\n",
        "You want to **guess which city they are in** based only on:\n",
        "\n",
        "* the selfie (observation)\n",
        "* knowledge of how they usually move between cities (transition probabilities)\n",
        "\n",
        "This real-life analogy is modeled as an HMM.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Explaining HMM structure**\n",
        "\n",
        "The notebook walks through:\n",
        "\n",
        "* **Hidden states** ‚Üí the city your friend is in\n",
        "* **Observations** ‚Üí the type of selfie you receive\n",
        "* **Transition probabilities** ‚Üí chance of moving from one city to another\n",
        "* **Emission probabilities** ‚Üí chance of getting a certain kind of selfie from a city\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. HMM assumptions**\n",
        "\n",
        "The two classic assumptions:\n",
        "\n",
        "1. **Markov property**: next city depends only on current city\n",
        "2. **Emission independence**: selfie depends only on current city, not previous days\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. HMM components**\n",
        "\n",
        "The notebook introduces all the key parameters:\n",
        "\n",
        "* **œÄ (initial distribution)** ‚Äì where the journey starts\n",
        "* **A (transition matrix)** ‚Äì how likely it is to move between states\n",
        "* **B (emission matrix)** ‚Äì how likely a state is to emit each observation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K_dp2-k-5jjA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDJIV2EVBuFZ"
      },
      "source": [
        "# Fun with Hidden Markov Models\n",
        "*by Loren Lugosch*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rWFkdjYOlk8"
      },
      "source": [
        "This notebook introduces the Hidden Markov Model (HMM), a simple model for sequential data.\n",
        "\n",
        "We will see:\n",
        "- what an HMM is and when you might want to use it;\n",
        "- the so-called \"three problems\" of an HMM; and\n",
        "- how to implement an HMM in PyTorch.\n",
        "\n",
        "(The code in this notebook can also be found at https://github.com/lorenlugosch/pytorch_HMM.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efPPcGy0gP6H"
      },
      "source": [
        "A hypothetical scenario\n",
        "------\n",
        "\n",
        "To motivate the use of HMMs, imagine that you have a friend who gets to do a lot of travelling. Every day, this jet-setting friend sends you a selfie from the city they‚Äôre in, to make you envious."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs3z7pnVib9g"
      },
      "source": [
        "<center>\n",
        "\n",
        "![Diagram of a traveling friend sending selfies](https://github.com/lorenlugosch/pytorch_HMM/blob/master/img/selfies.png?raw=true)\n",
        "</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTPNK3IjirDA"
      },
      "source": [
        "How would you go about guessing which city the friend is in each day, just by looking at the selfies?\n",
        "\n",
        "If the selfie contains a really obvious landmark, like the Eiffel Tower, it will be easy to figure out where the photo was taken. If not, it will be a lot harder to infer the city.\n",
        "\n",
        "But we have a clue to help us: the city the friend is in each day is not totally random. For example, the friend will probably remain in the same city for a few days to sightsee before flying to a new city."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4g7IG7CBx-Y"
      },
      "source": [
        "## The HMM setup\n",
        "\n",
        "The hypothetical scenario of the friend travelling between cities and sending you selfies can be modeled using an HMM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpwgbDTnRzRa"
      },
      "source": [
        "An HMM models a system that is in a particular state at any given time and produces an output that depends on that state.\n",
        "\n",
        "At each timestep or clock tick, the system randomly decides on a new state and jumps into that state. The system then randomly generates an observation. The states are \"hidden\": we can't observe them. (In the cities/selfies analogy, the unknown cities would be the hidden states, and the selfies would be the observations.)\n",
        "\n",
        "Let's denote the sequence of states as $\\mathbf{z} = \\{z_1, z_2, \\dots, z_T \\}$, where each state is one of a finite set of $N$ states, and the sequence of observations as $\\mathbf{x} = \\{x_1, x_2, \\dots, x_T\\}$. The observations could be discrete, like letters, or real-valued, like audio frames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV5fAhEQDAcJ"
      },
      "source": [
        "<center>\n",
        "\n",
        "![Diagram of an HMM for three timesteps](https://github.com/lorenlugosch/pytorch_HMM/blob/master/img/hmm.png?raw=true)\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMPrA6Uv-u-K"
      },
      "source": [
        "An HMM makes two key assumptions:\n",
        "- **Assumption 1:** The state at time $t$ depends *only* on the state at the previous time $t-1$.\n",
        "- **Assumption 2:** The output at time $t$ depends *only* on the state at time $t$.\n",
        "\n",
        "These two assumptions make it possible to efficiently compute certain quantities that we may be interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRNhSK7LgEIS"
      },
      "source": [
        "## Components of an HMM\n",
        "An HMM has three sets of trainable parameters.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pu3zm77vXwp"
      },
      "source": [
        "- The **transition model** is a square matrix $A$, where $A_{s, s'}$ represents $p(z_t = s|z_{t-1} = s')$, the probability of jumping from state $s'$ to state $s$.\n",
        "\n",
        "- The **emission model** $b_s(x_t)$ tells us $p(x_t|z_t = s)$, the probability of generating $x_t$ when the system is in state $s$. For discrete observations, which we will use in this notebook, the emission model is just a lookup table, with one row for each state, and one column for each observation. For real-valued observations, it is common to use a Gaussian mixture model or neural network to implement the emission model.\n",
        "\n",
        "- The **state priors** tell us $p(z_1 = s)$, the probability of starting in state $s$. We use $\\pi$ to denote the vector of state priors, so $\\pi_s$ is the state prior for state $s$.\n",
        "\n",
        "Let's program an HMM class in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZbW6Pj0og7K"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class HMM(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Hidden Markov Model with discrete observations.\n",
        "  \"\"\"\n",
        "  def __init__(self, M, N):\n",
        "    super(HMM, self).__init__()\n",
        "    self.M = M # number of possible observations\n",
        "    self.N = N # number of states\n",
        "\n",
        "    # A\n",
        "    self.transition_model = TransitionModel(self.N)\n",
        "\n",
        "    # b(x_t)\n",
        "    self.emission_model = EmissionModel(self.N,self.M)\n",
        "\n",
        "    # pi\n",
        "    self.unnormalized_state_priors = torch.nn.Parameter(torch.randn(self.N))\n",
        "\n",
        "    # use the GPU\n",
        "    self.is_cuda = torch.cuda.is_available()\n",
        "    if self.is_cuda: self.cuda()\n",
        "\n",
        "class TransitionModel(torch.nn.Module):\n",
        "  def __init__(self, N):\n",
        "    super(TransitionModel, self).__init__()\n",
        "    self.N = N\n",
        "    self.unnormalized_transition_matrix = torch.nn.Parameter(torch.randn(N,N))\n",
        "\n",
        "class EmissionModel(torch.nn.Module):\n",
        "  def __init__(self, N, M):\n",
        "    super(EmissionModel, self).__init__()\n",
        "    self.N = N\n",
        "    self.M = M\n",
        "    self.unnormalized_emission_matrix = torch.nn.Parameter(torch.randn(N,M))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eom3ueYtpXGo"
      },
      "source": [
        "To sample from the HMM, we start by picking a random initial state from the state prior distribution.\n",
        "\n",
        "Then, we sample an output from the emission distribution, sample a transition from the transition distribution, and repeat.\n",
        "\n",
        "(Notice that we pass the unnormalized model parameters through a softmax function to make them into probabilities.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Why are the transition, emission, and prior parameters named ‚Äúunnormalized‚Äù?**\n",
        "\n",
        "**Answer:**\n",
        "They are stored as **raw learnable tensors**, not probabilities yet.\n",
        "Before using them in Forward/Backward/Viterbi, we typically apply a **softmax** to convert them into valid probability distributions:\n",
        "\n",
        "* `pi = softmax(unnormalized_state_priors)`\n",
        "* `A = softmax(unnormalized_transition_matrix, dim=1)`\n",
        "* `B = softmax(unnormalized_emission_matrix, dim=1)`\n",
        "\n",
        "This allows PyTorch to optimize them freely without the constraint of summing to 1 during training.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why use `torch.nn.Parameter` for all HMM matrices?**\n",
        "\n",
        "**Answer:**\n",
        "`nn.Parameter` tells PyTorch:\n",
        "**‚ÄúThis tensor should be updated by gradient descent.‚Äù**\n",
        "So transition matrix, emission matrix, and state priors become **learnable parameters** when training with `loss.backward()`.\n",
        "\n",
        "If they were plain tensors, PyTorch would *not* update them.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why are the parameters initialized with `torch.randn()` instead of fixed values?**\n",
        "\n",
        "**Answer:**\n",
        "`torch.randn()` provides **random normal initialization**, which:\n",
        "\n",
        "* avoids symmetry in optimization\n",
        "* gives the model flexibility to learn any distribution\n",
        "* works well with softmax-based probability normalization\n",
        "\n",
        "Since HMM will be trained, random initialization is appropriate and common practice.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why is CUDA checked inside the HMM constructor?**\n",
        "\n",
        "**Answer:**\n",
        "Because HMM contains learnable parameters (`A`, `B`, `pi`), moving the main model to GPU automatically moves all submodules:\n",
        "\n",
        "```python\n",
        "if self.is_cuda:\n",
        "    self.cuda()\n",
        "```\n",
        "\n",
        "This ensures:\n",
        "\n",
        "* all computation (forward, backward, Viterbi) is **GPU-accelerated**\n",
        "* no mismatch between CPU and GPU tensors, which would lead to runtime errors\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why does the model separate TransitionModel and EmissionModel into different classes?**\n",
        "\n",
        "**Answer:**\n",
        "This design makes your HMM **modular and clean**:\n",
        "\n",
        "* TransitionModel handles **state ‚Üí state** probabilities\n",
        "* EmissionModel handles **state ‚Üí observation** probabilities\n",
        "* Each can be extended or replaced independently, e.g.,\n",
        "\n",
        "  * add dropout\n",
        "  * switch to neural emissions\n",
        "  * visualize parameters\n",
        "  * plug in learned embeddings\n",
        "\n",
        "It also makes the code easier to read and demo, since each part of the HMM is encapsulated.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BNbVCj_Oulyx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpgkwNyVwmyM"
      },
      "source": [
        "def sample(self, T=10):\n",
        "  state_priors = torch.nn.functional.softmax(self.unnormalized_state_priors, dim=0)\n",
        "  transition_matrix = torch.nn.functional.softmax(self.transition_model.unnormalized_transition_matrix, dim=0)\n",
        "  emission_matrix = torch.nn.functional.softmax(self.emission_model.unnormalized_emission_matrix, dim=1)\n",
        "\n",
        "  # sample initial state\n",
        "  z_t = torch.distributions.categorical.Categorical(state_priors).sample().item()\n",
        "  z = []; x = []\n",
        "  z.append(z_t)\n",
        "  for t in range(0,T):\n",
        "    # sample emission\n",
        "    x_t = torch.distributions.categorical.Categorical(emission_matrix[z_t]).sample().item()\n",
        "    x.append(x_t)\n",
        "\n",
        "    # sample transition\n",
        "    z_t = torch.distributions.categorical.Categorical(transition_matrix[:,z_t]).sample().item()\n",
        "    if t < T-1: z.append(z_t)\n",
        "\n",
        "  return x, z\n",
        "\n",
        "# Add the sampling method to our HMM class\n",
        "HMM.sample = sample"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Why do we apply softmax to the priors, transition matrix, and emission matrix before sampling?**\n",
        "\n",
        "**Answer:**\n",
        "The parameters stored in the model are **unnormalized logits**.\n",
        "Softmax converts them into valid probability distributions:\n",
        "\n",
        "* `state_priors` ‚Üí probabilities of initial states\n",
        "* `transition_matrix` ‚Üí probabilities of moving from one state to another\n",
        "* `emission_matrix` ‚Üí probabilities of emitting observations\n",
        "\n",
        "Without softmax, sampling would fail because the values would not sum to 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why does the transition sampling use `transition_matrix[:, z_t]` instead of `transition_matrix[z_t]`?**\n",
        "\n",
        "**Answer:**\n",
        "Because the transition matrix is stored so that:\n",
        "\n",
        "* **Columns** represent the *next-state* distribution\n",
        "* **Rows** represent the *from-state*\n",
        "\n",
        "So `transition_matrix[:, z_t]` means:\n",
        "\n",
        "> ‚ÄúGiven we are currently in state z_t, sample the next state based on the column z_t.‚Äù\n",
        "\n",
        "Many students expect row-wise transitions; your code uses column-wise.\n",
        "This is perfectly valid as long as consistency is maintained.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why do we sample emissions before sampling the next hidden state?**\n",
        "\n",
        "**Answer:**\n",
        "This follows the generative process of an HMM:\n",
        "\n",
        "1. Pick initial hidden state\n",
        "2. Emit observation from that state\n",
        "3. Transition to next hidden state\n",
        "4. Repeat\n",
        "\n",
        "The order **emission ‚Üí transition** reflects the standard generative structure:\n",
        "\n",
        "```\n",
        "z_t ‚Üí x_t\n",
        "z_t ‚Üí z_(t+1)\n",
        "```\n",
        "\n",
        "Both orders are allowed mathematically, but this one matches common textbooks.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why does the method return both `x` (observations) and `z` (hidden states)?**\n",
        "\n",
        "**Answer:**\n",
        "Because for a demo:\n",
        "\n",
        "* `x` shows what a **generated observation sequence** looks like\n",
        "* `z` allows you to **demonstrate the underlying hidden path**\n",
        "\n",
        "This is extremely helpful when teaching:\n",
        "\n",
        "* how HMMs generate data\n",
        "* how Viterbi decoding tries to recover z from x\n",
        "* how transition and emission probabilities shape the sequence\n",
        "\n",
        "It‚Äôs a great choice for visualization.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why is the initial state added to `z` before the sampling loop, but later states added inside the loop?**\n",
        "\n",
        "**Answer:**\n",
        "Because the first state is sampled **before** time-step iteration:\n",
        "\n",
        "```python\n",
        "z_t = Categorical(state_priors).sample()\n",
        "z.append(z_t)\n",
        "```\n",
        "\n",
        "Then each loop iteration emits an observation and transitions to the next state.\n",
        "The condition:\n",
        "\n",
        "```python\n",
        "if t < T-1:\n",
        "    z.append(z_t)\n",
        "```\n",
        "\n",
        "avoids adding an extra next-state after the final iteration.\n",
        "\n",
        "This keeps `len(z) = T`, matching the length of `x`.\n"
      ],
      "metadata": {
        "id": "lzFXUHgGu9EK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohsdYScawkRG"
      },
      "source": [
        "Let's try hard-coding an HMM for generating fake words. (We'll also add some helper functions for encoding and decoding strings.)\n",
        "\n",
        "We will assume that the system has one state for generating vowels and one state for generating consonants, and the transition matrix has 0s on the diagonal---in other words, the system cannot stay in the vowel state or the consonant state for one than one timestep; it has to switch.\n",
        "\n",
        "Since we pass the transition matrix through a softmax, to get 0s we set the unnormalized parameter values to $-\\infty$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyR7yv_3sBG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7524ecfd-a897-46f2-d45f-d3d8790786a4"
      },
      "source": [
        "import string\n",
        "alphabet = string.ascii_lowercase\n",
        "\n",
        "def encode(s):\n",
        "  \"\"\"\n",
        "  Convert a string into a list of integers\n",
        "  \"\"\"\n",
        "  x = [alphabet.index(ss) for ss in s]\n",
        "  return x\n",
        "\n",
        "def decode(x):\n",
        "  \"\"\"\n",
        "  Convert list of ints to string\n",
        "  \"\"\"\n",
        "  s = \"\".join([alphabet[xx] for xx in x])\n",
        "  return s\n",
        "\n",
        "# Initialize the model\n",
        "model = HMM(M=len(alphabet), N=2)\n",
        "\n",
        "# Hard-wiring the parameters!\n",
        "# Let state 0 = consonant, state 1 = vowel\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False # needed to do lines below\n",
        "model.unnormalized_state_priors[0] = 0.    # Let's start with a consonant more frequently\n",
        "model.unnormalized_state_priors[1] = -0.5\n",
        "print(\"State priors:\", torch.nn.functional.softmax(model.unnormalized_state_priors, dim=0))\n",
        "\n",
        "# In state 0, only allow consonants; in state 1, only allow vowels\n",
        "vowel_indices = torch.tensor([alphabet.index(letter) for letter in \"aeiou\"])\n",
        "consonant_indices = torch.tensor([alphabet.index(letter) for letter in \"bcdfghjklmnpqrstvwxyz\"])\n",
        "model.emission_model.unnormalized_emission_matrix[0, vowel_indices] = -np.inf\n",
        "model.emission_model.unnormalized_emission_matrix[1, consonant_indices] = -np.inf\n",
        "print(\"Emission matrix:\", torch.nn.functional.softmax(model.emission_model.unnormalized_emission_matrix, dim=1))\n",
        "\n",
        "# Only allow vowel -> consonant and consonant -> vowel\n",
        "model.transition_model.unnormalized_transition_matrix[0,0] = -np.inf  # consonant -> consonant\n",
        "model.transition_model.unnormalized_transition_matrix[0,1] = 0.       # vowel -> consonant\n",
        "model.transition_model.unnormalized_transition_matrix[1,0] = 0.       # consonant -> vowel\n",
        "model.transition_model.unnormalized_transition_matrix[1,1] = -np.inf  # vowel -> vowel\n",
        "print(\"Transition matrix:\", torch.nn.functional.softmax(model.transition_model.unnormalized_transition_matrix, dim=0))\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State priors: tensor([0.6225, 0.3775], device='cuda:0')\n",
            "Emission matrix: tensor([[0.0000, 0.2163, 0.0250, 0.0134, 0.0000, 0.0153, 0.0334, 0.0712, 0.0000,\n",
            "         0.0456, 0.0131, 0.0594, 0.0249, 0.0363, 0.0000, 0.1069, 0.0353, 0.0116,\n",
            "         0.0144, 0.0151, 0.0000, 0.0340, 0.1091, 0.0645, 0.0422, 0.0132],\n",
            "        [0.2717, 0.0000, 0.0000, 0.0000, 0.2163, 0.0000, 0.0000, 0.0000, 0.2508,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0335, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.2277, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       device='cuda:0')\n",
            "Transition matrix: tensor([[0., 1.],\n",
            "        [1., 0.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Why do we set `requires_grad = False` before manually assigning parameters?**\n",
        "\n",
        "**Answer:**\n",
        "`requires_grad=False` is needed because PyTorch **blocks in-place modifications** on tensors that require gradients.\n",
        "Since you‚Äôre *manually hard-wiring* transition/emission probabilities (e.g., setting some to `-inf`), you must disable gradients:\n",
        "\n",
        "```python\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "```\n",
        "\n",
        "Otherwise PyTorch will throw errors like:\n",
        "**‚Äúa leaf Variable that requires grad is being used in an in-place operation.‚Äù**\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why is `-np.inf` used in the emission and transition matrices?**\n",
        "\n",
        "**Answer:**\n",
        "`-inf` turns into **probability 0** after softmax:\n",
        "\n",
        "```\n",
        "softmax(-inf) = 0\n",
        "```\n",
        "\n",
        "This is a clean way to *completely forbid* certain transitions or emissions.\n",
        "\n",
        "Examples in your model:\n",
        "\n",
        "* Consonant state ‚Üí cannot emit vowels\n",
        "* Vowel state ‚Üí cannot emit consonants\n",
        "* Consonant ‚Üí consonant transitions forbidden\n",
        "* Vowel ‚Üí vowel transitions forbidden\n",
        "\n",
        "It enforces the structure:\n",
        "\n",
        "```\n",
        "consonant ‚Üî vowel alternation only\n",
        "```\n",
        "\n",
        "Perfect for demonstrating a rule-based HMM.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why do we convert letters to numbers using `encode()` and `decode()`?**\n",
        "\n",
        "**Answer:**\n",
        "HMMs operate on **indices**, not characters.\n",
        "\n",
        "`encode()` maps a string like `\"hello\"` to integer indices:\n",
        "`[7, 4, 11, 11, 14]`\n",
        "\n",
        "`decode()` converts predicted integer sequences back to readable strings.\n",
        "\n",
        "This mapping allows:\n",
        "\n",
        "* emission matrix to index observations\n",
        "* sampling to produce integer sequences\n",
        "* decoding to show human-interpretable letters\n",
        "\n",
        "It‚Äôs the standard preprocessing step for discrete HMMs.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why is the emission matrix built using vowel indices and consonant indices?**\n",
        "\n",
        "**Answer:**\n",
        "You define:\n",
        "\n",
        "* **State 0 = consonant state**\n",
        "* **State 1 = vowel state**\n",
        "\n",
        "Then enforce:\n",
        "\n",
        "```python\n",
        "state 0 emits ONLY consonants  \n",
        "state 1 emits ONLY vowels\n",
        "```\n",
        "\n",
        "by setting impossible emissions to `-inf`.\n",
        "\n",
        "This creates a **linguistically meaningful HMM** where sampling produces alternating consonant‚Äìvowel patterns (like ‚Äúb a l o n e‚Äù).\n",
        "\n",
        "It‚Äôs also excellent pedagogically because students immediately see how **emission constraints shape outputs**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why does the transition matrix forbid same-type transitions (C‚ÜíC and V‚ÜíV)?**\n",
        "\n",
        "**Answer:**\n",
        "You set:\n",
        "\n",
        "```python\n",
        "C‚ÜíC = -inf  \n",
        "V‚ÜíV = -inf  \n",
        "C‚ÜíV = 0  \n",
        "V‚ÜíC = 0\n",
        "```\n",
        "\n",
        "After softmax, the model becomes:\n",
        "\n",
        "```\n",
        "C ‚Üí V  with probability 1  \n",
        "V ‚Üí C  with probability 1\n",
        "```\n",
        "\n",
        "This enforces *strict alternation*:\n",
        "\n",
        "```\n",
        "consonant ‚Üí vowel ‚Üí consonant ‚Üí vowel ‚Üí ...\n",
        "```\n",
        "\n",
        "This cleanly demonstrates:\n",
        "\n",
        "* how transition probabilities shape hidden sequences\n",
        "* how to hard-code structural linguistic patterns\n",
        "* how HMM sampling behaves under deterministic transitions\n",
        "\n"
      ],
      "metadata": {
        "id": "ruIbxnE5vaix"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFaYq8jDttmi"
      },
      "source": [
        "Try sampling from our hard-coded model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8latFMD7ua0X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0528a6eb-7a49-42e8-9dc1-ebb7f3aa5f37"
      },
      "source": [
        "# Sample some outputs\n",
        "for _ in range(4):\n",
        "  sampled_x, sampled_z = model.sample(T=5)\n",
        "  print(\"x:\", decode(sampled_x))\n",
        "  print(\"z:\", sampled_z)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: ipeje\n",
            "z: [1, 0, 1, 0, 1]\n",
            "x: wiqiw\n",
            "z: [0, 1, 0, 1, 0]\n",
            "x: babix\n",
            "z: [0, 1, 0, 1, 0]\n",
            "x: hiley\n",
            "z: [0, 1, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzlTlfRgZod"
      },
      "source": [
        "## The Three Problems\n",
        "\n",
        "In a [classic tutorial](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf) on HMMs, Lawrence Rabiner describes \"three problems\" that need to be solved before you can effectively use an HMM. They are:\n",
        "- Problem 1: How do we efficiently compute $p(\\mathbf{x})$?\n",
        "- Problem 2: How do we find the most likely state sequence $\\mathbf{z}$ that could have generated the data?\n",
        "- Problem 3: How do we train the model?\n",
        "\n",
        "In the rest of the notebook, we will see how to solve each problem and implement the solutions in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_RfIAnmN2RZ"
      },
      "source": [
        "### Problem 1: How do we compute $p(\\mathbf{x})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zUUYH0giKV"
      },
      "source": [
        "\n",
        "#### *Why?*\n",
        "Why might we care about computing $p(\\mathbf{x})$? Here's two reasons.\n",
        "* Given two HMMs, $\\theta_1$ and $\\theta_2$, we can compute the likelihood of some data $\\mathbf{x}$ under each model, $p_{\\theta_1}(\\mathbf{x})$ and $p_{\\theta_2}(\\mathbf{x})$, to decide which model is a better fit to the data.\n",
        "\n",
        "  (For example, given an HMM for English speech and an HMM for French speech, we could compute the likelihood given each model, and pick the model with the higher likelihood to infer whether the person is speaking English or French.)\n",
        "* Being able to compute $p(\\mathbf{x})$ gives us a way to train the model, as we will see later.\n",
        "\n",
        "#### *How?*\n",
        "Given that we want $p(\\mathbf{x})$, how do we compute it?\n",
        "\n",
        "We've assumed that the data is generated by visiting some sequence of states $\\mathbf{z}$ and picking an output $x_t$ for each $z_t$ from the emission distribution $p(x_t|z_t)$. So if we knew $\\mathbf{z}$, then the probability of $\\mathbf{x}$ could be computed as follows:\n",
        "\n",
        "$$p(\\mathbf{x}|\\mathbf{z}) = \\prod_{t} p(x_t|z_t) p(z_t|z_{t-1})$$\n",
        "\n",
        "However, we don't know $\\mathbf{z}$; it's hidden. But we do know the probability of any given $\\mathbf{z}$, independent of what we observe. So we could get the probability of $\\mathbf{x}$ by summing over the different possibilities for $\\mathbf{z}$, like this:\n",
        "\n",
        "$$p(\\mathbf{x}) = \\sum_{\\mathbf{z}} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) = \\sum_{\\mathbf{z}} \\prod_{t} p(x_t|z_t) p(z_t|z_{t-1})$$\n",
        "\n",
        "The problem is: if you try to take that sum directly, you will need to compute $N^T$ terms. This is impossible to do for anything but very short sequences. For example, let's say the sequence is of length $T=100$ and there are $N=2$ possible states. Then we would need to check $N^T = 2^{100} \\approx 10^{30}$ different possible state sequences.\n",
        "\n",
        "We need a way to compute $p(\\mathbf{x})$ that doesn't require us to explicitly calculate all $N^T$ terms. For this, we use the forward algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrH0YdUAhS6J"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Forward Algorithm</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha_{s,1} := b_s(x_1) \\cdot \\pi_s$\n",
        ">\n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "> $\\alpha_{s,t} := b_s(x_t) \\cdot \\underset{s'}{\\sum} A_{s, s'} \\cdot \\alpha_{s',t-1} $\n",
        ">\n",
        "> $p(\\mathbf{x}) := \\underset{s}{\\sum} \\alpha_{s,T}$\\\n",
        "> return $p(\\mathbf{x})$\n",
        "________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAdpwRiMn8Vn"
      },
      "source": [
        "The forward algorithm is much faster than enumerating all $N^T$ possible state sequences: it requires only $O(N^2T)$ operations to run, since each step is mostly multiplying the vector of forward variables by the transition matrix. (And very often we can reduce that complexity even further, if the transition matrix is sparse.)\n",
        "\n",
        "There is one practical problem with the forward algorithm as presented above: it is prone to underflow due to multiplying a long chain of small numbers, since probabilities are always between 0 and 1. Instead, let's do everything in the log domain. In the log domain, a multiplication becomes a sum, and a sum becomes a [logsumexp](https://lorenlugosch.github.io/posts/2020/06/logsumexp/).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ8VsLFxA3iT"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Forward Algorithm (Log Domain)</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\text{log }\\alpha_{s,1} := \\text{log }b_s(x_1) + \\text{log }\\pi_s$\n",
        ">\n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "> $\\text{log }\\alpha_{s,t} := \\text{log }b_s(x_t) +  \\underset{s'}{\\text{logsumexp}} \\left( \\text{log }A_{s, s'} + \\text{log }\\alpha_{s',t-1} \\right)$\n",
        ">\n",
        "> $\\text{log }p(\\mathbf{x}) := \\underset{s}{\\text{logsumexp}} \\left( \\text{log }\\alpha_{s,T} \\right)$\\\n",
        "> return $\\text{log }p(\\mathbf{x})$\n",
        "________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g55ik6ZCEiJU"
      },
      "source": [
        "Now that we have a numerically stable version of the forward algorithm, let's implement it in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CMdK1EfE1SJ"
      },
      "source": [
        "def HMM_forward(self, x, T):\n",
        "  \"\"\"\n",
        "  x : IntTensor of shape (batch size, T_max)\n",
        "  T : IntTensor of shape (batch size)\n",
        "\n",
        "  Compute log p(x) for each example in the batch.\n",
        "  T = length of each example\n",
        "  \"\"\"\n",
        "  if self.is_cuda:\n",
        "  \tx = x.cuda()\n",
        "  \tT = T.cuda()\n",
        "\n",
        "  batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  log_alpha = torch.zeros(batch_size, T_max, self.N)\n",
        "  if self.is_cuda: log_alpha = log_alpha.cuda()\n",
        "\n",
        "  log_alpha[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
        "  for t in range(1, T_max):\n",
        "    log_alpha[:, t, :] = self.emission_model(x[:,t]) + self.transition_model(log_alpha[:, t-1, :])\n",
        "\n",
        "  # Select the sum for the final timestep (each x may have different length).\n",
        "  log_sums = log_alpha.logsumexp(dim=2)\n",
        "  log_probs = torch.gather(log_sums, 1, T.view(-1,1) - 1)\n",
        "  return log_probs\n",
        "\n",
        "def emission_model_forward(self, x_t):\n",
        "  log_emission_matrix = torch.nn.functional.log_softmax(self.unnormalized_emission_matrix, dim=1)\n",
        "  out = log_emission_matrix[:, x_t].transpose(0,1)\n",
        "  return out\n",
        "\n",
        "def transition_model_forward(self, log_alpha):\n",
        "  \"\"\"\n",
        "  log_alpha : Tensor of shape (batch size, N)\n",
        "  Multiply previous timestep's alphas by transition matrix (in log domain)\n",
        "  \"\"\"\n",
        "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "  # Matrix multiplication in the log domain\n",
        "  out = log_domain_matmul(log_transition_matrix, log_alpha.transpose(0,1)).transpose(0,1)\n",
        "  return out\n",
        "\n",
        "def log_domain_matmul(log_A, log_B):\n",
        "\t\"\"\"\n",
        "\tlog_A : m x n\n",
        "\tlog_B : n x p\n",
        "\toutput : m x p matrix\n",
        "\n",
        "\tNormally, a matrix multiplication\n",
        "\tcomputes out_{i,j} = sum_k A_{i,k} x B_{k,j}\n",
        "\n",
        "\tA log domain matrix multiplication\n",
        "\tcomputes out_{i,j} = logsumexp_k log_A_{i,k} + log_B_{k,j}\n",
        "\t\"\"\"\n",
        "\tm = log_A.shape[0]\n",
        "\tn = log_A.shape[1]\n",
        "\tp = log_B.shape[1]\n",
        "\n",
        "\t# log_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "\t# log_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "    # fix for PyTorch > 1.5 by egaznep on Github:\n",
        "\tlog_A_expanded = torch.reshape(log_A, (m,n,1))\n",
        "\tlog_B_expanded = torch.reshape(log_B, (1,n,p))\n",
        "\n",
        "\telementwise_sum = log_A_expanded + log_B_expanded\n",
        "\tout = torch.logsumexp(elementwise_sum, dim=1)\n",
        "\n",
        "\treturn out\n",
        "\n",
        "TransitionModel.forward = transition_model_forward\n",
        "EmissionModel.forward = emission_model_forward\n",
        "HMM.forward = HMM_forward"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Why do we use the Forward algorithm recurrence instead of brute-forcing all hidden-state sequences?**\n",
        "\n",
        "**Significance in code:**\n",
        "The line\n",
        "\n",
        "```python\n",
        "log_alpha[:, t, :] = emission(...) + transition_model(log_alpha[:, t-1])\n",
        "```\n",
        "\n",
        "compresses an exponential number of state paths into a **single dynamic-programming step**.\n",
        "\n",
        "**Significance for HMM:**\n",
        "An HMM with `N` states and sequence length `T` has **N^T** possible hidden paths.\n",
        "Brute forcing all paths is impossible.\n",
        "\n",
        "The Forward algorithm *efficiently sums over all possible hidden paths* while preserving exact probabilities:\n",
        "\n",
        "[\n",
        "p(x_{1:T}) = \\sum_{all ; z} p(x, z)\n",
        "]\n",
        "\n",
        "This is the **core reason** why HMMs are computationally feasible.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why do we work in log space instead of normal probability space?**\n",
        "\n",
        "**Significance in code:**\n",
        "All transitions and emissions go through:\n",
        "\n",
        "```python\n",
        "log_softmax(...)\n",
        "```\n",
        "\n",
        "**Significance for HMM:**\n",
        "HMMs multiply many probabilities. Even valid sequences may have likelihoods like:\n",
        "\n",
        "[\n",
        "10^{-40}, 10^{-80}, 10^{-150}\n",
        "]\n",
        "\n",
        "These collapse to zero in floating point arithmetic.\n",
        "\n",
        "Log space transforms:\n",
        "\n",
        "* multiplication ‚Üí addition\n",
        "* summation ‚Üí logsumexp\n",
        "* extremely tiny probabilities ‚Üí stable numbers\n",
        "\n",
        "The HMM Forward algorithm **relies on this stability** to produce meaningful likelihoods.\n",
        "\n",
        "If you do not use log space, the HMM forward pass becomes useless for moderate-length sequences.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why do we compute emission probabilities using indexing (`emission_model(x[:, t])`) instead of full matrix multiplication?**\n",
        "\n",
        "**Significance in code:**\n",
        "You only fetch emission probabilities **for the observed symbol** at time `t`.\n",
        "\n",
        "**Significance for HMM:**\n",
        "The HMM emission model defines:\n",
        "\n",
        "[\n",
        "p(x_t | z_t)\n",
        "]\n",
        "\n",
        "It is **conditional on the observation**, not on every possible observation.\n",
        "By indexing directly:\n",
        "\n",
        "```python\n",
        "emission_matrix[:, x_t]\n",
        "```\n",
        "\n",
        "you implement the exact HMM rule:\n",
        "*‚ÄúWhich states could have emitted this specific observed symbol?‚Äù*\n",
        "\n",
        "This avoids unnecessary work and respects the structure of discrete emission HMMs.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why do we use a log-domain matrix multiplication instead of regular matrix multiplication for transitions?**\n",
        "\n",
        "**Significance in code:**\n",
        "Your custom method:\n",
        "\n",
        "```python\n",
        "log_domain_matmul(log_A, log_B)\n",
        "```\n",
        "\n",
        "computes:\n",
        "\n",
        "[\n",
        "\\log \\sum_i \\exp(\\log A_{i \\to j} + \\log \\alpha_{t-1}(i))\n",
        "]\n",
        "\n",
        "**Significance for HMM:**\n",
        "The Forward algorithm needs to **sum over all previous states**:\n",
        "\n",
        "[\n",
        "\\alpha_t(j) = b_j(x_t) \\sum_i \\alpha_{t-1}(i) A_{i \\to j}\n",
        "]\n",
        "\n",
        "Regular matrix multiplication performs **products**, not log-sum-exp.\n",
        "Log-domain matmul implements the *exact HMM update rule* in a numerically safe way.\n",
        "\n",
        "Without this, you are not executing the true HMM Forward algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why do we use `logsumexp` over hidden states at the final timestep instead of taking a max or using the last Œ± value directly?**\n",
        "\n",
        "**Significance in code:**\n",
        "\n",
        "```python\n",
        "log_sums = log_alpha.logsumexp(dim=2)\n",
        "```\n",
        "\n",
        "**Significance for HMM:**\n",
        "For an HMM, the probability of the entire observed sequence is:\n",
        "\n",
        "[\n",
        "p(x_{1:T}) = \\sum_{states ; i} \\alpha_T(i)\n",
        "]\n",
        "\n",
        "This is a **sum**, not a maximum.\n",
        "Taking a max would correspond to the **Viterbi algorithm**, which finds:\n",
        "\n",
        "* most likely hidden path\n",
        "  not\n",
        "* total likelihood of the observations.\n",
        "\n",
        "Forward algorithm = **sum over all possible explanations**\n",
        "Viterbi = **best single explanation**\n",
        "\n",
        "So `logsumexp` maintains the probabilistic meaning of an HMM.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "idzAPkjxwFTA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-fNnZfqGb1m"
      },
      "source": [
        "Try running the forward algorithm on our vowels/consonants model from before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rMAmf-UGhbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f88562-0839-4027-d944-438aa46919c5"
      },
      "source": [
        "x = torch.stack( [torch.tensor(encode(\"cat\"))] )\n",
        "T = torch.tensor([3])\n",
        "print(model.forward(x, T))\n",
        "\n",
        "x = torch.stack( [torch.tensor(encode(\"aba\")), torch.tensor(encode(\"abb\"))] )\n",
        "T = torch.tensor([3,3])\n",
        "print(model.forward(x, T))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-9.6603]], device='cuda:0')\n",
            "tensor([[-5.1112],\n",
            "        [   -inf]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Why do we encode characters into integers instead of feeding letters directly?**\n",
        "\n",
        "**Significance:**\n",
        "HMMs operate on **discrete symbols**, not characters or strings.\n",
        "By converting `\"cat\"` ‚Üí `[2,0,19]`, we map letters into indices so they act as **observable emission symbols**.\n",
        "\n",
        "**Why not feed characters?**\n",
        "Characters have no numerical meaning; the HMM needs integer categories to index:\n",
        "\n",
        "* emission probabilities `P(x | state)`\n",
        "* transition matrices\n",
        "* priors\n",
        "\n",
        "This step mirrors how NLP models typically use **tokenization**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why did we *hard-wire* the model parameters instead of training them?**\n",
        "\n",
        "**Significance:**\n",
        "This demo is intended to **illustrate the structure and logic of an HMM**, not how it learns.\n",
        "\n",
        "Hard-wiring lets you:\n",
        "\n",
        "* **visually understand** how priors, transitions, and emissions influence probability\n",
        "* **enforce linguistic constraints** (like vowel/consonant alternation)\n",
        "* avoid randomness from training that can hide the intended behavior\n",
        "\n",
        "**Why not train?**\n",
        "Training would produce arbitrary parameter values unless the corpora contained perfect vowel‚Äìconsonant alternation, making the conceptual lesson weaker.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why force the emission matrix to allow only vowel‚Äìstate emissions and consonant‚Äìstate emissions?**\n",
        "\n",
        "**Significance:**\n",
        "This shows **how HMM states represent *latent categories*** ‚Äî here:\n",
        "\n",
        "* **State 0 ‚Üí consonant-emitter**\n",
        "* **State 1 ‚Üí vowel-emitter**\n",
        "\n",
        "By putting `-inf` for invalid symbols, we force:\n",
        "\n",
        "```\n",
        "P(vowel | consonant-state) = 0\n",
        "P(consonant | vowel-state) = 0\n",
        "```\n",
        "\n",
        "**Why not allow soft probabilities?**\n",
        "Because the purpose is to **illustrate deterministic constraints inside an HMM**.\n",
        "Soft probabilities would dilute the structure and make the alternation pattern less clear.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why restrict the transition matrix to ONLY vowel‚Üíconsonant and consonant‚Üívowel?**\n",
        "\n",
        "**Significance:**\n",
        "This enforces an **alternating HMM**:\n",
        "\n",
        "```\n",
        "C ‚Üí V ‚Üí C ‚Üí V ‚Üí ...\n",
        "```\n",
        "\n",
        "This helps students clearly see how transitions control **sequence structure**, independent of emissions.\n",
        "\n",
        "**Why not allow C‚ÜíC or V‚ÜíV transitions?**\n",
        "Because then the model could generate arbitrary sequences and you lose the clean, interpretable pattern.\n",
        "\n",
        "This is a teaching example of how transitions encode **grammar-like constraints**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why call `model.forward(x, T)` with tensor shapes like this?**\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "x = torch.stack([torch.tensor(encode(\"cat\"))])\n",
        "T = torch.tensor([3])\n",
        "```\n",
        "\n",
        "**Significance:**\n",
        "The forward pass computes:\n",
        "\n",
        "* the **log-likelihood** of each sequence\n",
        "* respecting:\n",
        "\n",
        "  * priors\n",
        "  * allowed transitions\n",
        "  * allowed emissions\n",
        "  * sequence length `T`\n",
        "\n",
        "`x` is shaped as a **batch of sequences**, so you see how HMMs naturally support **multiple sequences**, e.g.:\n",
        "\n",
        "```python\n",
        "x = torch.stack([encode(\"aba\"), encode(\"abb\")])\n",
        "T = [3, 3]\n",
        "```\n",
        "\n",
        "**Why not pass raw strings?**\n",
        "The model needs:\n",
        "\n",
        "* numeric emissions\n",
        "* fixed-length tensors\n",
        "* batch structure\n",
        "  None of these exist with raw Python strings.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Why do we see different probabilities for sequences like ‚Äúaba‚Äù vs ‚Äúabb‚Äù?**\n",
        "\n",
        "**Significance:**\n",
        "Because the HMM‚Äôs forced structure influences how likely a sequence is.\n",
        "Example:\n",
        "\n",
        "```\n",
        "a b a ‚Üí vowel, consonant, vowel ‚Üí fits perfectly\n",
        "a b b ‚Üí vowel, consonant, consonant ‚Üí INVALID final consonant\n",
        "```\n",
        "\n",
        "So the HMM assigns:\n",
        "\n",
        "* **high probability** to sequences that match the alternating pattern\n",
        "* **zero probability** where the last emission forbids the state structure\n",
        "\n",
        "This shows the power of HMMs in modeling **latent structure constraints**.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Why set priors to prefer starting in a consonant?**\n",
        "\n",
        "**Significance:**\n",
        "Most English-like words begin with consonants.\n",
        "So you set:\n",
        "\n",
        "```\n",
        "P(state0 = consonant) > P(state1 = vowel)\n",
        "```\n",
        "\n",
        "**Why not keep uniform priors?**\n",
        "Uniform priors hide the effect priors have on:\n",
        "\n",
        "* forward probabilities\n",
        "* decoding\n",
        "* sequence likelihood\n",
        "\n",
        "Explicit priors demonstrate how initial state assumptions affect the entire probability calculation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LjwLvhDwXF_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95TB2gvNHuLn"
      },
      "source": [
        "When using the vowel <-> consonant HMM from above, notice that the forward algorithm returns $-\\infty$ for $\\mathbf{x} = \\text{\"abb\"}$. That's because our transition matrix says the probability of vowel -> vowel and consonant -> consonant is 0, so the probability of $\\text{\"abb\"}$ happening is 0, and thus the log probability is $-\\infty$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBCrFobsEM8X"
      },
      "source": [
        "#### *Side note: deriving the forward algorithm*\n",
        "\n",
        "If you're interested in understanding how the forward algorithm actually computes $p(\\mathbf{x})$, read this section; if not, skip to the next part on \"Problem 2\" (finding the most likely state sequence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpHWWKcxhjkx"
      },
      "source": [
        "\n",
        "\n",
        "To derive the forward algorithm, start by deriving the forward variable:\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "    \\alpha_{s,t} &= p(x_1, x_2, \\dots, x_t, z_t=s) \\\\\n",
        "     &= p(x_t | x_1, x_2, \\dots, x_{t-1}, z_t = s) \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_t = s)  \\\\\n",
        "    &= p(x_t | z_t = s) \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_t = s) \\\\\n",
        "    &= p(x_t | z_t = s) \\cdot \\left( \\sum_{s'} p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s', z_t = s) \\right)\\\\\n",
        "    &= p(x_t | z_t = s) \\cdot \\left( \\sum_{s'} p(z_t = s | x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s') \\cdot p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s') \\right)\\\\\n",
        "    &= \\underbrace{p(x_t | z_t = s)}_{\\text{emission model}} \\cdot \\left( \\sum_{s'} \\underbrace{p(z_t = s | z_{t-1}=s')}_{\\text{transition model}} \\cdot \\underbrace{p(x_1, x_2, \\dots, x_{t-1}, z_{t-1}=s')}_{\\text{forward variable for previous timestep}} \\right)\\\\\n",
        "    &= b_s(x_t) \\cdot \\left( \\sum_{s'} A_{s, s'} \\cdot \\alpha_{s',t-1} \\right)\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "I'll explain how to get to each line of this equation from the previous line.\n",
        "\n",
        "Line 1 is the definition of the forward variable $\\alpha_{s,t}$.\n",
        "\n",
        "Line 2 is the chain rule ($p(A,B) = p(A|B) \\cdot p(B)$, where $A$ is $x_t$ and $B$ is all the other variables).\n",
        "\n",
        "In Line 3, we apply Assumption 2: the probability of observation $x_t$ depends only on the current state $z_t$.\n",
        "\n",
        "In Line 4, we marginalize over all the possible states in the previous timestep $t-1$.\n",
        "\n",
        "In Line 5, we apply the chain rule again.\n",
        "\n",
        "In Line 6, we apply Assumption 1: the current state depends only on the previous state.\n",
        "\n",
        "In Line 7, we substitute in the emission probability, the transition probability, and the forward variable for the previous timestep, to get the complete recursion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh1ovNjWDbIA"
      },
      "source": [
        "The formula above can be used for $t = 2 \\rightarrow T$. At $t=1$, there is no previous state, so instead of the transition matrix $A$, we use the state priors $\\pi$, which tell us the probability of starting in each state. Thus for $t=1$, the forward variables are computed as follows:\n",
        "\n",
        "$$\\begin{align}\n",
        "\\alpha_{s,1} &= p(x_1, z_1=s) \\\\\n",
        "  &= p(x_1 | z_1 = s) \\cdot p(z_1 = s)  \\\\\n",
        "&= b_s(x_1) \\cdot \\pi_s\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRzSqkRkEWKX"
      },
      "source": [
        "Finally, to compute $p(\\mathbf{x}) = p(x_1, x_2, \\dots, x_T)$, we marginalize over $\\alpha_{s,T}$, the forward variables computed in the last timestep:\n",
        "\n",
        "$$\\begin{align*}\n",
        "p(\\mathbf{x}) &= \\sum_{s} p(x_1, x_2, \\dots, x_T, z_T = s) \\\\\n",
        "&= \\sum_{s} \\alpha_{s,T}\n",
        "\\end{align*}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLBU8Iu7I5Tb"
      },
      "source": [
        "You can get from this formulation to the log domain formulation by taking the log of the forward variable, and using these identities:\n",
        "- $\\text{log }(a \\cdot b) = \\text{log }a + \\text{log }b$\n",
        "- $\\text{log }(a + b) = \\text{log }(e^{\\text{log }a} + e^{\\text{log }b}) = \\text{logsumexp}(\\text{log }a, \\text{log }b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxivzF8hgpiW"
      },
      "source": [
        "### Problem 2: How do we compute $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x})$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1Kv2yyiN7SX"
      },
      "source": [
        "Given an observation sequence $\\mathbf{x}$, we may want to find the most likely sequence of states that could have generated $\\mathbf{x}$. (Given the sequence of selfies, we want to infer what cities the friend visited.) In other words, we want $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x})$.\n",
        "\n",
        "We can use Bayes' rule to rewrite this expression:\n",
        "$$\\begin{align*}\n",
        "    \\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{z}|\\mathbf{x}) &= \\underset{\\mathbf{z}}{\\text{argmax }} \\frac{p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{p(\\mathbf{x})} \\\\\n",
        "    &= \\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})\n",
        "\\end{align*}$$\n",
        "\n",
        "Hmm! That last expression, $\\underset{\\mathbf{z}}{\\text{argmax }} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})$, looks suspiciously similar to the intractable expression we encountered before introducing the forward algorithm, $\\underset{\\mathbf{z}}{\\sum} p(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})$.\n",
        "\n",
        "And indeed, just as the intractable *sum* over all $\\mathbf{z}$ can be implemented efficiently using the forward algorithm, so too this intractable *argmax* can be implemented efficiently using a similar divide-and-conquer algorithm: the legendary Viterbi algorithm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niKZEX5xWeWR"
      },
      "source": [
        "________\n",
        "\n",
        "<u><b>The Viterbi Algorithm</b></u>\n",
        "\n",
        "> for $s=1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta_{s,1} := b_s(x_1) \\cdot \\pi_s$\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi_{s,1} := 0$\n",
        ">\n",
        "> for $t = 2 \\rightarrow T$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for $s = 1 \\rightarrow N$:\\\n",
        "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\delta_{s,t} := b_s(x_t) \\cdot \\left( \\underset{s'}{\\text{max }} A_{s, s'} \\cdot \\delta_{s',t-1} \\right)$\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\psi_{s,t} := \\underset{s'}{\\text{argmax }} A_{s, s'} \\cdot \\delta_{s',t-1}$\n",
        ">\n",
        "> $z_T^* := \\underset{s}{\\text{argmax }} \\delta_{s,T}$\\\n",
        "> for $t = T-1 \\rightarrow 1$:\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_{t}^* := \\psi_{z_{t+1}^*,t+1}$\n",
        ">\n",
        "> $\\mathbf{z}^* := \\{z_{1}^*, \\dots, z_{T}^* \\}$\\\n",
        "return $\\mathbf{z}^*$\n",
        "________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcHVTCucZV6K"
      },
      "source": [
        "The Viterbi algorithm looks somewhat gnarlier than the forward algorithm, but it is essentially the same algorithm, with two tweaks: 1) instead of taking the sum over previous states, we take the max; and 2) we record the argmax of the previous states in a table, and loop back over this table at the end to get $\\mathbf{z}^*$, the most likely state sequence. (And like the forward algorithm, we should run the Viterbi algorithm in the log domain for better numerical stability.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlN7IY_JZ5A-"
      },
      "source": [
        "Let's add the Viterbi algorithm to our PyTorch model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeDG8DVmZ-P0"
      },
      "source": [
        "def viterbi(self, x, T):\n",
        "  \"\"\"\n",
        "  x : IntTensor of shape (batch size, T_max)\n",
        "  T : IntTensor of shape (batch size)\n",
        "  Find argmax_z log p(x|z) for each (x) in the batch.\n",
        "  \"\"\"\n",
        "  if self.is_cuda:\n",
        "    x = x.cuda()\n",
        "    T = T.cuda()\n",
        "\n",
        "  batch_size = x.shape[0]; T_max = x.shape[1]\n",
        "  log_state_priors = torch.nn.functional.log_softmax(self.unnormalized_state_priors, dim=0)\n",
        "  log_delta = torch.zeros(batch_size, T_max, self.N).float()\n",
        "  psi = torch.zeros(batch_size, T_max, self.N).long()\n",
        "  if self.is_cuda:\n",
        "    log_delta = log_delta.cuda()\n",
        "    psi = psi.cuda()\n",
        "\n",
        "  log_delta[:, 0, :] = self.emission_model(x[:,0]) + log_state_priors\n",
        "  for t in range(1, T_max):\n",
        "    max_val, argmax_val = self.transition_model.maxmul(log_delta[:, t-1, :])\n",
        "    log_delta[:, t, :] = self.emission_model(x[:,t]) + max_val\n",
        "    psi[:, t, :] = argmax_val\n",
        "\n",
        "  # Get the log probability of the best path\n",
        "  log_max = log_delta.max(dim=2)[0]\n",
        "  best_path_scores = torch.gather(log_max, 1, T.view(-1,1) - 1)\n",
        "\n",
        "  # This next part is a bit tricky to parallelize across the batch,\n",
        "  # so we will do it separately for each example.\n",
        "  z_star = []\n",
        "  for i in range(0, batch_size):\n",
        "    z_star_i = [ log_delta[i, T[i] - 1, :].max(dim=0)[1].item() ]\n",
        "    for t in range(T[i] - 1, 0, -1):\n",
        "      z_t = psi[i, t, z_star_i[0]].item()\n",
        "      z_star_i.insert(0, z_t)\n",
        "\n",
        "    z_star.append(z_star_i)\n",
        "\n",
        "  return z_star, best_path_scores # return both the best path and its log probability\n",
        "\n",
        "def transition_model_maxmul(self, log_alpha):\n",
        "  log_transition_matrix = torch.nn.functional.log_softmax(self.unnormalized_transition_matrix, dim=0)\n",
        "\n",
        "  out1, out2 = maxmul(log_transition_matrix, log_alpha.transpose(0,1))\n",
        "  return out1.transpose(0,1), out2.transpose(0,1)\n",
        "\n",
        "def maxmul(log_A, log_B):\n",
        "\t\"\"\"\n",
        "\tlog_A : m x n\n",
        "\tlog_B : n x p\n",
        "\toutput : m x p matrix\n",
        "\n",
        "\tSimilar to the log domain matrix multiplication,\n",
        "\tthis computes out_{i,j} = max_k log_A_{i,k} + log_B_{k,j}\n",
        "\t\"\"\"\n",
        "\tm = log_A.shape[0]\n",
        "\tn = log_A.shape[1]\n",
        "\tp = log_B.shape[1]\n",
        "\n",
        "\tlog_A_expanded = torch.stack([log_A] * p, dim=2)\n",
        "\tlog_B_expanded = torch.stack([log_B] * m, dim=0)\n",
        "\n",
        "\telementwise_sum = log_A_expanded + log_B_expanded\n",
        "\tout1,out2 = torch.max(elementwise_sum, dim=1)\n",
        "\n",
        "\treturn out1,out2\n",
        "\n",
        "TransitionModel.maxmul = transition_model_maxmul\n",
        "HMM.viterbi = viterbi"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Why do we use log probabilities in Viterbi instead of raw probabilities?**\n",
        "\n",
        "**Significance:**\n",
        "Multiplying many small probabilities quickly underflows to zero.\n",
        "Viterbi uses **addition of log-probs instead of multiplication of probs**, which is numerically stable.\n",
        "\n",
        "**Why not stay in probability space?**\n",
        "\n",
        "* HMM sequences quickly get tiny: (10^{-30}), (10^{-50}), etc.\n",
        "* Logs prevent collapse to zero\n",
        "* Max over log-probs = max over probs (monotonic transformation)\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why does Viterbi use `max` (argmax path) while forward uses `sum` (total probability)?**\n",
        "\n",
        "**Significance:**\n",
        "Forward computes:\n",
        "\n",
        "> **Likelihood of the sequence** over *all* state paths.\n",
        "\n",
        "Viterbi computes:\n",
        "\n",
        "> **Most likely hidden-state path**, not its total probability.\n",
        "\n",
        "**Why not use sum here?**\n",
        "Sum answers *‚ÄúHow likely is the sequence?‚Äù*\n",
        "Max answers *‚ÄúWhich sequence of states best explains it?‚Äù*\n",
        "For decoding ‚Üí max is the correct operation.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why do we use a custom `maxmul()` instead of PyTorch matrix multiply?**\n",
        "\n",
        "**Significance:**\n",
        "Typical matrix multiplication uses:\n",
        "\n",
        "```\n",
        "sum_k A[i,k] * B[k,j]\n",
        "```\n",
        "\n",
        "Viterbi needs:\n",
        "\n",
        "```\n",
        "max_k A[i,k] + B[k,j]\n",
        "```\n",
        "\n",
        "This is **max-plus semiring** (a different algebra).\n",
        "\n",
        "Standard matmul cannot do that.\n",
        "\n",
        "Using a custom `maxmul()` explicitly demonstrates to learners:\n",
        "\n",
        "* HMM algorithms operate in special semirings\n",
        "* Viterbi uses max-plus\n",
        "* Forward uses log-sum-exp\n",
        "\n",
        "This is an educational moment: *same HMM, different algebra ‚Üí different inference*.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why do we store `psi` (the backpointer matrix)?**\n",
        "\n",
        "**Significance:**\n",
        "`psi[t][j]` tells you:\n",
        "\n",
        "> *‚ÄúFrom which previous state did we come to reach state `j` at time `t` with the highest probability?‚Äù*\n",
        "\n",
        "Without `psi`, you can compute the score of the best path, but **not the actual path**.\n",
        "\n",
        "**Why not recompute path afterward?**\n",
        "Because recomputing loses all intermediate decisions, impossible without backpointers.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why do we manually backtrack through time in a loop instead of parallelizing?**\n",
        "\n",
        "**Significance:**\n",
        "Parallelizing backtracking is hard because:\n",
        "\n",
        "* each step depends on the previous state‚Äôs choice\n",
        "* backtracking is inherently sequential\n",
        "  (you can only know the best state at `t-1` after knowing the best at `t`)\n",
        "\n",
        "For teaching purposes, this loop makes the logic transparent.\n",
        "\n",
        "**Why not vectorize anyway?**\n",
        "It complicates the code and hides the conceptual flow.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Why do we apply emission probabilities before transitions at the first step?**\n",
        "\n",
        "Code:\n",
        "\n",
        "```python\n",
        "log_delta[:, 0, :] = emission(x[:,0]) + log_state_priors\n",
        "```\n",
        "\n",
        "**Significance:**\n",
        "At time 0, no transitions have occurred.\n",
        "The best score for being in state `i` is:\n",
        "\n",
        "```\n",
        "P(z0 = i) + P(x0 | zi)\n",
        "```\n",
        "\n",
        "Teaching point: **first step = priors + first observation**.\n",
        "\n",
        "**Why not include transition from a dummy start state?**\n",
        "That‚Äôs equivalent, but increases conceptual overhead for students.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Why does ‚Äúaba‚Äù give a valid path but ‚Äúabb‚Äù fails or gives worse score?**\n",
        "\n",
        "**Significance:**\n",
        "Your HMM enforces alternating vowel/consonant states:\n",
        "\n",
        "```\n",
        "C ‚Üí V ‚Üí C ‚Üí V ...\n",
        "```\n",
        "\n",
        "‚Äúaba‚Äù\n",
        "\n",
        "```\n",
        "a (vowel)  \n",
        "b (consonant)  \n",
        "a (vowel)\n",
        "‚Üí perfectly fits transitions + emissions\n",
        "```\n",
        "\n",
        "‚Äúabb‚Äù\n",
        "\n",
        "```\n",
        "a (vowel)\n",
        "b (consonant)\n",
        "b (consonant) ‚Üí forbidden emission ‚Üí -inf score\n",
        "```\n",
        "\n",
        "Thus Viterbi correctly returns:\n",
        "\n",
        "* a valid state path for ‚Äúaba‚Äù\n",
        "* impossible/low-probability path for ‚Äúabb‚Äù\n",
        "\n",
        "This demonstrates how **HMM constraints produce structured decoding**.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Why is `best_path_scores` taken from the last timestep using gather?**\n",
        "\n",
        "**Significance:**\n",
        "If sequences have different lengths, Viterbi must take:\n",
        "\n",
        "```\n",
        "score of best state at time T[i] - 1\n",
        "```\n",
        "\n",
        "not at `T_max - 1`.\n",
        "\n",
        "This teaches:\n",
        "\n",
        "* HMMs support variable-length sequences\n",
        "* we score only up to the true end, not the padded part\n",
        "\n",
        "**Why not use a fixed-length sequence?**\n",
        "Because batching variable-length sequences is standard in NLP.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Why run Viterbi on a batch (two words at once) instead of one word at a time?**\n",
        "\n",
        "**Significance:**\n",
        "Efficient NLP models process multiple sequences simultaneously.\n",
        "Running:\n",
        "\n",
        "```python\n",
        "[\"aba\", \"abb\"]\n",
        "```\n",
        "\n",
        "together demonstrates:\n",
        "\n",
        "* vectorized HMM inference\n",
        "* shared parameters across sequences\n",
        "* practical batching behavior\n",
        "\n",
        "**Why not run separately?**\n",
        "Less realistic and slower in any real system.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Why return both `(z_star, best_path_scores)` instead of only the path?**\n",
        "\n",
        "**Significance:**\n",
        "Viterbi decoding answers two questions:\n",
        "\n",
        "1. **Which path is most probable?** ‚Üí `z_star`\n",
        "2. **How probable is that path?** ‚Üí `best_path_scores`\n",
        "\n",
        "This matters for:\n",
        "\n",
        "* comparing sequences\n",
        "* confidence scoring\n",
        "* ranking alternative segmentations or spellings\n",
        "\n",
        "**Why not return only the path?**\n",
        "You lose important interpretability and scoring information.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XoIel_Mewoph"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTGOaeXbaWie"
      },
      "source": [
        "Try running Viterbi on an input sequence, given the vowel/consonant HMM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeOTbaIMc23d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819af503-8175-4dcc-dd1a-be9e3b4dcba4"
      },
      "source": [
        "x = torch.stack( [torch.tensor(encode(\"aba\")), torch.tensor(encode(\"abb\"))] )\n",
        "T = torch.tensor([3,3])\n",
        "print(model.viterbi(x, T))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([[1, 0, 1], [1, 0, 0]], tensor([[-5.1112],\n",
            "        [   -inf]], device='cuda:0'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKr8YlafdzBx"
      },
      "source": [
        "For $\\mathbf{x} = \\text{\"aba\"}$, the Viterbi algorithm returns $\\mathbf{z}^* = \\{1,0,1\\}$. This corresponds to \"vowel, consonant, vowel\" according to the way we defined the states above, which is correct for this input sequence. Yay!\n",
        "\n",
        "For $\\mathbf{x} = \\text{\"abb\"}$, the Viterbi algorithm still returns a $\\mathbf{z}^*$, but we know this is gibberish because \"vowel, consonant, consonant\" is impossible under this HMM, and indeed the log probability of this path is $-\\infty$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCWw0_WienO_"
      },
      "source": [
        "Let's compare the \"forward score\" (the log probability of all possible paths, returned by the forward algorithm) with the \"Viterbi score\" (the log probability of the maximum likelihood path, returned by the Viterbi algorithm):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9fBOHvdeqWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5eceef4-71f3-4f4a-d21d-f02dbad05276"
      },
      "source": [
        "print(model.forward(x, T))\n",
        "print(model.viterbi(x, T)[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-5.1112],\n",
            "        [   -inf]], device='cuda:0')\n",
            "tensor([[-5.1112],\n",
            "        [   -inf]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InF6PJVOfHwH"
      },
      "source": [
        "The two scores are the same! That's because in this instance there is only one possible path through the HMM, so the probability of the most likely path is the same as the sum of the probabilities of all possible paths.\n",
        "\n",
        "In general, though, the forward score and Viterbi score will always be somewhat close. This is because of a property of the $\\text{logsumexp}$ function: $\\text{logsumexp}(\\mathbf{x}) \\approx \\max (\\mathbf{x})$. ($\\text{logsumexp}$ is sometimes referred to as the \"smooth maximum\" function.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x__70tB6gnkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad564617-f68b-43f5-cc94-7204f7cf8611"
      },
      "source": [
        "x = torch.tensor([1., 2., 3.])\n",
        "print(x.max(dim=0)[0])\n",
        "print(x.logsumexp(dim=0))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "tensor(3.4076)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvFtiWhzgy0V"
      },
      "source": [
        "### Problem 3: How do we train the model?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3JaykRalSBZ"
      },
      "source": [
        "Earlier, we hard-coded an HMM to have certain behavior. What we would like to do instead is have the HMM learn to model the data on its own. And while it is possible to use supervised learning with an HMM (by hard-coding the emission model or the transition model) so that the states have a particular interpretation, the really cool thing about HMMs is that they are naturally unsupervised learners, so they can learn to use their different states to represent different patterns in the data, without the programmer needing to indicate what each state means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K471fT4N-PR"
      },
      "source": [
        "Like many machine learning models, an HMM can be trained using maximum likelihood estimation, i.e.:\n",
        "\n",
        "$$\\theta^* = \\underset{\\theta}{\\text{argmin }} -\\sum_{\\mathbf{x}^i}\\text{log }p_{\\theta}(\\mathbf{x}^i)$$\n",
        "\n",
        "where $\\mathbf{x}^1, \\mathbf{x}^2, \\dots$ are training examples.\n",
        "\n",
        "The standard method for doing this is the Expectation-Maximization (EM) algorithm, which for HMMs is also called the \"Baum-Welch\" algorithm. In EM training, we alternate between an \"E-step\", where we estimate the values of the latent variables, and an \"M-step\", where the model parameters are updated given the estimated latent variables. (Think $k$-means, where you guess which cluster each data point belongs to, then reestimate where the clusters are, and repeat.) The EM algorithm has some nice properties: it is guaranteed at each step to decrease the loss function, and the E-step and M-step may have an exact closed form solution, in which case no pesky learning rates are required.\n",
        "\n",
        "But because the HMM forward algorithm is differentiable with respect to all the model parameters, we can also just take advantage of automatic differentiation methods in libraries like PyTorch and try to minimize $-\\text{log }p_{\\theta}(\\mathbf{x})$ directly, by backpropagating through the forward algorithm and running stochastic gradient descent. That means we don't need to write any additional HMM code to implement training: `loss.backward()` is all you need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVh0-369qZDC"
      },
      "source": [
        "Here we will implement SGD training for an HMM in PyTorch. First, some helper classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqiFobGHwdzc"
      },
      "source": [
        "import torch.utils.data\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, lines):\n",
        "    self.lines = lines # list of strings\n",
        "    collate = Collate() # function for generating a minibatch from strings\n",
        "    self.loader = torch.utils.data.DataLoader(self, batch_size=1024, num_workers=1, shuffle=True, collate_fn=collate)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    line = self.lines[idx].lstrip(\" \").rstrip(\"\\n\").rstrip(\" \").rstrip(\"\\n\")\n",
        "    return line\n",
        "\n",
        "class Collate:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    \"\"\"\n",
        "    Returns a minibatch of strings, padded to have the same length.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    batch_size = len(batch)\n",
        "    for index in range(batch_size):\n",
        "      x_ = batch[index]\n",
        "\n",
        "      # convert letters to integers\n",
        "      x.append(encode(x_))\n",
        "\n",
        "    # pad all sequences with 0 to have same length\n",
        "    x_lengths = [len(x_) for x_ in x]\n",
        "    T = max(x_lengths)\n",
        "    for index in range(batch_size):\n",
        "      x[index] += [0] * (T - len(x[index]))\n",
        "      x[index] = torch.tensor(x[index])\n",
        "\n",
        "    # stack into single tensor\n",
        "    x = torch.stack(x)\n",
        "    x_lengths = torch.tensor(x_lengths)\n",
        "    return (x,x_lengths)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDpwnPnAEA9"
      },
      "source": [
        "Let's load some training/testing data. By default, this will use the unix \"words\" file, but you could also use your own text file."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **1. Why do we print both `model.forward(x, T)` and `model.viterbi(x, T)[1]`?**\n",
        "\n",
        "### **Significance (HMM Concept)**\n",
        "\n",
        "You are demonstrating that:\n",
        "\n",
        "* **Forward algorithm** computes\n",
        "  ‚Üí *total probability of all state paths producing the sequence*.\n",
        "\n",
        "* **Viterbi algorithm** computes\n",
        "  ‚Üí *probability of only the best single state path*.\n",
        "\n",
        "So the forward log-probability is always **‚â•** the Viterbi log-score (log-sum-exp ‚â• max).\n",
        "Printing them together visually proves this property.\n",
        "\n",
        "### **Why not print only one?**\n",
        "\n",
        "Because students often confuse ‚Äúlikelihood of the sequence‚Äù with ‚Äúlikelihood of the best path.‚Äù\n",
        "Seeing both reinforces the conceptual distinction.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Why do we compute both `x.max(dim=0)` and `x.logsumexp(dim=0)`?**\n",
        "\n",
        "### **Significance (HMM Math Insight)**\n",
        "\n",
        "You are demonstrating the key algebraic difference:\n",
        "\n",
        "* Viterbi uses **max** (max-plus semiring)\n",
        "* Forward uses **logsumexp** (sum-product semiring)\n",
        "\n",
        "This simple vector example:\n",
        "\n",
        "```python\n",
        "x = [1, 2, 3]\n",
        "```\n",
        "\n",
        "lets students see:\n",
        "\n",
        "* Max = 3\n",
        "* Log-sum-exp = log(e¬π + e¬≤ + e¬≥) > 3\n",
        "\n",
        "This illustrates why forward scores > Viterbi scores always.\n",
        "\n",
        "### **Why not explain using only theory?**\n",
        "\n",
        "This tiny concrete example makes the connection obvious and intuitive.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Why do we pad sequences inside `Collate()` instead of letting PyTorch handle variable lengths automatically?**\n",
        "\n",
        "### **Significance (HMM Implementation Detail)**\n",
        "\n",
        "Your HMM implementation expects a tensor shaped:\n",
        "\n",
        "```\n",
        "batch_size √ó T_max\n",
        "```\n",
        "\n",
        "because:\n",
        "\n",
        "* Forward algorithm needs uniform time steps\n",
        "* Viterbi needs a consistent DP table shape\n",
        "* You manually track the true lengths via `x_lengths`\n",
        "\n",
        "### **Why not use PyTorch‚Äôs PackedSequence / RNN utilities?**\n",
        "\n",
        "Because this is a *hand-built* HMM, not an RNN.\n",
        "Packing/unpacking is unnecessary and would hide important details from students.\n",
        "\n",
        "Padding + explicit lengths is the standard method in *classic HMM implementations*.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Why does `Collate()` both pad **and** return `x_lengths` (T)?**\n",
        "\n",
        "### **Significance (HMM Requirement)**\n",
        "\n",
        "Even though you pad sequences, the forward and Viterbi algorithms must only use the true length.\n",
        "\n",
        "If you didn‚Äôt pass `T`, the model would try to interpret padded zeros as real emissions, which ruins:\n",
        "\n",
        "* log-probabilities\n",
        "* state decoding\n",
        "* evaluation results\n",
        "\n",
        "### **Why not mask instead of using `T`?**\n",
        "\n",
        "Masking complicates the code.\n",
        "Using `T` keeps the algorithms clean:\n",
        "\n",
        "* forward ‚Üí `log_probs = torch.gather(..., T-1)`\n",
        "* Viterbi ‚Üí backtracking starts at `T-1`\n",
        "\n",
        "This is the simplest correct approach.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Why does `__getitem__` strip whitespace before encoding the line?**\n",
        "\n",
        "### **Significance**\n",
        "\n",
        "Many datasets contain:\n",
        "\n",
        "* leading spaces\n",
        "* trailing spaces\n",
        "* stray newline characters\n",
        "\n",
        "If you don‚Äôt strip:\n",
        "\n",
        "* HMM will treat spaces as extra tokens\n",
        "* encoded sequences will be wrong\n",
        "* emissions matrices become polluted\n",
        "\n",
        "### **Why not let the encoder ignore unknown chars?**\n",
        "\n",
        "Encoding unknown characters silently hides mistakes.\n",
        "Stripping ensures clean, consistent training data.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Why stack two sequences (`aba`, `abb`) into the same batch?**\n",
        "\n",
        "### **Significance**\n",
        "\n",
        "Batching demonstrates how HMMs scale.\n",
        "Your forward and Viterbi implementations are **vectorized across batch dimension**, so they:\n",
        "\n",
        "* share transition/emission parameters\n",
        "* compute in parallel\n",
        "* mimic real HMM usage in NLP tasks\n",
        "\n",
        "### **Why not run sequences independently?**\n",
        "\n",
        "It hides the efficiency benefits of HMMs and breaks the demo‚Äôs focus on batching.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Why is padding done using the integer `0` specifically?**\n",
        "\n",
        "### **Significance**\n",
        "\n",
        "`0` corresponds to letter `'a'` in your alphabet.\n",
        "But in HMM training, that doesn‚Äôt matter because:\n",
        "\n",
        "* you always **mask out padded positions using `T`**\n",
        "* forward and Viterbi never look at padded timesteps\n",
        "* emissions for the padded section are irrelevant\n",
        "\n",
        "### **Why not use a special PAD token?**\n",
        "\n",
        "Because the model never actually reads padded timesteps‚Äî`T` handles it.\n",
        "This avoids enlarging the alphabet.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Why is the `TextDataset` holding a `DataLoader` inside it?**\n",
        "\n",
        "### **Significance**\n",
        "\n",
        "This design makes your training loop extremely simple:\n",
        "\n",
        "```python\n",
        "for x, T in dataset.loader:\n",
        "    ...\n",
        "```\n",
        "\n",
        "It packages:\n",
        "\n",
        "* the data\n",
        "* batching\n",
        "* padding\n",
        "* encoding\n",
        "\n",
        "into a single object.\n",
        "\n",
        "### **Why not put the DataLoader outside?**\n",
        "\n",
        "For a classroom demo, embedding it inside keeps code clean and easy to follow.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Why do we use `Counter` and train/test split but not shown in this snippet?**\n",
        "\n",
        "### **Significance**\n",
        "\n",
        "You are preparing the dataset to later show:\n",
        "\n",
        "* distribution of characters\n",
        "* splitting of corpus into training/testing for likelihood evaluation\n",
        "* classic NLP preprocessing\n",
        "\n",
        "This lays groundwork for:\n",
        "\n",
        "* learning HMM parameters\n",
        "* evaluating perplexity\n",
        "* computing held-out likelihood\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Why are we printing outputs after each step in this notebook-style code?**\n",
        "\n",
        "### **Significance**\n",
        "\n",
        "This is a **didactic style**:\n",
        "\n",
        "* Show intermediate numeric results\n",
        "* Reinforce understanding of each algorithmic step\n",
        "* Compare max vs log-sum-exp\n",
        "* Compare forward vs Viterbi\n",
        "* See batching effects directly\n",
        "\n",
        "### **Why not wrap everything in functions and hide output?**\n",
        "\n",
        "Because transparency is crucial for teaching how HMM inference works.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9JaopnSwxRO2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52NqFHg8ANsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a1e2be-7403-4dae-a33b-68038bb78f4e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/lorenlugosch/pytorch_HMM/master/data/train/training.txt\n",
        "\n",
        "filename = \"training.txt\"\n",
        "\n",
        "with open(filename, \"r\") as f:\n",
        "  lines = f.readlines() # each line of lines will have one word\n",
        "\n",
        "alphabet = list(Counter((\"\".join(lines))).keys())\n",
        "train_lines, valid_lines = train_test_split(lines, test_size=0.1, random_state=42)\n",
        "train_dataset = TextDataset(train_lines)\n",
        "valid_dataset = TextDataset(valid_lines)\n",
        "\n",
        "M = len(alphabet)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-23 04:43:58--  https://raw.githubusercontent.com/lorenlugosch/pytorch_HMM/master/data/train/training.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2493109 (2.4M) [text/plain]\n",
            "Saving to: ‚Äòtraining.txt.1‚Äô\n",
            "\n",
            "training.txt.1      100%[===================>]   2.38M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-11-23 04:43:58 (66.3 MB/s) - ‚Äòtraining.txt.1‚Äô saved [2493109/2493109]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0AqmyrK7IUn"
      },
      "source": [
        "We will use a Trainer class for training and testing the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iypy_neX9cpq"
      },
      "source": [
        "from tqdm import tqdm # for displaying progress bar\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, lr):\n",
        "    self.model = model\n",
        "    self.lr = lr\n",
        "    self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=0.00001)\n",
        "\n",
        "  def train(self, dataset):\n",
        "    train_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.train()\n",
        "    print_interval = 50\n",
        "    for idx, batch in enumerate(tqdm(dataset.loader)):\n",
        "      x,T = batch\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      log_probs = self.model(x,T)\n",
        "      loss = -log_probs.mean()\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "      train_loss += loss.cpu().data.numpy().item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        print(\"loss:\", loss.item())\n",
        "        for _ in range(5):\n",
        "          sampled_x, sampled_z = self.model.sample()\n",
        "          print(decode(sampled_x))\n",
        "          print(sampled_z)\n",
        "    train_loss /= num_samples\n",
        "    return train_loss\n",
        "\n",
        "  def test(self, dataset):\n",
        "    test_loss = 0\n",
        "    num_samples = 0\n",
        "    self.model.eval()\n",
        "    print_interval = 50\n",
        "    for idx, batch in enumerate(dataset.loader):\n",
        "      x,T = batch\n",
        "      batch_size = len(x)\n",
        "      num_samples += batch_size\n",
        "      log_probs = self.model(x,T)\n",
        "      loss = -log_probs.mean()\n",
        "      test_loss += loss.cpu().data.numpy().item() * batch_size\n",
        "      if idx % print_interval == 0:\n",
        "        print(\"loss:\", loss.item())\n",
        "        sampled_x, sampled_z = self.model.sample()\n",
        "        print(decode(sampled_x))\n",
        "        print(sampled_z)\n",
        "    test_loss /= num_samples\n",
        "    return test_loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUR8qbHm9dMg"
      },
      "source": [
        "Finally, initialize the model and run the main training loop. Every 50 batches, the code will produce a few samples from the model. Over time, these samples should look more and more realistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-NGIK1Q9g2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c0139a-3274-4326-b815-bc1d60686942"
      },
      "source": [
        "# Initialize model\n",
        "model = HMM(N=64, M=M)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "trainer = Trainer(model, lr=0.01)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "        print(\"========= Epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
        "        train_loss = trainer.train(train_dataset)\n",
        "        valid_loss = trainer.test(valid_dataset)\n",
        "\n",
        "        print(\"========= Results: epoch %d of %d =========\" % (epoch+1, num_epochs))\n",
        "        print(\"train loss: %.2f| valid loss: %.2f\\n\" % (train_loss, valid_loss) )"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= Epoch 1 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:40,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 38.02503204345703\n",
            "FbYkdNLhKk\n",
            "[1, 51, 8, 61, 49, 35, 22, 63, 28, 12]\n",
            "YlqRm-yXBU\n",
            "[54, 26, 27, 28, 22, 40, 32, 4, 51, 44]\n",
            "ZXnvXoCQDz\n",
            "[22, 43, 42, 58, 4, 39, 9, 41, 13, 46]\n",
            "eeWqolQGVN\n",
            "[52, 44, 25, 31, 40, 0, 31, 51, 51, 37]\n",
            "DWUYxpvudP\n",
            "[61, 46, 44, 34, 33, 33, 31, 14, 48, 37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 16.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 32.93939971923828\n",
            "UgoasnNGni\n",
            "[1, 50, 32, 42, 20, 32, 20, 16, 42, 42]\n",
            "\n",
            "piLespevY\n",
            "[2, 43, 61, 22, 7, 56, 14, 36, 17, 20]\n",
            "paieohscKs\n",
            "[47, 49, 2, 40, 2, 6, 26, 32, 18, 55]\n",
            "BNNH-eFaTK\n",
            "[30, 4, 17, 30, 37, 15, 33, 0, 28, 53]\n",
            "tnosAtoagj\n",
            "[47, 2, 62, 26, 19, 3, 7, 56, 55, 10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:06, 16.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 30.032291412353516\n",
            "XghquxnVtH\n",
            "[23, 38, 59, 48, 15, 32, 36, 17, 44, 9]\n",
            "bncgeQsmJr\n",
            "[36, 17, 34, 50, 7, 16, 14, 58, 17, 40]\n",
            "moilcpaWiL\n",
            "[15, 35, 17, 46, 34, 43, 33, 9, 4, 22]\n",
            "UEqvPAxOel\n",
            "[63, 49, 33, 40, 16, 2, 50, 35, 16, 45]\n",
            "-asTiupmTa\n",
            "[1, 27, 10, 33, 0, 35, 18, 41, 46, 42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 16.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 28.68271827697754\n",
            "Lfiptndabn\n",
            "[47, 15, 26, 33, 40, 32, 48, 37, 26, 19]\n",
            "Cvlslppmmy\n",
            "[58, 14, 35, 26, 4, 58, 14, 45, 29, 17]\n",
            "oeoeesnono\n",
            "[42, 7, 45, 43, 15, 26, 42, 61, 47, 15]\n",
            "mJsyhzvelC\n",
            "[54, 61, 26, 32, 44, 32, 15, 26, 29, 33]\n",
            "permipisuM\n",
            "[47, 58, 27, 34, 7, 20, 7, 48, 15, 26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 16.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.962074279785156\n",
            "ZaEeesatic\n",
            "[4, 16, 61, 21, 44, 26, 32, 36, 7, 47]\n",
            "mrelesBLni\n",
            "[4, 58, 15, 42, 7, 26, 45, 43, 42, 7]\n",
            "ezihotavib\n",
            "[15, 34, 16, 36, 17, 40, 16, 36, 17, 31]\n",
            "dooelGLana\n",
            "[48, 15, 15, 15, 26, 45, 43, 16, 42, 31]\n",
            "vuhaQaiono\n",
            "[54, 2, 43, 58, 36, 17, 47, 15, 42, 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 27.272933959960938\n",
            "inrhiiteae\n",
            "[54, 2, 40, 43, 17, 17, 45, 43, 17, 36]\n",
            "========= Results: epoch 1 of 10 =========\n",
            "train loss: 30.86| valid loss: 26.82\n",
            "\n",
            "========= Epoch 2 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:38,  5.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 26.66340446472168\n",
            "iseeidanra\n",
            "[16, 48, 7, 50, 7, 0, 16, 42, 40, 16]\n",
            "cjulnmngol\n",
            "[4, 13, 35, 36, 17, 36, 17, 50, 15, 42]\n",
            "unotrmytrn\n",
            "[54, 35, 17, 40, 16, 36, 17, 40, 43, 48]\n",
            "nosgcosavo\n",
            "[42, 7, 26, 20, 48, 15, 26, 16, 36, 17]\n",
            "boLaniarlc\n",
            "[47, 7, 40, 32, 29, 17, 40, 35, 36, 36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 16.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 25.913742065429688\n",
            "ecityzlyth\n",
            "[15, 36, 17, 40, 32, 36, 36, 17, 40, 43]\n",
            "cxoathoadh\n",
            "[47, 15, 35, 16, 40, 43, 32, 61, 48, 43]\n",
            "parialrtoa\n",
            "[30, 44, 35, 17, 16, 36, 17, 40, 32, 58]\n",
            "orwranlhea\n",
            "[58, 34, 53, 35, 16, 2, 1, 45, 15, 16]\n",
            "uoargrsssi\n",
            "[54, 2, 17, 2, 50, 15, 26, 26, 26, 32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:07, 13.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 25.620635986328125\n",
            "Rorastitan\n",
            "[4, 15, 35, 17, 26, 29, 17, 40, 32, 2]\n",
            "nrlamlleri\n",
            "[47, 35, 36, 16, 36, 36, 36, 7, 35, 32]\n",
            "armlitooes\n",
            "[16, 42, 12, 36, 17, 40, 15, 16, 26, 26]\n",
            "QchklacoJc\n",
            "[20, 47, 43, 32, 36, 17, 40, 32, 16, 40]\n",
            "sngrawvyan\n",
            "[54, 2, 50, 36, 16, 36, 36, 17, 16, 42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 16.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.614309310913086\n",
            "oemineabin\n",
            "[48, 15, 34, 33, 50, 7, 16, 36, 7, 36]\n",
            "melibcnvap\n",
            "[4, 15, 35, 17, 36, 7, 2, 48, 17, 29]\n",
            "demespleni\n",
            "[48, 15, 34, 7, 26, 45, 43, 15, 35, 32]\n",
            "fodnGdtame\n",
            "[48, 15, 2, 50, 35, 38, 40, 16, 34, 16]\n",
            "losclonyna\n",
            "[36, 17, 16, 42, 4, 16, 36, 17, 2, 16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 16.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.512184143066406\n",
            "mogatriurm\n",
            "[4, 17, 50, 17, 40, 43, 17, 15, 35, 36]\n",
            "bpytessyca\n",
            "[39, 36, 17, 40, 7, 26, 29, 17, 40, 17]\n",
            "salevessti\n",
            "[4, 16, 42, 7, 48, 7, 26, 26, 29, 17]\n",
            "ruarcyxteo\n",
            "[48, 32, 16, 24, 37, 61, 42, 40, 15, 35]\n",
            "Boliniolat\n",
            "[4, 16, 36, 17, 36, 7, 58, 36, 17, 40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.574323654174805\n",
            "tolotealar\n",
            "[47, 16, 36, 17, 40, 7, 26, 4, 16, 35]\n",
            "========= Results: epoch 2 of 10 =========\n",
            "train loss: 25.44| valid loss: 24.76\n",
            "\n",
            "========= Epoch 3 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.371315002441406\n",
            "Novinollis\n",
            "[4, 16, 36, 17, 40, 15, 35, 36, 7, 26]\n",
            "tagkatiaac\n",
            "[47, 16, 50, 43, 16, 40, 32, 16, 42, 40]\n",
            "ungoramyom\n",
            "[54, 2, 50, 15, 35, 16, 36, 17, 16, 36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|‚ñè         | 3/208 [00:00<00:20, 10.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oneonubnyo\n",
            "[58, 36, 7, 16, 2, 16, 25, 36, 17, 16]\n",
            "caresmolef\n",
            "[47, 32, 48, 7, 26, 29, 15, 35, 32, 42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 15.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.66459083557129\n",
            "prortanari\n",
            "[47, 35, 16, 35, 4, 16, 36, 16, 35, 17]\n",
            "aplesslete\n",
            "[16, 25, 36, 7, 26, 29, 36, 7, 40, 7]\n",
            "pocesperos\n",
            "[47, 16, 40, 7, 26, 29, 15, 35, 16, 26]\n",
            "Herothaouo\n",
            "[18, 15, 35, 16, 40, 43, 16, 11, 44, 16]\n",
            "maceaagipo\n",
            "[4, 16, 40, 43, 17, 16, 50, 17, 45, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:07, 13.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.52764892578125\n",
            "stlebetarg\n",
            "[26, 29, 36, 7, 50, 32, 40, 16, 35, 50]\n",
            "unredmedrs\n",
            "[54, 2, 48, 7, 0, 36, 7, 0, 35, 48]\n",
            "pecsaLogle\n",
            "[4, 16, 42, 40, 16, 54, 2, 50, 36, 7]\n",
            "lolbyprini\n",
            "[4, 16, 34, 39, 61, 14, 43, 16, 42, 32]\n",
            "dedosirine\n",
            "[48, 7, 0, 16, 42, 7, 35, 17, 42, 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 15.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.304121017456055\n",
            "mentruresy\n",
            "[4, 7, 42, 40, 43, 15, 35, 7, 48, 17]\n",
            "tumburatur\n",
            "[47, 15, 34, 4, 46, 35, 17, 40, 15, 35]\n",
            "vetionirit\n",
            "[4, 7, 40, 32, 16, 34, 17, 36, 17, 40]\n",
            "eyssunthur\n",
            "[4, 17, 26, 29, 15, 42, 40, 32, 16, 35]\n",
            "thytablest\n",
            "[47, 43, 17, 45, 16, 25, 36, 7, 26, 29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 16.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.471256256103516\n",
            "lycarshoge\n",
            "[43, 17, 45, 16, 42, 40, 43, 17, 50, 7]\n",
            "Anarcastda\n",
            "[54, 2, 16, 42, 40, 17, 26, 29, 48, 17]\n",
            "ssmedrozhi\n",
            "[26, 29, 57, 46, 0, 35, 17, 40, 43, 17]\n",
            "tobivedyce\n",
            "[47, 16, 40, 32, 48, 7, 36, 17, 40, 7]\n",
            "emnetrarol\n",
            "[20, 34, 13, 21, 40, 43, 16, 35, 16, 36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.334529876708984\n",
            "cefllwassg\n",
            "[4, 7, 0, 36, 36, 36, 17, 26, 29, 15]\n",
            "========= Results: epoch 3 of 10 =========\n",
            "train loss: 24.46| valid loss: 24.28\n",
            "\n",
            "========= Epoch 4 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:39,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.5795955657959\n",
            "antedlidoc\n",
            "[16, 42, 40, 7, 0, 36, 17, 48, 17, 45]\n",
            "flaucthoas\n",
            "[21, 36, 7, 46, 42, 40, 43, 16, 46, 26]\n",
            "saronlotis\n",
            "[47, 33, 35, 16, 36, 36, 17, 40, 32, 26]\n",
            "tolelosuca\n",
            "[47, 16, 36, 7, 35, 16, 26, 26, 45, 16]\n",
            "batiglunes\n",
            "[4, 17, 40, 17, 50, 36, 17, 36, 17, 26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 16.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.024890899658203\n",
            "IrphifonGe\n",
            "[21, 35, 47, 43, 32, 48, 15, 42, 40, 32]\n",
            "odeneredlo\n",
            "[16, 48, 32, 48, 15, 35, 7, 0, 35, 16]\n",
            "scotodatis\n",
            "[26, 45, 16, 40, 16, 48, 17, 40, 32, 26]\n",
            "oYtvetliss\n",
            "[16, 42, 40, 36, 17, 40, 43, 32, 26, 29]\n",
            "diwnislaph\n",
            "[48, 17, 25, 36, 17, 45, 43, 16, 45, 43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:06, 15.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.986164093017578\n",
            "insjormnos\n",
            "[54, 2, 26, 29, 15, 35, 57, 36, 17, 29]\n",
            "merenpissa\n",
            "[4, 7, 35, 16, 2, 45, 17, 26, 29, 16]\n",
            "dkectrrphr\n",
            "[60, 12, 7, 42, 40, 43, 35, 47, 43, 35]\n",
            "teumiriolm\n",
            "[47, 15, 54, 34, 7, 35, 17, 16, 36, 36]\n",
            "myxatemesh\n",
            "[4, 61, 34, 17, 40, 15, 34, 7, 26, 29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 16.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.548397064208984\n",
            "terideotih\n",
            "[4, 15, 35, 32, 48, 15, 16, 40, 32, 48]\n",
            "stydesmacb\n",
            "[26, 29, 32, 36, 7, 26, 29, 16, 42, 55]\n",
            "tolciuscur\n",
            "[4, 15, 35, 45, 16, 46, 26, 45, 15, 35]\n",
            "thiontical\n",
            "[47, 43, 32, 16, 42, 40, 32, 45, 33, 35]\n",
            "ritrinsmss\n",
            "[4, 17, 40, 6, 32, 2, 26, 29, 10, 42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 16.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.776012420654297\n",
            "chesordond\n",
            "[47, 43, 7, 29, 15, 35, 0, 16, 2, 0]\n",
            "whubhavion\n",
            "[47, 43, 46, 0, 43, 16, 48, 32, 16, 2]\n",
            "lonvithysa\n",
            "[4, 16, 2, 48, 17, 40, 6, 61, 29, 16]\n",
            "franouscar\n",
            "[47, 43, 17, 2, 16, 46, 26, 45, 16, 35]\n",
            "denranflti\n",
            "[48, 7, 42, 40, 16, 2, 47, 43, 40, 32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.0762939453125\n",
            "pslaggandb\n",
            "[26, 29, 36, 17, 50, 50, 16, 2, 0, 53]\n",
            "========= Results: epoch 4 of 10 =========\n",
            "train loss: 24.06| valid loss: 24.00\n",
            "\n",
            "========= Epoch 5 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:40,  5.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.225631713867188\n",
            "ratarobned\n",
            "[36, 17, 40, 16, 35, 16, 25, 36, 7, 0]\n",
            "alyceangha\n",
            "[16, 36, 61, 45, 15, 16, 2, 50, 43, 17]\n",
            "orthauntra\n",
            "[16, 42, 40, 43, 16, 46, 42, 40, 43, 17]\n",
            "luntiiutas\n",
            "[4, 46, 42, 40, 32, 16, 46, 29, 17, 26]\n",
            "cinanvitio\n",
            "[4, 32, 2, 16, 2, 48, 32, 40, 32, 16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñç       | 51/208 [00:03<00:13, 11.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.670207977294922\n",
            "destlertin\n",
            "[48, 17, 26, 29, 36, 7, 42, 40, 32, 36]\n",
            "suspiragga\n",
            "[4, 46, 26, 29, 32, 48, 17, 50, 50, 33]\n",
            "skideadeis\n",
            "[26, 29, 32, 48, 15, 17, 40, 43, 32, 26]\n",
            "relataremo\n",
            "[4, 32, 36, 17, 40, 16, 48, 15, 34, 17]\n",
            "yonenepled\n",
            "[4, 16, 2, 7, 48, 7, 0, 36, 7, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 102/208 [00:06<00:06, 15.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.64531898498535\n",
            "phauctysat\n",
            "[47, 43, 58, 46, 42, 40, 61, 29, 17, 40]\n",
            "osmeansmet\n",
            "[16, 26, 29, 15, 16, 2, 26, 29, 17, 40]\n",
            "cosresWoga\n",
            "[47, 16, 50, 43, 7, 26, 29, 16, 50, 17]\n",
            "Mortyginys\n",
            "[47, 15, 35, 29, 17, 50, 17, 36, 61, 29]\n",
            "stdertodis\n",
            "[26, 29, 48, 7, 42, 40, 16, 0, 32, 26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 154/208 [00:09<00:03, 16.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 24.15966033935547\n",
            "astoncurin\n",
            "[17, 26, 29, 16, 2, 45, 33, 35, 16, 36]\n",
            "derouslioZ\n",
            "[48, 15, 35, 58, 46, 29, 36, 32, 16, 7]\n",
            "inilinansp\n",
            "[54, 2, 32, 36, 17, 2, 16, 42, 26, 29]\n",
            "jentianten\n",
            "[27, 7, 42, 40, 32, 16, 42, 40, 15, 42]\n",
            "calenicife\n",
            "[47, 33, 36, 7, 42, 32, 45, 32, 18, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 202/208 [00:12<00:00, 12.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.66124725341797\n",
            "Cantivenoa\n",
            "[47, 16, 42, 40, 32, 48, 7, 42, 32, 16]\n",
            "ciphuostan\n",
            "[4, 20, 14, 43, 44, 17, 26, 40, 16, 2]\n",
            "micardager\n",
            "[4, 32, 45, 33, 35, 0, 17, 50, 7, 35]\n",
            "daneenisma\n",
            "[4, 16, 2, 32, 7, 42, 32, 26, 29, 16]\n",
            "ingeonciph\n",
            "[54, 2, 50, 32, 16, 42, 40, 32, 14, 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:13<00:00, 15.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.87270736694336\n",
            "rancsouscu\n",
            "[4, 16, 2, 40, 43, 58, 46, 26, 45, 33]\n",
            "========= Results: epoch 5 of 10 =========\n",
            "train loss: 23.85| valid loss: 23.80\n",
            "\n",
            "========= Epoch 6 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:40,  5.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.867509841918945\n",
            "aussanicke\n",
            "[16, 46, 26, 29, 16, 42, 32, 45, 12, 7]\n",
            "ungananges\n",
            "[54, 2, 50, 16, 48, 16, 2, 50, 7, 26]\n",
            "pessorchan\n",
            "[4, 7, 26, 29, 15, 35, 9, 6, 16, 42]\n",
            "wertinesho\n",
            "[4, 7, 42, 40, 32, 48, 7, 26, 29, 16]\n",
            "phanedicat\n",
            "[47, 43, 16, 36, 7, 0, 32, 45, 33, 40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:10, 14.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.760662078857422\n",
            "wacelentos\n",
            "[47, 16, 42, 7, 36, 7, 42, 48, 16, 42]\n",
            "nerivancec\n",
            "[48, 15, 35, 17, 48, 16, 42, 40, 7, 47]\n",
            "adisitynlo\n",
            "[16, 0, 32, 48, 32, 40, 61, 36, 36, 58]\n",
            "musquenani\n",
            "[4, 46, 26, 30, 44, 7, 42, 33, 36, 17]\n",
            "teleciabne\n",
            "[47, 16, 36, 7, 42, 32, 16, 25, 36, 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:06, 16.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.685054779052734\n",
            "whhumoworf\n",
            "[47, 6, 43, 37, 34, 16, 22, 16, 35, 48]\n",
            "elidicaciv\n",
            "[7, 35, 32, 48, 32, 45, 33, 40, 32, 48]\n",
            "palderatyn\n",
            "[47, 33, 35, 0, 15, 35, 17, 40, 61, 36]\n",
            "mustaiflen\n",
            "[4, 46, 26, 40, 58, 32, 18, 36, 7, 42]\n",
            "taliteleno\n",
            "[4, 16, 36, 17, 40, 15, 35, 7, 42, 32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 15.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.92264175415039\n",
            "dotrodumes\n",
            "[47, 16, 40, 43, 58, 48, 33, 36, 7, 26]\n",
            "Memmasseor\n",
            "[4, 20, 34, 34, 17, 26, 29, 15, 16, 42]\n",
            "muboeunomo\n",
            "[4, 46, 39, 47, 1, 54, 2, 16, 34, 16]\n",
            "castoydelz\n",
            "[47, 16, 42, 40, 58, 32, 48, 15, 35, 48]\n",
            "cauntinelt\n",
            "[4, 16, 46, 42, 40, 32, 48, 15, 2, 40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 15.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.52283477783203\n",
            "lerpizetis\n",
            "[4, 15, 35, 29, 32, 48, 7, 40, 32, 26]\n",
            "gecatiessr\n",
            "[47, 15, 45, 33, 40, 17, 7, 26, 29, 15]\n",
            "Viconsescu\n",
            "[4, 32, 45, 16, 2, 48, 7, 26, 47, 15]\n",
            "pocermazeb\n",
            "[47, 16, 40, 15, 35, 49, 16, 36, 7, 13]\n",
            "unemlyhrxe\n",
            "[54, 2, 20, 34, 36, 61, 29, 35, 34, 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.63994026184082\n",
            "lexdigilid\n",
            "[4, 7, 42, 48, 17, 50, 32, 36, 32, 48]\n",
            "========= Results: epoch 6 of 10 =========\n",
            "train loss: 23.67| valid loss: 23.64\n",
            "\n",
            "========= Epoch 7 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.194503784179688\n",
            "apertortan\n",
            "[16, 14, 15, 35, 29, 16, 42, 40, 16, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|‚ñè         | 3/208 [00:00<00:23,  8.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bounchaler\n",
            "[47, 1, 54, 2, 45, 6, 16, 36, 7, 42]\n",
            "cobakeshic\n",
            "[4, 16, 13, 17, 12, 7, 26, 29, 32, 45]\n",
            "quicanlyou\n",
            "[30, 44, 32, 45, 33, 36, 36, 61, 58, 46]\n",
            "iucrubnest\n",
            "[54, 2, 47, 43, 17, 25, 36, 7, 26, 29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 16.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.39376449584961\n",
            "angralitas\n",
            "[16, 2, 50, 43, 16, 36, 17, 40, 17, 26]\n",
            "doveftzele\n",
            "[4, 16, 48, 7, 18, 32, 48, 7, 36, 7]\n",
            "unacarogdo\n",
            "[54, 2, 17, 45, 33, 35, 58, 50, 48, 33]\n",
            "rermilymed\n",
            "[4, 15, 35, 49, 16, 36, 61, 29, 15, 0]\n",
            "ivertrogan\n",
            "[54, 48, 15, 35, 40, 43, 58, 50, 16, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:06, 16.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.470476150512695\n",
            "frekeshtob\n",
            "[47, 43, 17, 12, 7, 26, 6, 40, 33, 25]\n",
            "bongetesti\n",
            "[47, 16, 2, 50, 7, 40, 7, 26, 40, 32]\n",
            "cuilnyphda\n",
            "[30, 44, 32, 36, 36, 61, 14, 6, 48, 16]\n",
            "mirpeblyge\n",
            "[4, 20, 34, 14, 17, 25, 36, 61, 50, 7]\n",
            "holymaldic\n",
            "[4, 16, 36, 61, 34, 16, 36, 48, 32, 45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 16.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.511812210083008\n",
            "iltetatist\n",
            "[54, 2, 29, 17, 40, 17, 40, 32, 26, 29]\n",
            "ipheurogym\n",
            "[20, 14, 6, 58, 46, 42, 58, 50, 61, 34]\n",
            "praminesso\n",
            "[47, 43, 20, 34, 32, 48, 7, 26, 29, 16]\n",
            "Slhoneceda\n",
            "[21, 9, 6, 16, 48, 7, 42, 16, 0, 16]\n",
            "gritidamag\n",
            "[50, 43, 17, 40, 32, 48, 16, 34, 16, 50]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 14.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.536495208740234\n",
            "gixberemer\n",
            "[4, 20, 34, 13, 15, 35, 58, 34, 15, 35]\n",
            "Ghedasereb\n",
            "[47, 6, 7, 0, 17, 40, 15, 35, 17, 25]\n",
            "rodiectora\n",
            "[4, 16, 48, 7, 16, 42, 40, 16, 35, 16]\n",
            "eneanienzi\n",
            "[7, 42, 7, 16, 42, 32, 16, 2, 48, 32]\n",
            "culsenglel\n",
            "[47, 54, 42, 40, 15, 2, 50, 36, 7, 36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.625118255615234\n",
            "vophtiaakl\n",
            "[4, 58, 14, 6, 40, 32, 48, 17, 12, 36]\n",
            "========= Results: epoch 7 of 10 =========\n",
            "train loss: 23.53| valid loss: 23.52\n",
            "\n",
            "========= Epoch 8 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:56,  3.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.70162582397461\n",
            "cactearsin\n",
            "[47, 16, 42, 40, 7, 33, 35, 29, 32, 48]\n",
            "nestedfale\n",
            "[4, 7, 26, 40, 15, 0, 18, 33, 35, 7]\n",
            "hitillabru\n",
            "[6, 32, 40, 32, 36, 36, 17, 13, 43, 46]\n",
            "eneshenter\n",
            "[58, 48, 7, 26, 6, 7, 42, 40, 15, 35]\n",
            "diccuzalop\n",
            "[4, 32, 45, 40, 32, 48, 33, 36, 58, 14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 16.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.505718231201172\n",
            "thoscedert\n",
            "[9, 6, 16, 26, 40, 15, 0, 15, 35, 9]\n",
            "Carmedfice\n",
            "[47, 33, 35, 29, 15, 0, 18, 32, 45, 7]\n",
            "lispelzice\n",
            "[4, 20, 34, 14, 15, 35, 48, 32, 45, 7]\n",
            "iltinestik\n",
            "[54, 2, 40, 32, 48, 7, 26, 29, 17, 12]\n",
            "anogematae\n",
            "[16, 42, 58, 50, 20, 34, 17, 40, 17, 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:06, 16.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.149898529052734\n",
            "Poncichyla\n",
            "[47, 16, 42, 40, 32, 40, 6, 61, 36, 17]\n",
            "mymaJuster\n",
            "[4, 28, 49, 17, 41, 46, 26, 40, 15, 35]\n",
            "leesserziv\n",
            "[4, 7, 7, 26, 29, 15, 35, 48, 32, 48]\n",
            "Nakitesckm\n",
            "[4, 17, 12, 17, 40, 7, 26, 45, 12, 4]\n",
            "unicaylemi\n",
            "[54, 2, 32, 45, 16, 5, 43, 58, 49, 32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 16.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.393461227416992\n",
            "inpryntuek\n",
            "[54, 2, 47, 43, 61, 42, 40, 44, 16, 12]\n",
            "griinitura\n",
            "[47, 43, 17, 32, 48, 32, 40, 46, 35, 33]\n",
            "damanecass\n",
            "[4, 58, 49, 16, 2, 58, 45, 33, 26, 29]\n",
            "uncyeoussb\n",
            "[54, 2, 45, 61, 63, 58, 46, 26, 29, 39]\n",
            "hagrablysn\n",
            "[6, 16, 50, 43, 16, 25, 36, 61, 34, 48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 14.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.228116989135742\n",
            "hilllatera\n",
            "[6, 32, 36, 36, 36, 17, 40, 15, 35, 17]\n",
            "nonogindio\n",
            "[4, 16, 42, 58, 50, 32, 2, 0, 32, 16]\n",
            "Merllylerc\n",
            "[4, 15, 35, 36, 36, 61, 36, 7, 35, 9]\n",
            "iicalonpel\n",
            "[3, 32, 45, 33, 36, 58, 2, 47, 15, 36]\n",
            "Squestcoso\n",
            "[21, 30, 44, 7, 26, 40, 63, 58, 29, 16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.39180564880371\n",
            "teoukomate\n",
            "[4, 7, 16, 46, 29, 58, 49, 17, 40, 15]\n",
            "========= Results: epoch 8 of 10 =========\n",
            "train loss: 23.42| valid loss: 23.42\n",
            "\n",
            "========= Epoch 9 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/208 [00:00<00:39,  5.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.45665168762207\n",
            "raslentrou\n",
            "[4, 20, 29, 36, 16, 42, 40, 43, 58, 46]\n",
            "arpyablafa\n",
            "[20, 35, 14, 5, 17, 25, 36, 16, 48, 33]\n",
            "merotroupp\n",
            "[48, 15, 35, 17, 40, 43, 58, 46, 14, 14]\n",
            "Lkinicrood\n",
            "[4, 12, 32, 2, 32, 45, 43, 58, 38, 0]\n",
            "tanatlonte\n",
            "[4, 16, 42, 17, 40, 43, 16, 42, 40, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 15.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.599506378173828\n",
            "Agiotinedi\n",
            "[21, 50, 32, 16, 40, 32, 48, 7, 0, 32]\n",
            "Obperivath\n",
            "[21, 3, 14, 15, 35, 32, 48, 17, 40, 6]\n",
            "reutarhymb\n",
            "[4, 16, 46, 29, 58, 9, 6, 61, 34, 25]\n",
            "sustoleary\n",
            "[60, 46, 26, 40, 15, 36, 7, 33, 35, 61]\n",
            "scinditave\n",
            "[60, 45, 32, 2, 0, 32, 40, 16, 48, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:06, 16.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.485023498535156\n",
            "scthomatic\n",
            "[60, 45, 40, 6, 58, 34, 16, 42, 32, 45]\n",
            "Ercyumysma\n",
            "[21, 42, 9, 28, 37, 34, 61, 26, 49, 17]\n",
            "pmatongetl\n",
            "[26, 49, 17, 40, 16, 2, 50, 7, 29, 36]\n",
            "phomorocoo\n",
            "[14, 6, 58, 34, 15, 35, 16, 45, 51, 51]\n",
            "metchimeno\n",
            "[4, 7, 42, 9, 6, 32, 48, 7, 42, 16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 153/208 [00:09<00:03, 16.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.612308502197266\n",
            "sprastogla\n",
            "[60, 47, 43, 20, 60, 40, 58, 50, 43, 16]\n",
            "stentesest\n",
            "[60, 40, 15, 42, 40, 7, 40, 7, 26, 29]\n",
            "fwilaotire\n",
            "[47, 22, 33, 36, 17, 38, 40, 32, 48, 7]\n",
            "symiceboys\n",
            "[60, 61, 34, 32, 45, 7, 52, 51, 28, 26]\n",
            "grozzallyn\n",
            "[50, 43, 58, 8, 8, 33, 36, 36, 61, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 15.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.08568572998047\n",
            "podveniosm\n",
            "[47, 15, 0, 48, 7, 42, 32, 16, 26, 29]\n",
            "honfarsuQu\n",
            "[6, 16, 2, 23, 33, 35, 60, 46, 30, 44]\n",
            "chtarivece\n",
            "[47, 6, 40, 33, 35, 32, 48, 7, 42, 7]\n",
            "frypertity\n",
            "[47, 43, 61, 14, 15, 35, 40, 32, 40, 61]\n",
            "tulyousvir\n",
            "[47, 33, 36, 61, 58, 46, 29, 48, 7, 35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.78101348876953\n",
            "metlemmeub\n",
            "[4, 7, 40, 43, 20, 34, 49, 7, 17, 25]\n",
            "========= Results: epoch 9 of 10 =========\n",
            "train loss: 23.34| valid loss: 23.36\n",
            "\n",
            "========= Epoch 10 of 10 =========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/208 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.565593719482422\n",
            "remaciabca\n",
            "[4, 20, 34, 17, 40, 32, 16, 39, 45, 17]\n",
            "inenenamme\n",
            "[54, 2, 16, 42, 7, 42, 58, 34, 49, 16]\n",
            "ponomatine\n",
            "[47, 16, 42, 58, 49, 17, 40, 32, 48, 7]\n",
            "intitieifi\n",
            "[54, 2, 40, 32, 40, 32, 16, 32, 18, 32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|‚ñè         | 3/208 [00:00<00:20, 10.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tudoaredwe\n",
            "[4, 46, 0, 58, 16, 35, 7, 0, 22, 15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|‚ñà‚ñà‚ñå       | 53/208 [00:03<00:09, 17.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.107942581176758\n",
            "norsupbaph\n",
            "[4, 51, 35, 60, 46, 29, 31, 16, 14, 6]\n",
            "prppruarsh\n",
            "[47, 43, 20, 14, 43, 44, 17, 35, 26, 6]\n",
            "lureaestre\n",
            "[4, 46, 35, 7, 17, 7, 26, 40, 43, 20]\n",
            "Ariesistic\n",
            "[21, 35, 32, 16, 48, 32, 26, 29, 32, 45]\n",
            "goperetora\n",
            "[47, 16, 14, 15, 35, 7, 42, 58, 43, 58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 103/208 [00:06<00:06, 15.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.038089752197266\n",
            "enrodedynd\n",
            "[16, 2, 43, 58, 0, 7, 0, 61, 2, 0]\n",
            "crylochied\n",
            "[47, 43, 61, 36, 58, 9, 6, 32, 16, 48]\n",
            "knteriloif\n",
            "[54, 42, 40, 15, 35, 32, 36, 58, 38, 18]\n",
            "ojenoodMly\n",
            "[16, 27, 7, 42, 58, 38, 0, 61, 36, 61]\n",
            "banapenoma\n",
            "[4, 16, 2, 20, 14, 15, 42, 58, 49, 17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 151/208 [00:09<00:04, 14.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.52859878540039\n",
            "prasoptral\n",
            "[47, 43, 20, 29, 16, 14, 40, 43, 16, 36]\n",
            "sipushyder\n",
            "[60, 20, 14, 46, 26, 6, 61, 0, 15, 35]\n",
            "pirisiapii\n",
            "[47, 33, 35, 32, 26, 32, 16, 14, 43, 20]\n",
            "Nestyrrope\n",
            "[4, 7, 26, 40, 33, 35, 43, 58, 14, 15]\n",
            "trogissito\n",
            "[47, 43, 58, 50, 32, 26, 29, 32, 40, 58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 203/208 [00:12<00:00, 16.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.023834228515625\n",
            "hmentineli\n",
            "[6, 49, 16, 42, 40, 32, 48, 15, 36, 32]\n",
            "pyunnierwh\n",
            "[47, 1, 54, 2, 48, 32, 16, 35, 9, 6]\n",
            "thettorolm\n",
            "[9, 6, 7, 42, 40, 15, 35, 33, 36, 49]\n",
            "nanistPung\n",
            "[4, 20, 42, 32, 26, 29, 47, 54, 2, 50]\n",
            "moirrorome\n",
            "[4, 51, 33, 35, 43, 58, 43, 58, 49, 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:12<00:00, 16.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 23.31765365600586\n",
            "sprierycop\n",
            "[60, 14, 43, 20, 15, 35, 28, 45, 16, 14]\n",
            "========= Results: epoch 10 of 10 =========\n",
            "train loss: 23.30| valid loss: 23.32\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zymBj9QrDHRM"
      },
      "source": [
        "You may wish to try different values of $N$ and see what the impact on sample quality is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auBibYUTtIom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e334f04-7df1-4d23-c7ca-0cd7d8b9bfd2"
      },
      "source": [
        "x = torch.tensor(encode(\"quack\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T))\n",
        "\n",
        "x = torch.tensor(encode(\"quick\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T))\n",
        "\n",
        "x = torch.tensor(encode(\"qurck\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T)) # should have lower probability---in English only vowels follow \"qu\"\n",
        "\n",
        "x = torch.tensor(encode(\"qiick\")).unsqueeze(0)\n",
        "T = torch.tensor([5])\n",
        "print(model.viterbi(x,T)) # should have lower probability---in English only \"u\" follows \"q\"\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([[30, 44, 17, 45, 12]], tensor([[-13.7824]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[30, 44, 32, 45, 12]], tensor([[-10.9867]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[30, 44, 35, 45, 12]], tensor([[-19.2461]], device='cuda:0', grad_fn=<GatherBackward0>))\n",
            "([[30, 44, 32, 45, 12]], tensor([[-19.3918]], device='cuda:0', grad_fn=<GatherBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def show_prob(word):\n",
        "    x = torch.tensor(encode(word)).unsqueeze(0)\n",
        "    T = torch.tensor([len(word)])\n",
        "\n",
        "    path, logp = model.viterbi(x, T)\n",
        "\n",
        "    # Convert log-prob ‚Üí normal probability\n",
        "    prob = torch.exp(logp).item()\n",
        "\n",
        "    print(f\"word: {word}\")\n",
        "    print(f\"best state path: {path[0]}\")\n",
        "    print(f\"log probability: {logp.item():.4f}\")\n",
        "    print(f\"normal probability: {prob:.10f}\\n\")  # 10 decimal places\n",
        "\n",
        "\n",
        "# Run your examples\n",
        "show_prob(\"quack\")\n",
        "show_prob(\"quick\")\n",
        "show_prob(\"qurck\")  # should be low\n",
        "show_prob(\"qiick\")  # should be low\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCQRiB6MI2OW",
        "outputId": "108f1d59-4c47-4230-80fb-9c42ee810a2f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: quack\n",
            "best state path: [30, 44, 17, 45, 12]\n",
            "log probability: -13.7824\n",
            "normal probability: 0.0000010337\n",
            "\n",
            "word: quick\n",
            "best state path: [30, 44, 32, 45, 12]\n",
            "log probability: -10.9867\n",
            "normal probability: 0.0000169247\n",
            "\n",
            "word: qurck\n",
            "best state path: [30, 44, 35, 45, 12]\n",
            "log probability: -19.2461\n",
            "normal probability: 0.0000000044\n",
            "\n",
            "word: qiick\n",
            "best state path: [30, 44, 32, 45, 12]\n",
            "log probability: -19.3918\n",
            "normal probability: 0.0000000038\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **1. Why do we build the alphabet using `Counter((\"\".join(lines)))` instead of reusing the earlier alphabet = 'a'..'z'?**\n",
        "\n",
        "### **Significance (HMM + Data-Driven Modeling)**\n",
        "\n",
        "This time the HMM is being **trained on real text**, not hard-coded toy examples.\n",
        "\n",
        "So we extract the alphabet *from the actual dataset itself*, meaning:\n",
        "\n",
        "* If the corpus contains accented letters, apostrophes, hyphens ‚Äî they get included\n",
        "* If the dataset excludes some letters, the model won‚Äôt waste emission parameters on unused symbols\n",
        "\n",
        "### **Why not use fixed 26-letters?**\n",
        "\n",
        "Because real data vocabulary may be:\n",
        "\n",
        "* smaller\n",
        "* larger\n",
        "* domain-specific\n",
        "\n",
        "A data-driven alphabet prevents building an incorrect emission matrix.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Why do we train the HMM with `N=64` states? Why 64 specifically?**\n",
        "\n",
        "### **Significance (What HMM states are actually doing)**\n",
        "\n",
        "In a real HMM, the **states are not vowels/consonants** anymore ‚Äî they are **latent linguistic clusters**.\n",
        "\n",
        "64 states gives the model enough ‚Äúexpressive capacity‚Äù to learn:\n",
        "\n",
        "* onset vs nucleus vs coda patterns\n",
        "* consonant types (plosives, fricatives, liquids‚Ä¶)\n",
        "* vowel classes\n",
        "* clusters like ‚Äúqu‚Äù, ‚Äúck‚Äù, ‚Äúng‚Äù, ‚Äúsh‚Äù, ‚Äúth‚Äù\n",
        "* position-dependent variants (initial-q, mid-q, final-q, etc.)\n",
        "\n",
        "### **Why not use 2 or 3 states?**\n",
        "\n",
        "Because natural language morphology is too rich ‚Äî low-state HMMs cannot capture even basic English orthography patterns.\n",
        "\n",
        "### **Why not use 256 states?**\n",
        "\n",
        "Training becomes unstable and slow for this demo; 64 is a sweet spot of expressive power vs simplicity.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Why is the training loss defined as `loss = -log_probs.mean()`?**\n",
        "\n",
        "### **Significance (Maximum Likelihood for HMMs)**\n",
        "\n",
        "Forward returns **log p(x)**, so maximizing likelihood means minimizing **negative log-likelihood**.\n",
        "\n",
        "This corresponds to MLE:\n",
        "\n",
        "```\n",
        "maximize    ‚àë log pŒ∏(x)\n",
        "equivalent to minimize  -‚àë log pŒ∏(x)\n",
        "```\n",
        "\n",
        "### **Why not cross-entropy?**\n",
        "\n",
        "Cross-entropy applies to classification targets.\n",
        "Here, the target is an entire *sequence*, so the forward probability is correct.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Why show sampled sequences during training (`model.sample()`) every 50 batches?**\n",
        "\n",
        "### **Significance (Interpreting HMM learning qualitatively)**\n",
        "\n",
        "Sampling shows **what the model believes English words look like** at the current epoch.\n",
        "\n",
        "You can visually track:\n",
        "\n",
        "* random gibberish at epoch 1\n",
        "* emergence of vowel/consonant balance\n",
        "* discovery of frequent patterns:\n",
        "\n",
        "  * ‚Äúqu‚Äù\n",
        "  * ‚Äúck‚Äù\n",
        "  * double consonants\n",
        "  * English-like endings (‚Äúing‚Äù, ‚Äúed‚Äù, ‚Äúer‚Äù)\n",
        "\n",
        "Sampling is the **best sanity check** to ensure training is working.\n",
        "\n",
        "### **Why not wait until the end?**\n",
        "\n",
        "Intermediate monitoring is essential when teaching or debugging sequence models.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Why use Adam optimizer with weight decay?**\n",
        "\n",
        "### **Significance (Stabilizing HMM Learning)**\n",
        "\n",
        "HMMs trained by gradient descent can suffer from:\n",
        "\n",
        "* very sharp posteriors\n",
        "* collapse of states\n",
        "* overfitting emission distributions\n",
        "* becoming degenerate (one state dominates)\n",
        "\n",
        "Weight decay acts like **soft regularization** on:\n",
        "\n",
        "* state priors\n",
        "* transitions\n",
        "* emissions\n",
        "\n",
        "Adam helps with noisy gradients from variable-length minibatches.\n",
        "\n",
        "### **Why not use Baum‚ÄìWelch (EM)?**\n",
        "\n",
        "Because:\n",
        "\n",
        "* EM is harder to implement from scratch\n",
        "* cannot run on GPUs cleanly\n",
        "* gradient-based HMMs are easier to integrate into PyTorch‚Äôs ecosystem\n",
        "* pedagogically simpler for showing training loops\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Why do we split 90% training / 10% validation?**\n",
        "\n",
        "### **Significance (True Evaluation of Generative Models)**\n",
        "\n",
        "Validation loss measures:\n",
        "\n",
        "* generalization of the HMM\n",
        "* whether the model has learned real English structure instead of memorization\n",
        "\n",
        "Overfitting appears when:\n",
        "\n",
        "* train loss ‚Üì\n",
        "* validation loss ‚Üë\n",
        "\n",
        "Classic for generative sequence models.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Why does the model correctly score `\"quack\"` or `\"quick\"` higher than `\"qurck\"` or `\"qiick\"`?**\n",
        "\n",
        "### **Significance (HMM‚Äôs ability to learn orthographic dependencies)**\n",
        "\n",
        "During training, the model learns high transition+emission probability for:\n",
        "\n",
        "```\n",
        "q ‚Üí u\n",
        "```\n",
        "\n",
        "Because in English, almost every \"q\" is followed by \"u\".\n",
        "\n",
        "Similarly it learns:\n",
        "\n",
        "* ‚Äúck‚Äù\n",
        "* ‚Äúqu‚Äù\n",
        "* ‚Äúing‚Äù\n",
        "* ‚Äúsh‚Äù, ‚Äúch‚Äù\n",
        "* vowel/consonant alternation tendencies\n",
        "\n",
        "### **Why not use RNNs to learn this?**\n",
        "\n",
        "Because you are demonstrating that **even simple HMMs** can capture surprising amounts of structure.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Why do we wrap everything in a `Trainer` class instead of writing a single loop?**\n",
        "\n",
        "### **Significance (Clean Architecture)**\n",
        "\n",
        "This emphasizes modularity:\n",
        "\n",
        "* `trainer.train(dataset)`\n",
        "* `trainer.test(dataset)`\n",
        "\n",
        "In real machine learning pipelines, such separation is important.\n",
        "\n",
        "### **Why not use Lightning or high-level trainers?**\n",
        "\n",
        "Because you want students to see **exactly how training works**, step-by-step.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Why do we repeatedly evaluate validation loss every epoch?**\n",
        "\n",
        "### **Significance**\n",
        "\n",
        "Monitoring validation loss ensures the HMM:\n",
        "\n",
        "* is not collapsing\n",
        "* is improving English modeling\n",
        "* is not diverging\n",
        "* is not overfitting\n",
        "\n",
        "Generative models MUST be monitored because they can silently degrade.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Why does `viterbi(\"qurck\")` have lower probability?**\n",
        "\n",
        "### **Significance (Sequence-Level Reasoning)**\n",
        "\n",
        "Viterbi detects the **best hidden state path**.\n",
        "For irregular words like ‚Äúqurck‚Äù, the emission/transition structure must force unlikely sequences of states, lowering score.\n",
        "\n",
        "This directly shows students the interpretability of HMMs:\n",
        "\n",
        "* Viterbi path is inspectable\n",
        "* You can see state clusters that correspond to linguistic categories\n",
        "* You observe where the sequence violates English patterns\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qrJhaqM7xysh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eZeQXWjhDev"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "HMMs used to be very popular in natural language processing, but they have largely been overshadowed by neural network models like RNNs and Transformers. Still, it is fun and instructive to study the HMM; some commonly used machine learning techniques like [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf) are inspired by HMM methods. HMMs are [still used in conjunction with neural networks in speech recognition](https://arxiv.org/abs/1811.07453), where the assumption of a one-hot state makes sense for modelling phonemes, which are spoken one at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXQOBz5zqe10"
      },
      "source": [
        "## Acknowledgments\n",
        "\n",
        "This notebook is based partly on Lawrence Rabiner's excellent article \"[A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf)\", which you may also like to check out. Thanks also to Dima Serdyuk and Kyle Gorman for their feedback on the draft."
      ]
    }
  ]
}