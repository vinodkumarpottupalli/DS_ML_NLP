{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BertModel"
      ],
      "metadata": {
        "id": "_KDtopLdF63A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define important variables and hyperparameters for later use\n",
        "SRC_LANG = \"en\"\n",
        "TGT_LANG = \"fr\"\n",
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "LR = 0.01\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "NUM_HEADS = 4\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kO1IbwwTF-Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the \"Opus Books\" dataset: A translation dataset with various language pairs available\n",
        "# Link to interactive dataset viewer: https://huggingface.co/datasets/Helsinki-NLP/opus_books\n",
        "dataset = load_dataset(\"opus_books\", f\"en-ru\", split=\"train[:1%]\")\n",
        "\n",
        "# Loading a pre-trained tokenizer to tokenize the text in the dataset\n",
        "# Tokenization output example: This is a cat -> 43 105 1236 4\n",
        "tkr = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Ensuring padding and EOS (end of sentence) tokens exist\n",
        "if tkr.eos_token is None:\n",
        "  tkr.eos_token = tkr.sep_token or \"</s>\"\n",
        "if tkr.pad_token is None:\n",
        "  tkr.pad_token = tkr.eos_token\n",
        "\n",
        "SRC_VOCAB_SIZE = len(tkr)\n",
        "TGT_VOCAB_SIZE = len(tkr)\n",
        "\n",
        "# Preprocessing the data and creating a dataset that returns tokenized text (not strings!) which we can pass to the model\n",
        "# Whenever you're training a model to do something, chances are you'll have to preprocess your input data yourself\n",
        "# You can use the following as a reference template for how you can do that\n",
        "\n",
        "class TranslationDataset(Dataset): # Custom dataset class must inherit from PyTorch's Dataset class!\n",
        "  def __init__(self, dataset, tkr, max_len=50):\n",
        "      self.dataset = dataset\n",
        "      self.tkr = tkr\n",
        "      self.max_len = max_len\n",
        "\n",
        "  # The __len__ and __getitem__ methods MUST be implemented since PyTorch uses that at the back-end to retrieve samples\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  # Returns a preprocessed dataitem at a given index\n",
        "  def __getitem__(self, idx):\n",
        "    src_text = self.dataset[idx][\"translation\"][SRC_LANG]\n",
        "    tgt_text = self.dataset[idx][\"translation\"][TGT_LANG]\n",
        "\n",
        "    # Tokenizing both source and target texts and returning the tokenized IDs\n",
        "    src = self.tkr(\n",
        "        src_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=self.max_len,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    tgt = self.tkr(\n",
        "        tgt_text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=self.max_len+1,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        src.input_ids.squeeze(0),\n",
        "        tgt.input_ids.squeeze(0),\n",
        "    )\n",
        "\n",
        "train_data = TranslationDataset(dataset, tkr, MAX_LEN)\n",
        "\n",
        "# Dataloaders allow us to batch data, shuffle it, and much more.\n",
        "# The first input to this function MUST be a PyTorch Dataset class (like the custom one we created above)\n",
        "# Doc: https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "3scj-RcaGMFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A sample from the dataset\n",
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JchsTutOTXX",
        "outputId": "9eca7336-8380-430b-e9f5-c95c6335866f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0', 'translation': {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a Simple Attention Layer\n",
        "\n",
        "We require this layer to receive some input embeddings and possibly an attention mask. In return, it computes the attention scores among all of those embeddings according to the following formula:\n",
        "\n",
        "$Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$"
      ],
      "metadata": {
        "id": "WnBvV56Scx1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  # Input IDs -> [batch_size, seq_len]\n",
        "  # Input embeddings -> [batch_size, seq_len, emb_dim]\n",
        "  def __init__(self, emb_dim, num_heads):\n",
        "    super().__init__()\n",
        "    assert emb_dim % num_heads == 0, \"Embedding dimension MUST be divisible by number of heads!\"\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = emb_dim // num_heads\n",
        "\n",
        "    # Linear layers that project input embeddings to queries, keys, and values\n",
        "    self.q_proj = nn.Linear(emb_dim, emb_dim)\n",
        "    self.k_proj = nn.Linear(emb_dim, emb_dim)\n",
        "    self.v_proj = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "    # Final linear layer to project concatenated heads\n",
        "    self.out_proj = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "\n",
        "    B, L_q, D = query.size()\n",
        "    L_k = key.size(1)\n",
        "    H = self.num_heads\n",
        "    head_dim = self.head_dim\n",
        "\n",
        "    # First, we project the input embeddings into query, key and value vectors respectively\n",
        "    Q = self.q_proj(query)  # [batch_size, seq_len, emb_dim]\n",
        "    K = self.k_proj(key)\n",
        "    V = self.v_proj(value)\n",
        "\n",
        "    # Then, we split each matrix into H heads\n",
        "    # In other words, we can reshape the last (rightmost) dimension of each matrix from (emb_dim) to (H, head_dim)\n",
        "    Q = Q.view(B, L_q, H, head_dim).transpose(1, 2) # [B, H, L, head_dim]\n",
        "    K = K.view(B, L_k, H, head_dim).transpose(1, 2)\n",
        "    V = V.view(B, L_k, H, head_dim).transpose(1, 2)\n",
        "\n",
        "    # Compute scaled dot-product attention (the inner part of the softmax in the formula given above)\n",
        "    scores = (Q @ K.transpose(-2, -1)) / (head_dim ** 0.5) # [B, H, L, L]\n",
        "\n",
        "    # You might want to pass a mask to this layer to denote tokens/embedding vectors that you DO NOT want to attend to\n",
        "    # 1. In cases of padding, you do not want to attend to padding tokens\n",
        "    # 2. In the case of cross attention in decoders, you want the decoder to mask out future tokens\n",
        "    if mask is not None:\n",
        "      # Wherever the mask is 0, we replace the corresponding elements in scores with -infinity (why not 0?)\n",
        "      # doc: https://docs.pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    attn = F.softmax(scores, dim=-1) # [B, H, L, L]\n",
        "\n",
        "    out = attn @ V  # [B, H, L, head_dim]\n",
        "\n",
        "    # Now we concatenate the heads back together by:\n",
        "    # 1. Transposing the 2nd and 3rd dimensions to make the tensor [B, L, H, head_dim]\n",
        "    # 2. Reshape the tensor to make it [B, L, D] (remember, D = H * head_dim)\n",
        "    out = out.transpose(1, 2).reshape(B, L_q, D)\n",
        "\n",
        "    # Final output projection\n",
        "    out = self.out_proj(out)\n",
        "\n",
        "    return out, attn\n"
      ],
      "metadata": {
        "id": "6feaMYSzaZhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an Encoder"
      ],
      "metadata": {
        "id": "S4-O7SjPgcMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the encoder of our transformer model\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, num_heads, hid_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    # Embedding layer\n",
        "    self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    # Self attention block\n",
        "    # Right now, we are using our custom attention layer but PyTorch also has its own implementation of the same\n",
        "    # PyTorch attention layer: https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
        "    self.self_attn = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "    # Feedforward network (AKA: MLP)\n",
        "    self.linear1 = nn.Linear(emb_dim, hid_dim)\n",
        "    self.linear2 = nn.Linear(hid_dim, emb_dim)\n",
        "\n",
        "    # Activation function (usually this is a non-linear function like ReLU)\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "    # Layer normalization layers\n",
        "    self.norm1 = nn.LayerNorm(emb_dim)\n",
        "    self.norm2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    # Convert the input IDs into embedding vectors\n",
        "    x = self.emb(input_ids)\n",
        "\n",
        "    # Self attend over the input embeddings\n",
        "    attn_out, _ = self.self_attn(x, x, x)\n",
        "\n",
        "    # Residual connection makes sure the input signal doesn't get completely lost\n",
        "    # Layer normalization are often done to make sure the output values are small and close together to maintain stability\n",
        "    x = self.norm1(x + attn_out)\n",
        "\n",
        "    #Feed-forward Network/MLP\n",
        "    mlp_out = self.linear2(self.activation(self.linear1(x)))\n",
        "    x = self.norm2(x + mlp_out)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "8LsTpOm9KIGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder (Optional)"
      ],
      "metadata": {
        "id": "0Gt2AXrzxd3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, num_heads, hid_dim):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.self_attn = MultiHeadAttention(emb_dim, num_heads)\n",
        "    self.cross_attn = MultiHeadAttention(emb_dim, num_heads)\n",
        "\n",
        "    self.linear1 = nn.Linear(emb_dim, hid_dim)\n",
        "    self.linear2 = nn.Linear(hid_dim, emb_dim)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(emb_dim)\n",
        "    self.norm2 = nn.LayerNorm(emb_dim)\n",
        "    self.norm3 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, output_ids, enc_out):\n",
        "    batch_size, seq_len = output_ids.shape\n",
        "\n",
        "    y = self.emb(output_ids)\n",
        "\n",
        "    # Mask future tokens because we don't want the model to be able to peek at the \"future\" tokens\n",
        "    # We do this by zeroing out all tokens that come after the current token so that the decoder can only look at previous tokens\n",
        "    attn_mask = torch.tril(torch.ones((1, 1, seq_len, seq_len), device=output_ids.device), diagonal=0).bool()\n",
        "\n",
        "    # Self-attention: attending over the current hidden state of the decoder itself\n",
        "    self_attn_out, _ = self.self_attn(y, y, y, mask=attn_mask)\n",
        "    y = self.norm1(y + self_attn_out)\n",
        "\n",
        "    # Cross-attention: attending over the current decoder state and the output of the encoder\n",
        "    # Query: Current decoder state\n",
        "    # Key, Value: Output of the encoder\n",
        "    cross_attn_out, _ = self.cross_attn(y, enc_out, enc_out)\n",
        "    y = self.norm2(y + cross_attn_out)\n",
        "\n",
        "    # FFN\n",
        "    mlp_out = self.linear2(self.activation(self.linear1(y)))\n",
        "    y = self.norm3(y + mlp_out)\n",
        "\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "E3kHm96rKW-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class that controls how the components work with each other\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, emb_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        enc_out = self.encoder(x)\n",
        "        output = self.decoder(y, enc_out)\n",
        "        output = self.lm_head(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4Dxd8lPaKeN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61bee866"
      },
      "source": [
        "encoder = Encoder(SRC_VOCAB_SIZE, EMB_DIM, NUM_HEADS, HID_DIM)\n",
        "decoder = Decoder(TGT_VOCAB_SIZE, EMB_DIM, NUM_HEADS, HID_DIM)\n",
        "model = Seq2Seq(encoder, decoder, EMB_DIM, TGT_VOCAB_SIZE).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple training loop\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tkr.pad_token_id)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for input_ids, output_ids in train_loader:\n",
        "    input_ids = input_ids.to(device)\n",
        "    output_ids = output_ids.to(device)\n",
        "\n",
        "    # We take all of the output_ids except the last one and treat it as the current state of generation\n",
        "    y_input = output_ids[:, :-1]\n",
        "\n",
        "    # [1, 2, 3, 4, 5]\n",
        "    # [1, 2, 3, 4]\n",
        "    # [2, 3, 4, 5] <- Expected output\n",
        "    # [2, 10, 4, 6] <- Model output\n",
        "\n",
        "    # The next step of generation must generate the same set of tokens SHIFTED TO THE RIGHT by one step\n",
        "    y_output = output_ids[:, 1:]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_ids, y_input)\n",
        "\n",
        "    output = output.reshape(-1, TGT_VOCAB_SIZE)\n",
        "    y_output = y_output.reshape(-1)\n",
        "\n",
        "    loss = criterion(output, y_output)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "vh6z1FYMKiG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3a549d-a80e-4d1a-8392-3139216bad94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 | Loss: 5.6766\n",
            "Epoch 2/2 | Loss: 5.5260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, tkr, max_len=50):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # Tokenize the English sentence\n",
        "    input_ids = tkr(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_len\n",
        "    ).input_ids.to(device)\n",
        "\n",
        "    # Encode source\n",
        "    enc_out = model.encoder(input_ids)\n",
        "\n",
        "    # Start decoding with <pad> or <sos>\n",
        "    output_tokens = torch.tensor([[tkr.pad_token_id]], device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "      # Get logits for the next token\n",
        "      output = model.decoder(output_tokens, enc_out)\n",
        "      logits = output[:, -1, :]  # [batch, vocab]\n",
        "      next_token = torch.argmax(logits, dim=-1).unsqueeze(0)  # [1, 1]\n",
        "\n",
        "      output_tokens = torch.cat([output_tokens, next_token], dim=1)\n",
        "\n",
        "      # Stop if EOS generated\n",
        "      if next_token.item() == tkr.eos_token_id:\n",
        "        break\n",
        "\n",
        "    translated = tkr.decode(output_tokens.squeeze().tolist(), skip_special_tokens=True)\n",
        "    return translated\n",
        "\n",
        "# Testing the model by translating an arbitrary sentence\n",
        "test_sentence = \"Hello. My name is Bob\"\n",
        "translation = translate_sentence(test_sentence, model, tkr)\n",
        "print(\"\\nInput:\", test_sentence)\n",
        "print(\"Predicted Translation:\", translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6N8UiODONb9",
        "outputId": "baa89886-9354-4dbe-c955-c08f0e51ca46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: Hello. My name is Bob\n",
            "Predicted Translation: them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In them In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Brief Glance at an Encoder-Only Model"
      ],
      "metadata": {
        "id": "mJi0yTvdxPxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH8XYuUgw8_W",
        "outputId": "219fda83-921c-4eff-e22c-f7ea6a78bb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}