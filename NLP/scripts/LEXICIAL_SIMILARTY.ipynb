{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPuhuGb7bRPu",
        "outputId": "01d1ebcc-688c-442d-e8b4-1271976a953f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuwUO78MbhbZ",
        "outputId": "d2d118d5-f2fb-47b2-f8ac-2e6afc3b9717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All senses of 'interest':\n",
            "\n",
            "dog.n.01  -->  a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
            "frump.n.01  -->  a dull unattractive unpleasant girl or woman\n",
            "dog.n.03  -->  informal term for a man\n",
            "cad.n.01  -->  someone who is morally reprehensible\n",
            "frank.n.02  -->  a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
            "pawl.n.01  -->  a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
            "andiron.n.01  -->  metal supports for logs in a fireplace\n",
            "chase.v.01  -->  go after with the intent to catch\n"
          ]
        }
      ],
      "source": [
        "word = \"dog\"\n",
        "synsets = wn.synsets(word)\n",
        "\n",
        "print(\"All senses of 'interest':\\n\")\n",
        "for s in synsets:\n",
        "    print(f\"{s.name()}  -->  {s.definition()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsbvR2kcbk2B",
        "outputId": "71315a62-cb61-440c-b303-b6416eb15c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: sloping land (especially the slope beside a body of water)\n",
            "Examples: ['they pulled the canoe up on the bank', 'he sat on the bank of the river and watched the currents']\n"
          ]
        }
      ],
      "source": [
        "s = wn.synset('bank.n.01')\n",
        "print(\"Definition:\", s.definition())\n",
        "print(\"Examples:\", s.examples())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIy-4zYLbnYZ",
        "outputId": "0600f96f-e7b3-47d2-e640-a7da050bc4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms: [Synset('domestic_animal.n.01'), Synset('canine.n.02')]\n",
            "Hyponyms: [Synset('mexican_hairless.n.01'), Synset('puppy.n.01'), Synset('poodle.n.01'), Synset('newfoundland.n.01'), Synset('corgi.n.01'), Synset('dalmatian.n.02'), Synset('leonberg.n.01'), Synset('cur.n.01'), Synset('lapdog.n.01'), Synset('pooch.n.01'), Synset('pug.n.01'), Synset('griffon.n.02'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('basenji.n.01'), Synset('great_pyrenees.n.01'), Synset('working_dog.n.01'), Synset('hunting_dog.n.01')]\n",
            "cat vs dog similarity : 0.2\n",
            "cat vs car similarity : 0.05555555555555555\n"
          ]
        }
      ],
      "source": [
        "s = wn.synset('dog.n.01')\n",
        "\n",
        "print(\"Hypernyms:\", s.hypernyms())\n",
        "print(\"Hyponyms:\", s.hyponyms())\n",
        "cat = wn.synset('cat.n.01')\n",
        "dog = wn.synset('dog.n.01')\n",
        "car = wn.synset('car.n.01')\n",
        "\n",
        "print(\"cat vs dog similarity :\", cat.path_similarity(dog))\n",
        "print(\"cat vs car similarity :\", cat.path_similarity(car))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyTluNZ0b1wo",
        "outputId": "765140f7-9179-4a10-d990-1dc9e258b4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog.n.01\n",
            "  domestic_animal.n.01\n",
            "    animal.n.01\n",
            "      organism.n.01\n",
            "        living_thing.n.01\n",
            "          whole.n.02\n",
            "            object.n.01\n",
            "              physical_entity.n.01\n",
            "                entity.n.01\n",
            "  canine.n.02\n",
            "    carnivore.n.01\n",
            "      placental.n.01\n",
            "        mammal.n.01\n",
            "          vertebrate.n.01\n",
            "            chordate.n.01\n",
            "              animal.n.01\n",
            "                organism.n.01\n",
            "                  living_thing.n.01\n",
            "                    whole.n.02\n",
            "                      object.n.01\n",
            "                        physical_entity.n.01\n",
            "                          entity.n.01\n"
          ]
        }
      ],
      "source": [
        "def print_tree(synset, depth=0):\n",
        "    print(\"  \" * depth + synset.name())\n",
        "    for hyper in synset.hypernyms():\n",
        "        print_tree(hyper, depth+1)\n",
        "\n",
        "print_tree(wn.synset(\"dog.n.01\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZgZQAHqdXKC",
        "outputId": "53a51d65-194c-4605-92a2-15d9413dba9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# install & downloads\n",
        "!pip install -q nltk scikit-learn\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')       # tokenizer for contexts\n",
        "nltk.download('stopwords')   # optional stopwords for features\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(\"Setup done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LrzYJW5dY_I",
        "outputId": "d6bb5cbf-5673-46ff-b2c0-36944becd427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Senses for 'interest':\n",
            " - interest.n.01        : a sense of concern with and curiosity about someone or something\n",
            " - sake.n.01            : a reason for wanting something done\n",
            " - interest.n.03        : the power of attracting or holding one's attention (because it is unusual or exciting etc.)\n",
            " - interest.n.04        : a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
            " - interest.n.05        : (law) a right or legal share of something; a financial involvement with something\n",
            " - interest.n.06        : (usually plural) a social group whose members control some field of activity and who have common aims\n",
            " - pastime.n.01         : a diversion that occupies one's time and thoughts (usually pleasantly)\n",
            " - interest.v.01        : excite the curiosity of; engage the interest of\n",
            " - concern.v.02         : be on the mind of\n",
            " - matter_to.v.01       : be of importance or consequence\n",
            "\n",
            "Senses for 'bank':\n",
            " - bank.n.01            : sloping land (especially the slope beside a body of water)\n",
            " - depository_financial_institution.n.01 : a financial institution that accepts deposits and channels the money into lending activities\n",
            " - bank.n.03            : a long ridge or pile\n",
            " - bank.n.04            : an arrangement of similar objects in a row or in tiers\n",
            " - bank.n.05            : a supply or stock held in reserve for future use (especially in emergencies)\n",
            " - bank.n.06            : the funds held by a gambling house or the dealer in some gambling games\n",
            " - bank.n.07            : a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
            " - savings_bank.n.02    : a container (usually with a slot in the top) for keeping money at home\n",
            " - bank.n.09            : a building in which the business of banking transacted\n",
            " - bank.n.10            : a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
            " - bank.v.01            : tip laterally\n",
            " - bank.v.02            : enclose with a bank\n",
            " - bank.v.03            : do business with a bank or keep an account at a bank\n",
            " - bank.v.04            : act as the banker in a game or in gambling\n",
            " - bank.v.05            : be in the banking business\n",
            " - deposit.v.02         : put into a bank account\n",
            " - bank.v.07            : cover with ashes so to control the rate of burning\n",
            " - trust.v.01           : have confidence or faith in\n"
          ]
        }
      ],
      "source": [
        "# quick sanity: show WordNet senses for 'interest' and 'bank'\n",
        "for w in ['interest','bank']:\n",
        "    print(f\"\\nSenses for '{w}':\")\n",
        "    for s in wn.synsets(w):\n",
        "        print(f\" - {s.name():20} : {s.definition()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKnbxwrjdggL",
        "outputId": "4ccc2221-8a3b-4646-833e-ca5952b2ddfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: She paid 3% interest on the loan.\n",
            "Lesk sense: interest.v.01\n",
            "Definition: excite the curiosity of; engage the interest of\n",
            "\n",
            "Sentence: He showed a great interest in astronomy.\n",
            "Lesk sense: sake.n.01\n",
            "Definition: a reason for wanting something done\n",
            "\n",
            "Sentence: I sat on the bank and watched the river flow.\n",
            "Lesk sense: depository_financial_institution.n.01\n",
            "Definition: a financial institution that accepts deposits and channels the money into lending activities\n",
            "\n",
            "Sentence: He went to the river bank to fish.\n",
            "Lesk sense: bank.v.07\n",
            "Definition: cover with ashes so to control the rate of burning\n"
          ]
        }
      ],
      "source": [
        "# Lesk algorithm usage examples\n",
        "sentences = [\n",
        "    \"She paid 3% interest on the loan.\",\n",
        "    \"He showed a great interest in astronomy.\",\n",
        "    \"I sat on the bank and watched the river flow.\",\n",
        "    \"He went to the river bank to fish.\"\n",
        "]\n",
        "\n",
        "for sent in sentences:\n",
        "    # simple lesk expects tokenized context and the ambiguous word\n",
        "    tok = word_tokenize(sent)\n",
        "    sense = lesk(tok, 'interest' if 'interest' in sent else 'bank')\n",
        "    print(\"\\nSentence:\", sent)\n",
        "    if sense:\n",
        "        print(\"Lesk sense:\", sense.name())\n",
        "        print(\"Definition:\", sense.definition())\n",
        "    else:\n",
        "        print(\"Lesk returned: None\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pywsd\n",
        "!python -m pywsd.download   # downloads WordNet data for pywsd\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG0Bzy2sMqa2",
        "outputId": "3636ca2d-ce40-4835-d84b-e803c335eb49"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pywsd in /usr/local/lib/python3.12/dist-packages (1.2.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from pywsd) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pywsd) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pywsd) (2.2.2)\n",
            "Requirement already satisfied: wn==0.0.23 in /usr/local/lib/python3.12/dist-packages (from pywsd) (0.0.23)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from pywsd) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->pywsd) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->pywsd) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->pywsd) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->pywsd) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->pywsd) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pywsd) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pywsd) (2025.2)\n",
            "Warming up PyWSD (takes ~10 secs)... Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pywsd/__init__.py\", line 34, in <module>\n",
            "    simple_lesk('This is a foo bar sentence', 'bar')\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pywsd/lesk.py\", line 241, in simple_lesk\n",
            "    ambiguous_word = lemmatize(ambiguous_word, pos=pos)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pywsd/utils.py\", line 92, in lemmatize\n",
            "    pos = pos if pos else penn2morphy(pos_tag([ambiguous_word])[0][1],\n",
            "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pywsd.lesk import adapted_lesk\n",
        "\n",
        "sentences = [\n",
        "    \"She paid 3% interest on the loan.\",\n",
        "    \"He showed a great interest in astronomy.\",\n",
        "    \"I sat on the bank and watched the river flow.\",\n",
        "    \"He went to the river bank to fish.\"\n",
        "]\n",
        "\n",
        "def get_target_word(sentence):\n",
        "    if \"interest\" in sentence.lower():\n",
        "        return \"interest\"\n",
        "    return \"bank\"\n",
        "\n",
        "for sent in sentences:\n",
        "    target = get_target_word(sent)\n",
        "    sense = adapted_lesk(sent, target)\n",
        "    print(\"\\nSentence:\", sent)\n",
        "    print(\"Predicted Sense:\", sense.name())\n",
        "    print(\"Definition:\", sense.definition())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRc7H_mbMz1J",
        "outputId": "d55f127b-e718-425e-eaee-ab78667af399"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: She paid 3% interest on the loan.\n",
            "Predicted Sense: interest.n.04\n",
            "Definition: a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
            "\n",
            "Sentence: He showed a great interest in astronomy.\n",
            "Predicted Sense: interest.n.03\n",
            "Definition: the power of attracting or holding one's attention (because it is unusual or exciting etc.)\n",
            "\n",
            "Sentence: I sat on the bank and watched the river flow.\n",
            "Predicted Sense: bank.n.01\n",
            "Definition: sloping land (especially the slope beside a body of water)\n",
            "\n",
            "Sentence: He went to the river bank to fish.\n",
            "Predicted Sense: bank.n.10\n",
            "Definition: a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. What does the code demonstrate about the word “bank”?**\n",
        "\n",
        "The code retrieves **all WordNet synsets** for *bank*, prints their definitions, and shows how the same word has multiple senses (river bank, financial bank). This demonstrates **polysemy** and sense inventory in WordNet.\n",
        "\n",
        "### **2. How does the code compute semantic similarity between words?**\n",
        "\n",
        "It uses **path similarity** between synsets (e.g., *cat.n.01* vs *dog.n.01*). Path similarity measures semantic closeness based on the distance between concepts in the WordNet hierarchy.\n",
        "\n",
        "### **3. What is the purpose of the custom `print_tree()` function?**\n",
        "\n",
        "`print_tree()` recursively prints the **hypernym hierarchy** for a synset (e.g., dog → canine → carnivore → …). This visualizes where a concept sits in WordNet’s semantic taxonomy.\n",
        "\n",
        "### **4. How does the code perform Word Sense Disambiguation (WSD)?**\n",
        "\n",
        "It applies the **Lesk algorithm** from NLTK, which disambiguates a word (interest/bank) based on overlapping words between context and dictionary definitions.\n",
        "\n",
        "### **5. Why does the algorithm download nltk resources like 'wordnet', 'punkt', and 'stopwords'?**\n",
        "\n",
        "* **wordnet** → sense inventory\n",
        "* **omw-1.4** → multilingual WordNet\n",
        "* **punkt** → tokenizer required for Lesk\n",
        "* **stopwords** → useful for preprocessing context\n",
        "  These components are necessary for semantic lookup and WSD.\n",
        "\n",
        "-------------------------------------------\n",
        "\n",
        "### **1. Why is WordNet useful for understanding lexical semantics?**\n",
        "\n",
        "WordNet provides a structured semantic network (synonyms, hypernyms, hyponyms), helping learners understand how meaning is represented and related across words.\n",
        "\n",
        "### **2. What is the significance of studying hypernyms and hyponyms?**\n",
        "\n",
        "They encode **is-a relationships** (e.g., dog → animal). Understanding these hierarchies is essential for tasks like ontology building, text classification, and semantic reasoning.\n",
        "\n",
        "### **3. Why is path similarity meaningful?**\n",
        "\n",
        "Path similarity quantifies how conceptually close two words are. It reflects **semantic distance** and serves as a classical baseline for similarity tasks before neural embeddings.\n",
        "\n",
        "### **4. Why do we still teach the Lesk WSD algorithm?**\n",
        "\n",
        "Lesk is simple, interpretable, and demonstrates the **core idea of sense disambiguation via context overlap**. It provides conceptual grounding before introducing transformer-based WSD.\n",
        "\n",
        "### **5. How does this demo connect to modern NLP / LLMs?**\n",
        "\n",
        "WordNet relations (synonymy, hypernymy) and WSD logic form the foundation of semantic modeling. Modern LLMs implicitly learn these relations, and your demo shows the explicit classical version.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-RI-o7dLCnDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpBPjllWdv_y",
        "outputId": "2267a095-a9bb-499a-c06d-422687b0ea45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3333333333333333\n",
            "\n",
            "Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "interest_curiosity       0.00      0.00      0.00         1\n",
            "    interest_money       0.33      1.00      0.50         1\n",
            "    interest_stake       0.00      0.00      0.00         1\n",
            "\n",
            "          accuracy                           0.33         3\n",
            "         macro avg       0.11      0.33      0.17         3\n",
            "      weighted avg       0.11      0.33      0.17         3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "# --- Dataset ---\n",
        "examples = [\n",
        "    (\"She pays 3% interest on the loan.\", \"interest_money\"),\n",
        "    (\"The bank charges interest on late payments.\", \"interest_money\"),\n",
        "    (\"Interest on the mortgage is high this month.\", \"interest_money\"),\n",
        "\n",
        "    (\"He showed great interest in astronomy.\", \"interest_curiosity\"),\n",
        "    (\"I have an interest in modern art.\", \"interest_curiosity\"),\n",
        "    (\"Playing chess is one of my interests.\", \"interest_curiosity\"),\n",
        "\n",
        "    (\"Microsoft purchased a controlling interest in the company.\", \"interest_stake\"),\n",
        "    (\"They sold their interest in the factory.\", \"interest_stake\"),\n",
        "    (\"Business interests lobbied for the legislation.\", \"interest_stake\"),\n",
        "]\n",
        "\n",
        "# --- Better featurizer ---\n",
        "def featurize(sent, focus='interest', window=4):\n",
        "    toks = word_tokenize(sent.lower())\n",
        "    idxs = [i for i,t in enumerate(toks) if t == focus]\n",
        "\n",
        "    features = {}\n",
        "    for idx in idxs:\n",
        "        start = max(0, idx - window)\n",
        "        end = min(len(toks), idx + window + 1)\n",
        "        ctx = toks[start:end]\n",
        "        for t in ctx:\n",
        "            if t.isalpha() and t not in stop and t != focus:\n",
        "                features[f\"ctx({t})\"] = 1\n",
        "\n",
        "    if not idxs:  # fallback\n",
        "        for t in toks:\n",
        "            if t.isalpha() and t not in stop:\n",
        "                features[f\"bow({t})\"] = 1\n",
        "\n",
        "    return features\n",
        "\n",
        "# --- Vectorize ---\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "X_dict = [featurize(s) for s, lbl in examples]\n",
        "y = [lbl for s,lbl in examples]\n",
        "\n",
        "vec = DictVectorizer(sparse=False)\n",
        "X = vec.fit_transform(X_dict)\n",
        "\n",
        "# --- Stratified Split (critical) ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- Train ---\n",
        "clf = LogisticRegression(max_iter=500, multi_class='ovr')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nReport:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5_V57i9d5Fj",
        "outputId": "e95bede0-0fd4-4854-c091-293e774e4a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm interested in learning guitar. -> interest_money\n",
            "The bank increased interest rates. -> interest_money\n",
            "They sold their interest in the partnership. -> interest_stake\n"
          ]
        }
      ],
      "source": [
        "def predict_sentence(sent):\n",
        "    f = featurize(sent)\n",
        "    xv = vec.transform(f)  # note: DictVectorizer expects 2D; transform accepts list, but here dictionary okay\n",
        "    pred = clf.predict(xv.reshape(1, -1))\n",
        "    return pred[0]\n",
        "\n",
        "tests = [\n",
        "    \"I'm interested in learning guitar.\",\n",
        "    \"The bank increased interest rates.\",\n",
        "    \"They sold their interest in the partnership.\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    print(t, \"->\", predict_sentence(t))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8DlUjkxeOhP",
        "outputId": "06702e93-c88b-4ded-9ce8-9ba1785cce9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the', 'cat', 'drank', 'milk'],\n",
              " ['the', 'dog', 'drank', 'water'],\n",
              " ['a', 'cat', 'chased', 'a', 'mouse'],\n",
              " ['the', 'dog', 'chased', 'the', 'cat'],\n",
              " ['milk', 'and', 'water', 'are', 'liquids'],\n",
              " ['the', 'mouse', 'drank', 'water']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "corpus = [\n",
        "    \"the cat drank milk\",\n",
        "    \"the dog drank water\",\n",
        "    \"a cat chased a mouse\",\n",
        "    \"the dog chased the cat\",\n",
        "    \"milk and water are liquids\",\n",
        "    \"the mouse drank water\",\n",
        "]\n",
        "\n",
        "# tokenize\n",
        "tokenized = [word_tokenize(sent.lower()) for sent in corpus]\n",
        "\n",
        "tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-KDzWBZeQeo",
        "outputId": "9d6d7f1e-e90a-4f49-8b40-9dff1ca357c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat', 'chased', 'dog', 'drank', 'liquids', 'milk', 'mouse', 'water']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "# collect vocabulary\n",
        "vocab = sorted({w for sent in tokenized for w in sent if w.isalpha() and w not in stop})\n",
        "vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgaszAQieWP-",
        "outputId": "de1128fc-bace-4dc6-aa5b-3e8bfad55727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat {'drank': 1, 'milk': 1, 'chased': 2}\n",
            "dog {'drank': 1, 'water': 1, 'chased': 1}\n",
            "milk {'cat': 1, 'drank': 1, 'water': 1}\n",
            "water {'dog': 1, 'drank': 2, 'milk': 1, 'liquids': 1, 'mouse': 1}\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "\n",
        "# initialize co-occurrence counts\n",
        "cooc = defaultdict(lambda: Counter())\n",
        "\n",
        "for sent in tokenized:\n",
        "    for i, w in enumerate(sent):\n",
        "        if w not in vocab:\n",
        "            continue\n",
        "        # context window\n",
        "        left = max(0, i-window_size)\n",
        "        right = min(len(sent), i+window_size+1)\n",
        "        # context = [c for j, c in enumerate(sent[left:right]) if j != i and c in vocab]\n",
        "        context = [sent[j] for j in range(left, right) if j != i and sent[j] in vocab]\n",
        "        for c in context:\n",
        "            cooc[w][c] += 1\n",
        "\n",
        "# show some counts\n",
        "for w in [\"cat\", \"dog\", \"milk\", \"water\"]:\n",
        "    print(w, dict(cooc[w]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kbw8xoneaSn",
        "outputId": "ffcdd901-e76f-468f-e547-a907238f439c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 2., 0., 1., 0., 1., 0., 0.],\n",
              "       [2., 0., 1., 0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 1., 0., 0., 0., 1.],\n",
              "       [1., 0., 1., 0., 0., 1., 1., 2.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [1., 0., 0., 1., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 1., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 2., 1., 1., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# matrix with rows = words, columns = contexts\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "count_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "for w in vocab:\n",
        "    for c,count in cooc[w].items():\n",
        "        count_matrix[word2idx[w], word2idx[c]] = count\n",
        "\n",
        "count_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaT8uzRwefSU",
        "outputId": "51f2005c-a44a-4707-fdb1-8f91ca1ab9fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-27.63102112,   1.32175584, -27.63102112,   0.22314355,\n",
              "         -27.63102112,   0.91629073, -27.63102112, -27.63102112],\n",
              "        [  1.32175584, -27.63102112,   0.91629073, -27.63102112,\n",
              "         -27.63102112, -27.63102112,   0.91629073, -27.63102112],\n",
              "        [-27.63102112,   0.91629073, -27.63102112,   0.51082562,\n",
              "         -27.63102112, -27.63102112, -27.63102112,   0.51082562],\n",
              "        [  0.22314355, -27.63102112,   0.51082562, -27.63102112,\n",
              "         -27.63102112,   0.51082562,   0.51082562,   0.51082562],\n",
              "        [-27.63102112, -27.63102112, -27.63102112, -27.63102112,\n",
              "         -27.63102112, -27.63102112, -27.63102112,   1.60943791],\n",
              "        [  0.91629073, -27.63102112, -27.63102112,   0.51082562,\n",
              "         -27.63102112, -27.63102112, -27.63102112,   0.51082562],\n",
              "        [-27.63102112,   0.91629073, -27.63102112,   0.51082562,\n",
              "         -27.63102112, -27.63102112, -27.63102112,   0.51082562],\n",
              "        [-27.63102112, -27.63102112,   0.51082562,   0.51082562,\n",
              "           1.60943791,   0.51082562,   0.51082562, -27.63102112]]),\n",
              " array([[0.        , 1.32175584, 0.        , 0.22314355, 0.        ,\n",
              "         0.91629073, 0.        , 0.        ],\n",
              "        [1.32175584, 0.        , 0.91629073, 0.        , 0.        ,\n",
              "         0.        , 0.91629073, 0.        ],\n",
              "        [0.        , 0.91629073, 0.        , 0.51082562, 0.        ,\n",
              "         0.        , 0.        , 0.51082562],\n",
              "        [0.22314355, 0.        , 0.51082562, 0.        , 0.        ,\n",
              "         0.51082562, 0.51082562, 0.51082562],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 1.60943791],\n",
              "        [0.91629073, 0.        , 0.        , 0.51082562, 0.        ,\n",
              "         0.        , 0.        , 0.51082562],\n",
              "        [0.        , 0.91629073, 0.        , 0.51082562, 0.        ,\n",
              "         0.        , 0.        , 0.51082562],\n",
              "        [0.        , 0.        , 0.51082562, 0.51082562, 1.60943791,\n",
              "         0.51082562, 0.51082562, 0.        ]]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# total co-occurrences\n",
        "total = count_matrix.sum()\n",
        "\n",
        "# compute probabilities\n",
        "p_wc = count_matrix / total\n",
        "p_w = p_wc.sum(axis=1, keepdims=True)\n",
        "p_c = p_wc.sum(axis=0, keepdims=True)\n",
        "\n",
        "PMI = np.log((p_wc) / (p_w * p_c + 1e-12) + 1e-12)  # add eps to avoid div/0\n",
        "PPMI = np.maximum(PMI, 0)\n",
        "\n",
        "PMI, PPMI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1qQ4XN_EelNT"
      },
      "outputs": [],
      "source": [
        "def cosine(v1, v2):\n",
        "    if np.linalg.norm(v1)==0 or np.linalg.norm(v2)==0:\n",
        "        return 0\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def sim(word1, word2, M=PPMI):\n",
        "    return cosine(M[word2idx[word1]], M[word2idx[word2]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_Vwkm1FepRs",
        "outputId": "681b7421-b6b6-4fcd-89a3-dab4c58bbae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat   – dog   : 0.6994\n",
            "cat   – mouse : 0.6994\n",
            "milk  – water : 0.1173\n",
            "dog   – water : 0.1173\n",
            "cat   – milk  : 0.0602\n"
          ]
        }
      ],
      "source": [
        "pairs = [\n",
        "    (\"cat\", \"dog\"),\n",
        "    (\"cat\", \"mouse\"),\n",
        "    (\"milk\", \"water\"),\n",
        "    (\"dog\", \"water\"),\n",
        "    (\"cat\", \"milk\"),\n",
        "]\n",
        "\n",
        "for w1, w2 in pairs:\n",
        "    print(f\"{w1:5} – {w2:5} : {sim(w1, w2):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O84-HlJjew5I",
        "outputId": "e14132d6-5564-4506-b6ea-39630c89cc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(SVD) cat   – dog   : 0.7559\n",
            "(SVD) cat   – mouse : 0.7559\n",
            "(SVD) milk  – water : 0.1250\n",
            "(SVD) dog   – water : 0.1205\n",
            "(SVD) cat   – milk  : 0.0115\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(n_components=5)\n",
        "dense = svd.fit_transform(PPMI)\n",
        "\n",
        "def sim_dense(w1, w2):\n",
        "    return cosine(dense[word2idx[w1]], dense[word2idx[w2]])\n",
        "\n",
        "for w1, w2 in pairs:\n",
        "    print(f\"(SVD) {w1:5} – {w2:5} : {sim_dense(w1, w2):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. How does the code perform Word Sense Disambiguation (WSD)?**\n",
        "\n",
        "Your demo includes **two WSD approaches**:\n",
        "\n",
        "1. **Lesk algorithm** → dictionary-overlap–based unsupervised WSD.\n",
        "2. **Logistic Regression classifier** → supervised WSD trained on a small labeled dataset using bag-of-words features.\n",
        "\n",
        "This allows students to compare classical rule-based WSD with data-driven machine learning WSD.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What features are used in the supervised WSD classifier?**\n",
        "\n",
        "The classifier uses a simple **bag-of-words context representation**:\n",
        "\n",
        "* tokenized words (lowercased)\n",
        "* stopwords removed\n",
        "* target word (“interest”) excluded\n",
        "  Each remaining word becomes a binary feature `has(word)=1`.\n",
        "\n",
        "These are converted into vectors via `DictVectorizer`.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. How is the co-occurrence matrix constructed for PMI?**\n",
        "\n",
        "The code:\n",
        "\n",
        "* tokenizes each sentence\n",
        "* defines a **window size = 2**\n",
        "* counts how often each word occurs with every other within that window\n",
        "* stores counts in a matrix (rows = target words, columns = context words)\n",
        "\n",
        "This produces the raw statistics needed for PMI and PPMI.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. How is PMI and PPMI computed?**\n",
        "\n",
        "PMI = log( P(w,c) / (P(w) * P(c)) )\n",
        "PPMI = max(PMI, 0)\n",
        "\n",
        "The code applies smoothing (epsilon = 1e-12) to avoid division by zero and negative values are zeroed out to form PPMI.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What is the purpose of applying SVD to the PPMI matrix?**\n",
        "\n",
        "SVD reduces the high-dimensional co-occurrence space to dense semantic vectors (like early word embeddings).\n",
        "This demonstrates how **distributional similarity → low-dimensional embeddings → semantic similarity**.\n",
        "\n",
        "----------------------------------------------\n",
        "\n",
        "### **1. Why is it important to compare WordNet similarity with PMI similarity?**\n",
        "\n",
        "It shows the difference between:\n",
        "\n",
        "* **knowledge-based semantics** (WordNet hierarchy)\n",
        "* **distributional semantics** (“you shall know a word by the company it keeps”)\n",
        "\n",
        "Students see how meaning can come from structured knowledge vs. raw corpus statistics.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why do we train a supervised WSD model when unsupervised methods exist?**\n",
        "\n",
        "Lesk is interpretable but brittle.\n",
        "Supervised models adapt to real usage and generally achieve higher accuracy, showing the **evolution from rule-based → statistical → neural WSD**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why is PMI/PPMI important for understanding modern embeddings?**\n",
        "\n",
        "PMI is mathematically related to **Skip-gram Negative Sampling (SGNS)** and **GloVe**.\n",
        "Your demo helps students understand how classical distributional statistics inspired modern word embeddings and even transformer semantics.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What is the educational value of building a tiny labeled WSD dataset?**\n",
        "\n",
        "It illustrates the **full ML pipeline** on a small scale:\n",
        "\n",
        "* feature extraction\n",
        "* vectorization\n",
        "* train/test split\n",
        "* training\n",
        "* evaluation\n",
        "* prediction on unseen sentences\n",
        "  Students understand WSD without needing huge datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why reduce dimensionality with SVD?**\n",
        "\n",
        "SVD demonstrates the core idea behind embeddings:\n",
        "\n",
        "* compress co-occurrence information\n",
        "* preserve key semantic dimensions\n",
        "* produce dense vectors that enable smooth similarity computations\n",
        "\n",
        "This is the “classical ancestor” of word2vec/GloVe.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VETy7hwlDgYh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiFXKsxhezBn",
        "outputId": "7697e752-c8a5-470b-afed-5e4764a48743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The cat drinks milk.',\n",
              " 'Dogs and cats are common household pets.',\n",
              " 'Milk is a white liquid produced by mammals.',\n",
              " 'The dog chased the cat near the river.',\n",
              " 'Fresh milk and water are essential for health.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# small sample corpus (you can replace with your own)\n",
        "documents = [\n",
        "    \"The cat drinks milk.\",\n",
        "    \"Dogs and cats are common household pets.\",\n",
        "    \"Milk is a white liquid produced by mammals.\",\n",
        "    \"The dog chased the cat near the river.\",\n",
        "    \"Fresh milk and water are essential for health.\",\n",
        "]\n",
        "\n",
        "documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-h3qYzgfDC4",
        "outputId": "d48fc3f1-6dc6-462b-9426-0636d6f1e7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape (documents × terms): (5, 20)\n",
            "Vocabulary sample: ['cat', 'drinks', 'milk', 'dogs', 'cats', 'common', 'household', 'pets', 'white', 'liquid']\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Shape (documents × terms):\", tfidf_matrix.shape)\n",
        "print(\"Vocabulary sample:\", list(vectorizer.vocabulary_.keys())[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RIjCGKgfI7R",
        "outputId": "0ced59df-0ef8-4d50-f2b5-3ee69af3dec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: 'cat milk'\n",
            "\n",
            "Score: 0.7237  |  Doc 0: The cat drinks milk.\n",
            "Score: 0.2879  |  Doc 3: The dog chased the cat near the river.\n",
            "Score: 0.2028  |  Doc 4: Fresh milk and water are essential for health.\n",
            "\n",
            "Query: 'dog chased'\n",
            "\n",
            "Score: 0.6558  |  Doc 3: The dog chased the cat near the river.\n",
            "Score: 0.0000  |  Doc 4: Fresh milk and water are essential for health.\n",
            "Score: 0.0000  |  Doc 2: Milk is a white liquid produced by mammals.\n",
            "\n",
            "Query: 'white liquid'\n",
            "\n",
            "Score: 0.6705  |  Doc 2: Milk is a white liquid produced by mammals.\n",
            "Score: 0.0000  |  Doc 4: Fresh milk and water are essential for health.\n",
            "Score: 0.0000  |  Doc 3: The dog chased the cat near the river.\n"
          ]
        }
      ],
      "source": [
        "def search(query, top_k=3):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "\n",
        "    ranked = scores.argsort()[::-1]\n",
        "    print(f\"\\nQuery: '{query}'\\n\")\n",
        "\n",
        "    for idx in ranked[:top_k]:\n",
        "        print(f\"Score: {scores[idx]:.4f}  |  Doc {idx}: {documents[idx]}\")\n",
        "\n",
        "# Test queries\n",
        "search(\"cat milk\")\n",
        "search(\"dog chased\")\n",
        "search(\"white liquid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maepOmHWfM7B",
        "outputId": "d5f1c1d1-22e8-4eb3-cea6-fe45261823c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TF ONLY] Query: 'white liquid'\n",
            "\n",
            "Score: 0.6325 | Doc 2: Milk is a white liquid produced by mammals.\n",
            "Score: 0.0000 | Doc 4: Fresh milk and water are essential for health.\n",
            "Score: 0.0000 | Doc 3: The dog chased the cat near the river.\n",
            "\n",
            "Query: 'white liquid'\n",
            "\n",
            "Score: 0.6705  |  Doc 2: Milk is a white liquid produced by mammals.\n",
            "Score: 0.0000  |  Doc 4: Fresh milk and water are essential for health.\n",
            "Score: 0.0000  |  Doc 3: The dog chased the cat near the river.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "tf_vectorizer = CountVectorizer(stop_words='english')\n",
        "tf_matrix = tf_vectorizer.fit_transform(documents)\n",
        "\n",
        "def search_tf(query, top_k=3):\n",
        "    query_vec = tf_vectorizer.transform([query])\n",
        "    scores = cosine_similarity(query_vec, tf_matrix).flatten()\n",
        "    ranked = scores.argsort()[::-1]\n",
        "    print(f\"\\n[TF ONLY] Query: '{query}'\\n\")\n",
        "    for idx in ranked[:top_k]:\n",
        "        print(f\"Score: {scores[idx]:.4f} | Doc {idx}: {documents[idx]}\")\n",
        "\n",
        "search_tf(\"white liquid\")\n",
        "search(\"white liquid\")  # TF-IDF version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "RP33h53SfP88",
        "outputId": "12ee8f0a-568b-4a72-e169-6770cfd53bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF weights for Document 2:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             tfidf\n",
              "mammals   0.474125\n",
              "produced  0.474125\n",
              "white     0.474125\n",
              "liquid    0.474125\n",
              "milk      0.317527\n",
              "chased    0.000000\n",
              "cats      0.000000\n",
              "cat       0.000000\n",
              "common    0.000000\n",
              "dog       0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08862059-c12a-46cf-9558-1bfe5604f389\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mammals</th>\n",
              "      <td>0.474125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>produced</th>\n",
              "      <td>0.474125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>white</th>\n",
              "      <td>0.474125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>liquid</th>\n",
              "      <td>0.474125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>milk</th>\n",
              "      <td>0.317527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chased</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cats</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cat</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>common</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dog</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08862059-c12a-46cf-9558-1bfe5604f389')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-08862059-c12a-46cf-9558-1bfe5604f389 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-08862059-c12a-46cf-9558-1bfe5604f389');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0e5c605e-a062-4f19-bef1-7181c454a155\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e5c605e-a062-4f19-bef1-7181c454a155')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0e5c605e-a062-4f19-bef1-7181c454a155 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"show_weights(2)  # example: milk document\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"tfidf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2380030705012197,\n        \"min\": 0.0,\n        \"max\": 0.4741246485558491,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4741246485558491,\n          0.31752680284846835,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "def show_weights(doc_id):\n",
        "    vec = tfidf_matrix[doc_id].T.todense()\n",
        "    df = pd.DataFrame(vec, index=feature_names, columns=['tfidf'])\n",
        "    print(f\"\\nTF-IDF weights for Document {doc_id}:\")\n",
        "    display(df.sort_values('tfidf', ascending=False).head(10))\n",
        "\n",
        "show_weights(2)  # example: milk document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JSy_UCO-fauM",
        "outputId": "f5f8da61-d36d-4d79-d9c4-218f2cc1d1b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          D0   D1        D2        D3        D4\n",
              "D0  1.000000  0.0  0.146763  0.208308  0.146763\n",
              "D1  0.000000  1.0  0.000000  0.000000  0.000000\n",
              "D2  0.146763  0.0  1.000000  0.000000  0.100823\n",
              "D3  0.208308  0.0  0.000000  1.000000  0.000000\n",
              "D4  0.146763  0.0  0.100823  0.000000  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a032d925-c493-49bf-8f14-8cad5124c631\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>D0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.146763</td>\n",
              "      <td>0.208308</td>\n",
              "      <td>0.146763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D2</th>\n",
              "      <td>0.146763</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D3</th>\n",
              "      <td>0.208308</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D4</th>\n",
              "      <td>0.146763</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a032d925-c493-49bf-8f14-8cad5124c631')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a032d925-c493-49bf-8f14-8cad5124c631 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a032d925-c493-49bf-8f14-8cad5124c631');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3ffbc317-b539-4a69-982c-4fec94b213fd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3ffbc317-b539-4a69-982c-4fec94b213fd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3ffbc317-b539-4a69-982c-4fec94b213fd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"D0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.39855042918294836,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.20830757925910778,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"D1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44721359549995804,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0000000000000002,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"D2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4243849493135702,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          0.10082327052717012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"D3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4334139494633821,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.20830757925910778,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"D4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4243849493135702,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          1.0000000000000002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "pd.DataFrame(sim_matrix, columns=[f\"D{i}\" for i in range(len(documents))], index=[f\"D{i}\" for i in range(len(documents))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_1lGiSEfzwB",
        "outputId": "4fd31a2b-03b1-4960-f8fe-fdaf87bb9d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Starting download of 100 Wikipedia pages...\n",
            "\n",
            "   ✔ Batch processed in 49.44 sec (Total: 43 pages)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:41:06] Collected 50 / 100\n",
            "   ✔ Batch processed in 46.97 sec (Total: 78 pages)\n",
            "[04:42:11] Collected 100 / 100\n",
            "   ✔ Batch processed in 26.37 sec (Total: 100 pages)\n",
            "\n",
            "🎉 Completed downloading all pages!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "!pip install wikipedia\n",
        "\n",
        "import requests\n",
        "import time\n",
        "import wikipedia\n",
        "from datetime import datetime\n",
        "\n",
        "wikipedia.set_lang(\"en\")\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Raghvendra-Kumar-Demo/1.0 (contact: raghvendra.kumar@1004.com)\"\n",
        "}\n",
        "\n",
        "def get_random_titles_batch(batch_size=50):\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"random\",\n",
        "        \"rnlimit\": batch_size,\n",
        "        \"rnnamespace\": 0\n",
        "    }\n",
        "\n",
        "    for _ in range(5):  # retry up to 5 times\n",
        "        try:\n",
        "            r = requests.get(URL, params=PARAMS, headers=HEADERS, timeout=10)\n",
        "            data = r.json()\n",
        "            return [item[\"title\"] for item in data[\"query\"][\"random\"]]\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def get_random_wiki_pages(n_pages=100):\n",
        "    pages = []\n",
        "    last_print = time.time()\n",
        "\n",
        "    print(f\"Starting download of {n_pages} Wikipedia pages...\\n\")\n",
        "\n",
        "    while len(pages) < n_pages:\n",
        "        batch_start = time.time()\n",
        "        titles = get_random_titles_batch(50)\n",
        "        if not titles:\n",
        "            print(\"⚠️ API failed, retrying...\")\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "\n",
        "        for title in titles:\n",
        "            try:\n",
        "                page = wikipedia.page(title)\n",
        "                text = page.content\n",
        "                if len(text) > 500:\n",
        "                    pages.append((title, text))\n",
        "\n",
        "                    # Print progress every 50 pages added\n",
        "                    if len(pages) % 50 == 0:\n",
        "                        print(f\"[{datetime.now().strftime('%H:%M:%S')}] \"\n",
        "                              f\"Collected {len(pages)} / {n_pages}\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if len(pages) >= n_pages:\n",
        "                break\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "        print(f\"   ✔ Batch processed in {batch_time:.2f} sec \"\n",
        "              f\"(Total: {len(pages)} pages)\")\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    print(\"\\n🎉 Completed downloading all pages!\")\n",
        "    return pages\n",
        "\n",
        "\n",
        "wiki_docs = get_random_wiki_pages(100)\n",
        "len(wiki_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sz0E4g5tf3yQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6efcfb-97ef-4967-c9fb-9a0458843151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape: (100, 11604)\n"
          ]
        }
      ],
      "source": [
        "titles = [t for t, _ in wiki_docs]\n",
        "texts = [txt for _, txt in wiki_docs]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=20000)\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5o0uDVAkgx-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac1d280-3dc3-44ff-f7e1-0cdf258f7214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Query: quantum mechanics\n",
            "\n",
            "📘 Alexios Polychronakos  | score=0.3439\n",
            "\n",
            "📘 Li Shigu  | score=0.0000\n",
            "\n",
            "📘 Community gardening in Singapore  | score=0.0000\n",
            "\n",
            "📘 Tom Allin  | score=0.0000\n",
            "\n",
            "📘 Rarotonga International Airport  | score=0.0000\n",
            "\n",
            "\n",
            "🔍 Query: Indian classical music\n",
            "\n",
            "📘 Indian Congress (Socialist)  | score=0.2105\n",
            "\n",
            "📘 Juan Orrego-Salas  | score=0.0982\n",
            "\n",
            "📘 Celebrate (Whitney Houston and Jordin Sparks song)  | score=0.0625\n",
            "\n",
            "📘 L.A.X (musician)  | score=0.0467\n",
            "\n",
            "📘 Death by Design (album)  | score=0.0415\n",
            "\n",
            "\n",
            "🔍 Query: machine learning algorithms\n",
            "\n",
            "📘 Damir Mirvić  | score=0.0808\n",
            "\n",
            "📘 Meet Wally Sparks  | score=0.0313\n",
            "\n",
            "📘 Juan Orrego-Salas  | score=0.0227\n",
            "\n",
            "📘 SpyHunter (security software)  | score=0.0214\n",
            "\n",
            "📘 Katy Croff Bell  | score=0.0207\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def search_wiki(query, top_k=5):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    ranked = scores.argsort()[::-1]\n",
        "\n",
        "    print(f\"\\n🔍 Query: {query}\\n\")\n",
        "    for idx in ranked[:top_k]:\n",
        "        print(f\"📘 {titles[idx]}  | score={scores[idx]:.4f}\\n\")\n",
        "\n",
        "# example searches\n",
        "search_wiki(\"quantum mechanics\")\n",
        "search_wiki(\"Indian classical music\")\n",
        "search_wiki(\"machine learning algorithms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia beautifulsoup4 lxml\n",
        "import requests\n",
        "import time\n",
        "import wikipedia\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup   # needed to fix parser warning\n",
        "import re\n",
        "\n",
        "# ---------------------------------------------\n",
        "# FIX-1: Clean text (removes citations, spacing)\n",
        "# ---------------------------------------------\n",
        "def clean_text(t):\n",
        "    t = re.sub(r'\\[[0-9]+\\]', '', t)      # remove numeric citations\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()    # normalize whitespace\n",
        "    return t\n",
        "\n",
        "# ---------------------------------------------\n",
        "# FIX-2: patch wikipedia library to avoid warning\n",
        "# ---------------------------------------------\n",
        "def safe_html_parser(html):\n",
        "    return BeautifulSoup(html, features=\"lxml\")\n",
        "\n",
        "import wikipedia.wikipedia as wk\n",
        "wk.BeautifulSoup = safe_html_parser    # monkey-patch\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Set language + user agent\n",
        "# ---------------------------------------------\n",
        "wikipedia.set_lang(\"en\")\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Raghvendra-Kumar-Demo/1.0 (contact: raghvendra.kumar@iitp.ac.in)\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Fetch random titles\n",
        "# ---------------------------------------------\n",
        "def get_random_titles_batch(batch_size=50):\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"random\",\n",
        "        \"rnlimit\": batch_size,\n",
        "        \"rnnamespace\": 0\n",
        "    }\n",
        "\n",
        "    for _ in range(5):\n",
        "        try:\n",
        "            r = requests.get(URL, params=PARAMS, headers=HEADERS, timeout=10)\n",
        "            data = r.json()\n",
        "            return [item[\"title\"] for item in data[\"query\"][\"random\"]]\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Fetch random Wikipedia pages\n",
        "# ---------------------------------------------\n",
        "def get_random_wiki_pages(n_pages=100):\n",
        "    pages = []\n",
        "\n",
        "    print(f\"Starting download of {n_pages} Wikipedia pages...\\n\")\n",
        "\n",
        "    while len(pages) < n_pages:\n",
        "        batch_start = time.time()\n",
        "        titles = get_random_titles_batch(50)\n",
        "\n",
        "        if not titles:\n",
        "            print(\"⚠️ API failed, retrying...\")\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "\n",
        "        for title in titles:\n",
        "            try:\n",
        "                page = wikipedia.page(title)\n",
        "                raw_text = page.content\n",
        "                text = clean_text(raw_text)\n",
        "\n",
        "                if len(text) > 500:\n",
        "                    pages.append((title, text))\n",
        "\n",
        "                    if len(pages) % 50 == 0:\n",
        "                        print(f\"[{datetime.now().strftime('%H:%M:%S')}] \"\n",
        "                              f\"Collected {len(pages)} / {n_pages}\")\n",
        "\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            if len(pages) >= n_pages:\n",
        "                break\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "        print(f\"   ✔ Batch processed in {batch_time:.2f} sec \"\n",
        "              f\"(Total: {len(pages)} pages)\")\n",
        "\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    print(\"\\n🎉 Completed downloading all pages!\")\n",
        "    return pages\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Download random Wikipedia pages\n",
        "# ----------------------------------------------------------------------\n",
        "wiki_docs = get_random_wiki_pages(100)\n",
        "print(len(wiki_docs))\n",
        "\n",
        "titles = [t for t, _ in wiki_docs]\n",
        "texts  = [txt for _, txt in wiki_docs]\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# FIX-3: Much better TF-IDF settings\n",
        "# ----------------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    min_df=2,         # remove extremely rare words\n",
        "    max_df=0.8,       # remove overly common/broad words\n",
        "    ngram_range=(1,2) # bigrams improve quality\n",
        ")\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Search function\n",
        "# ----------------------------------------------------------------------\n",
        "def search_wiki(query, top_k=5):\n",
        "    query = clean_text(query)  # normalize\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    ranked = scores.argsort()[::-1]\n",
        "\n",
        "    print(f\"\\n🔍 Query: {query}\\n\")\n",
        "    for idx in ranked[:top_k]:\n",
        "        print(f\"📘 {titles[idx]}  | score={scores[idx]:.4f}\\n\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Example searches\n",
        "# ----------------------------------------------------------------------\n",
        "search_wiki(\"quantum mechanics\")\n",
        "search_wiki(\"Indian classical music\")\n",
        "search_wiki(\"machine learning algorithms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF4RiRmCHLi7",
        "outputId": "98497313-219c-4da2-f22e-dd7274354d07"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.11.12)\n",
            "Starting download of 100 Wikipedia pages...\n",
            "\n",
            "   ✔ Batch processed in 47.51 sec (Total: 35 pages)\n",
            "[04:56:10] Collected 50 / 100\n",
            "   ✔ Batch processed in 47.76 sec (Total: 65 pages)\n",
            "[04:57:12] Collected 100 / 100\n",
            "   ✔ Batch processed in 41.06 sec (Total: 100 pages)\n",
            "\n",
            "🎉 Completed downloading all pages!\n",
            "100\n",
            "TF-IDF matrix shape: (100, 4277)\n",
            "\n",
            "🔍 Query: quantum mechanics\n",
            "\n",
            "📘 Sing Again  | score=0.0000\n",
            "\n",
            "📘 Scott C. Fergus  | score=0.0000\n",
            "\n",
            "📘 Yang Shengshi  | score=0.0000\n",
            "\n",
            "📘 Pterophylla racemosa  | score=0.0000\n",
            "\n",
            "📘 American River 50 Mile Endurance Run  | score=0.0000\n",
            "\n",
            "\n",
            "🔍 Query: Indian classical music\n",
            "\n",
            "📘 Sintilimab  | score=0.2146\n",
            "\n",
            "📘 CNR Music  | score=0.1233\n",
            "\n",
            "📘 Tamanna (1942 film)  | score=0.0849\n",
            "\n",
            "📘 Sean Ardoin  | score=0.0767\n",
            "\n",
            "📘 Cry Along with the Babies  | score=0.0723\n",
            "\n",
            "\n",
            "🔍 Query: machine learning algorithms\n",
            "\n",
            "📘 Velachery taluk  | score=0.1910\n",
            "\n",
            "📘 Information Systems Professional  | score=0.0805\n",
            "\n",
            "📘 Panaeolus semiovatus var. semiovatus  | score=0.0793\n",
            "\n",
            "📘 Lienzo charro  | score=0.0719\n",
            "\n",
            "📘 Ivan Ivanišević  | score=0.0544\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. How does the TF-IDF–based search function work?**\n",
        "\n",
        "Your `search(query)` function:\n",
        "\n",
        "1. Converts the query into a TF-IDF vector using the same vocabulary learned from documents.\n",
        "2. Computes cosine similarity between the query and each document.\n",
        "3. Ranks documents from most to least relevant.\n",
        "   This demonstrates a classical vector-space information retrieval model.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why is TF search (`search_tf`) included alongside TF-IDF search?**\n",
        "\n",
        "You implemented both to show the difference:\n",
        "\n",
        "* **TF (raw term frequency)** → favors documents that repeat words, even if common.\n",
        "* **TF-IDF** → gives higher weight to rare, discriminative words.\n",
        "  The comparison helps students understand **why TF-IDF improves retrieval quality**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. What does `show_weights(doc_id)` demonstrate?**\n",
        "\n",
        "This function prints the **top TF-IDF weights** for any document.\n",
        "Students can see which words uniquely represent that document (e.g., “white”, “liquid” for the milk document).\n",
        "It teaches **interpretability** of TF-IDF vectors.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. How is document–document similarity computed in the code?**\n",
        "\n",
        "You compute a full **cosine similarity matrix** on all TF-IDF vectors:\n",
        "\n",
        "```\n",
        "sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "```\n",
        "\n",
        "This reveals which documents are semantically close to each other (e.g., “cat–dog chasing” sentences cluster).\n",
        "\n",
        "---\n",
        "\n",
        "### **5. How does the Wikipedia download module work?**\n",
        "\n",
        "Your code:\n",
        "\n",
        "* Uses the **Wikipedia Random API** to fetch random page titles in batches of 50.\n",
        "* Downloads full page content with retries + progress printing.\n",
        "* Builds a corpus of 100 articles for large-scale TF-IDF search.\n",
        "* Creates a high-dimensional TF-IDF matrix (max 20,000 features).\n",
        "\n",
        "This enables **real search** over real Wikipedia articles, not toy examples.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **1. Why is TF-IDF still taught when modern search uses embeddings?**\n",
        "\n",
        "TF-IDF is:\n",
        "\n",
        "* simple\n",
        "* interpretable\n",
        "* fast\n",
        "* surprisingly strong for domain-specific or small corpora\n",
        "  It shows students **the mathematical foundation of vector search**, which embeddings later extend.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why is cosine similarity used for search and clustering?**\n",
        "\n",
        "Cosine focuses on **direction**, not magnitude, making it ideal for text:\n",
        "\n",
        "* two documents are similar if they share important terms\n",
        "* independent of document length\n",
        "  It models semantic closeness in sparse vector spaces.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why download 100+ Wikipedia pages for the TF-IDF search demo?**\n",
        "\n",
        "Realistic examples illustrate:\n",
        "\n",
        "* high-vocabulary retrieval\n",
        "* noise in real-world documents\n",
        "* genuine search results (e.g., queries like \"quantum mechanics\")\n",
        "  This transitions the demo from a **toy example → practical IR system**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What does the feature-weight inspection teach?**\n",
        "\n",
        "It highlights that TF-IDF vectors are **explainable**:\n",
        "\n",
        "* each weight = how important a term is\n",
        "* you can open the vector and interpret the score\n",
        "  This contrasts with modern embedding models, where features are opaque.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Why is comparing TF vs TF-IDF searches educational?**\n",
        "\n",
        "Because it clearly demonstrates:\n",
        "\n",
        "* TF returns documents with repeated words (even if irrelevant)\n",
        "* TF-IDF returns conceptually relevant documents\n",
        "  Students learn *why* weighting by inverse document frequency is necessary.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e5u5QF-tERSs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3-UfKneEW7d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}